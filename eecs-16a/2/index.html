<!doctype html><html lang=en dir=ltr>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<meta name=description content="02-01: Linear (in)dependance, Matrix Transformations #    Slides  Note 3   Recall the simple tomography example from Note 1, in which we tried to determine the composition of a box of bottles by shining light at different angles and measuring light absorption. The Gaussian elimination algorithm implied that we needed to take at least 9 measurements to properly identify the 9 bottles in a box so that we had at least one equation per variable.">
<meta name=theme-color content="#FFFFFF">
<meta name=color-scheme content="light dark"><meta property="og:title" content="Week 2: (In)dependence">
<meta property="og:description" content="02-01: Linear (in)dependance, Matrix Transformations #    Slides  Note 3   Recall the simple tomography example from Note 1, in which we tried to determine the composition of a box of bottles by shining light at different angles and measuring light absorption. The Gaussian elimination algorithm implied that we needed to take at least 9 measurements to properly identify the 9 bottles in a box so that we had at least one equation per variable.">
<meta property="og:type" content="article">
<meta property="og:url" content="http://notes.mehvix.com/eecs-16a/2/"><meta property="article:section" content="docs">
<meta property="article:modified_time" content="2022-01-31T16:19:58-08:00">
<title>Week 2: (In)dependence | notes.mehvix.com</title>
<link rel=manifest href=/manifest.json>
<link rel=icon href=/favicon.png type=image/x-icon>
<link rel=stylesheet href=/book.min.89a77f7e702a8626749b948bbfb01109823daf6c1246ca407d1378833494c402.css integrity="sha256-iad/fnAqhiZ0m5SLv7ARCYI9r2wSRspAfRN4gzSUxAI=" crossorigin=anonymous>
<script defer src=/flexsearch.min.js></script>
<script defer src=/en.search.min.52e60d7617902bdd54987b01ecabe67f5b47ff9a08952373ba049bc4bf44ede5.js integrity="sha256-UuYNdheQK91UmHsB7Kvmf1tH/5oIlSNzugSbxL9E7eU=" crossorigin=anonymous></script>
<script defer src=/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js integrity="sha256-b2+Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC+NdcPIvZhzk=" crossorigin=anonymous></script>
<link rel=icon href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üìì</text></svg>">
<script>MathJax={tex:{inlineMath:[["$","$"],[".$","$"]]}}</script>
<script id=MathJax-script async src=https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.min.js integrity="sha512-9DkJEmXbL/Tdj8b1SxJ4H2p3RCAXKsu8RqbznEjhFYw0cFIWlII+PnGDU2FX3keyE9Ev6eFaDPyEAyAL2cEX0Q==" crossorigin=anonymous referrerpolicy=no-referrer></script>
<link rel=stylesheet href=http://notes.mehvix.com/css/custom_styling.css>
</head>
<body dir=ltr>
<input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control>
<main class="container flex">
<aside class=book-menu>
<div class=book-menu-content>
<nav>
<h2 class=book-brand>
<a class="flex align-center" href=/><span>notes.mehvix.com</span>
</a>
</h2>
<div class=book-search>
<input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/>
<div class="book-search-spinner hidden"></div>
<ul id=book-search-results></ul>
</div>
<ul>
<li>
<input type=checkbox id=section-254493c648c1f0be4cf9348c45ddbe37 class=toggle>
<label for=section-254493c648c1f0be4cf9348c45ddbe37 class="flex justify-between">
<a role=button>CogSci C100</a>
</label>
<ul>
<li>
<a href=http://notes.mehvix.com/cogsci-c100/intro/>Introduction</a>
</li>
<li>
<a href=http://notes.mehvix.com/cogsci-c100/perception/>Perception</a>
</li>
</ul>
</li>
<li>
<input type=checkbox id=section-20c391324c82ffdef893b60d754b7005 class=toggle checked>
<label for=section-20c391324c82ffdef893b60d754b7005 class="flex justify-between">
<a role=button>EECS 16A</a>
</label>
<ul>
<li>
<a href=http://notes.mehvix.com/eecs-16a/0/>Week 0: System Design & Linear Equations</a>
</li>
<li>
<a href=http://notes.mehvix.com/eecs-16a/1/>Week 1: Gaussian Elim. & Matrices + Vectors</a>
</li>
<li>
<a href=http://notes.mehvix.com/eecs-16a/2/ class=active>Week 2: (In)dependence</a>
</li>
</ul>
</li>
<li>
<input type=checkbox id=section-7571d6f544c6ffc847948b0f74d82ef1 class=toggle>
<label for=section-7571d6f544c6ffc847948b0f74d82ef1 class="flex justify-between">
<a role=button>Engineering 29</a>
</label>
<ul>
<li>
<a href=http://notes.mehvix.com/e-29/0/>Week 0: Intro & Tolerancing</a>
</li>
<li>
<a href=http://notes.mehvix.com/e-29/1/>Week 1: Fundamentals of Graphical Communication & Subtractive Processes</a>
</li>
<li>
<a href=http://notes.mehvix.com/e-29/2/>Week 2: Cutting-based Processes & Other Subtractive Processes</a>
</li>
</ul>
</li>
<li>
<input type=checkbox id=section-32aba3efd274a559b3dccd4e800c9d4b class=toggle>
<label for=section-32aba3efd274a559b3dccd4e800c9d4b class="flex justify-between">
<a href=http://notes.mehvix.com/docs/math-53/>Math 53</a>
</label>
<ul>
<li>
<a href=http://notes.mehvix.com/math-53/10/>10: Parametric Equations and Polar Coordinates</a>
</li>
<li>
<a href=http://notes.mehvix.com/math-53/12/>12: Vectors & Geometry of Space</a>
</li>
<li>
<a href=http://notes.mehvix.com/math-53/13/>13: Vector Functions</a>
</li>
<li>
<a href=http://notes.mehvix.com/math-53/14/>14: Partial Derivatives</a>
</li>
<li>
<a href=http://notes.mehvix.com/math-53/15/>15: Multiple Integrals</a>
</li>
<li>
<a href=http://notes.mehvix.com/math-53/16/>16: Vector Calculus</a>
</li>
<li>
<a href=http://notes.mehvix.com/math-53/trig/>Trig Identities</a>
</li>
<li>
<a href=http://notes.mehvix.com/math-53/trig-calc/>Trig Calculus</a>
</li>
</ul>
</li>
<li>
<input type=checkbox id=section-98c46cabf82aecebcca8f97f2965f738 class=toggle>
<label for=section-98c46cabf82aecebcca8f97f2965f738 class="flex justify-between">
<a href=http://notes.mehvix.com/docs/physics-7b/>Physics 7B</a>
</label>
<ul>
<li>
<a href=http://notes.mehvix.com/physics-7b/17/>17: Temperature, Thermal Expansion, & Ideal Gas Law</a>
</li>
<li>
<a href=http://notes.mehvix.com/physics-7b/18/>18: Kinetic Theory of Gases</a>
</li>
<li>
<a href=http://notes.mehvix.com/physics-7b/19/>19: Heat & First Law of Thermo</a>
</li>
<li>
<a href=http://notes.mehvix.com/physics-7b/20/>20: Second Law of Thermo</a>
</li>
<li>
<a href=http://notes.mehvix.com/physics-7b/21/>21: Electric Charges & Fields</a>
</li>
<li>
<a href=http://notes.mehvix.com/physics-7b/22/>22: Flux & Gauss's Law</a>
</li>
<li>
<a href=http://notes.mehvix.com/physics-7b/23/>23: Electric Potential</a>
</li>
<li>
<a href=http://notes.mehvix.com/physics-7b/24/>24: Capacitance, Dielectrics, Electric Energy Storage</a>
</li>
<li>
<a href=http://notes.mehvix.com/physics-7b/25/>25: Electric Current and Resistance</a>
</li>
<li>
<a href=http://notes.mehvix.com/physics-7b/26/>26: DC Circuits</a>
</li>
<li>
<a href=http://notes.mehvix.com/physics-7b/27/>27: Magnetism</a>
</li>
<li>
<a href=http://notes.mehvix.com/physics-7b/28/>28: Sources of Magnetic Field</a>
</li>
<li>
<a href=http://notes.mehvix.com/physics-7b/29/>29: Electromagnetic Induction & Faraday's Law</a>
</li>
<li>
<a href=http://notes.mehvix.com/physics-7b/30/>30: Inductance, Electromagnetic Oscillations, & AC Circuits</a>
</li>
</ul>
</li>
<li>
<a href=http://notes.mehvix.com/anthro-c12ac/>Anthro C12AC</a>
</li>
<li>
<a href=http://notes.mehvix.com/asamst-20a/>ASAMST 20A</a>
</li>
<li>
<input type=checkbox id=section-a18e6d23bfa638ffa382e954bd79207f class=toggle>
<label for=section-a18e6d23bfa638ffa382e954bd79207f class="flex justify-between">
<a role=button>AP Notes</a>
</label>
<ul>
<li>
<a href=http://notes.mehvix.com/ap/huge/>AP Human Geography</a>
</li>
<li>
<a href=http://notes.mehvix.com/ap/cmech/>AP Physics C: Mechanics</a>
</li>
<li>
<a href=http://notes.mehvix.com/ap/stats/>AP Statistics</a>
</li>
</ul>
</li>
</ul>
<ul>
<li>
<a href=https://cs61a.rouxl.es/ target=_blank rel=noopener>
CS61A (Anto's)
</a>
</li>
<li>
<a href=# target=_blank rel=noopener>
‚Äî
</a>
</li>
<li>
<a href=https://www.mehvix.com target=_blank rel=noopener>
w¬≥.mehvix.com
</a>
</li>
<li>
<a href=https://pass.mehvix.com target=_blank rel=noopener>
pass.mehvix.com
</a>
</li>
</ul>
</nav>
<script>(function(){var a=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(b){localStorage.setItem("menu.scrollTop",a.scrollTop)}),a.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>
</div>
</aside>
<div class=book-page>
<header class=book-header>
<div class="flex align-center justify-between">
<label for=menu-control>
<img src=/svg/menu.svg class=book-icon alt=Menu>
</label>
<strong>Week 2: (In)dependence</strong>
<label for=toc-control>
<img src=/svg/toc.svg class=book-icon alt="Table of Contents">
</label>
</div>
<aside class="hidden clearfix">
<nav id=TableOfContents>
<ul>
<li><a href=#02-01-linear-independance-matrix-transformations>02-01: Linear (in)dependance, Matrix Transformations</a>
<ul>
<li><a href=#linear-dependence>Linear Dependence</a></li>
<li><a href=#linear-independence>Linear Independence</a>
<ul>
<li><a href=#systems-of-linear-equations>Systems of Linear Equations</a></li>
</ul>
</li>
<li><a href=#row-perspective>Row Perspective</a></li>
<li><a href=#span>Span</a></li>
</ul>
</li>
</ul>
</nav>
</aside>
</header>
<article class=markdown><h1 id=02-01-linear-independance-matrix-transformations>
02-01: Linear (in)dependance, Matrix Transformations
<a class=anchor href=#02-01-linear-independance-matrix-transformations>#</a>
</h1>
<ul>
<li>
<a href=https://eecs16a.org/lecture/Lecture1B_Slides.pdf>Slides</a></li>
<li>
<a href=https://eecs16a.org/lecture/Note3.pdf>Note 3</a></li>
</ul>
<blockquote>
<p>Recall the simple tomography example from Note 1, in which we tried to determine the composition of a box of bottles by shining light at different angles and measuring light absorption. The Gaussian elimination algorithm implied that we needed to take at least 9 measurements to properly identify the 9 bottles in a box so that we had at least one equation per variable. However, will taking any 9 measurements guarantee that we can find a solution? Answering this question requires an understanding of linear dependence. In this note, we will define linear dependence (and independence), and take a look at what it implies for systems of linear equations.</p>
</blockquote>
<hr>
<h2 id=linear-dependence>
Linear Dependence
<a class=anchor href=#linear-dependence>#</a>
</h2>
<ul>
<li><strong>Linear dependence</strong> is a very useful concept that is often used to characterize the ‚Äúredundancy‚Äù of information in real world applications.
<ul>
<li>Closely tied to the idea of free and basic variables as we‚Äôve already seen</li>
</ul>
</li>
<li>We will give three (equivalent) definitions of linear dependence:
<ol>
<li>A set of vectors .$\{\vec v_1, \dots \vec v_n \}$ is linearly dependent if there exists scalars .$\alpha_1, \dots, \alpha_n$ such that .$\alpha_1 \vec v_1 + \dots + \alpha_n \vec v_n = \vec 0$ and not all .$\alpha_i$ are equal to zero. This combination of all-zero scalars has a special name: the &ldquo;trivial solution.&rdquo;</li>
<li>A set of vectors .$\{\vec v_1, \dots \vec v_n \}$ is linearly dependent if there exists scalars .$\alpha_1, \dots, \alpha_n$ and an index .$i$ such that .$\vec v_i = \sum_{j\neq i} \alpha_j \vec v_j$. In other words, a set of vectors is linearly dependent if one of the vectors could be written as a linear combination of the rest of the vectors</li>
<li>A set of vectors is either linearly dependent or linearly independent. More specifically, consider the sum in the first definition. If there is a solution to satisfy this equation other than to make all the scalars .$\alpha_1 = \dots = \alpha_n = 0$, (that is, a nontrivial solution) then the vectors are linearly dependent.</li>
</ol>
</li>
<li>Why three (equivalent) definitions? Because each is useful in different settings.
<ul>
<li>It is often easier mathematically to show linear dependence with definition (1) since we don‚Äôt need to try to ‚Äúsingle out‚Äù a vector to get started with the proof.</li>
<li>(2) gives us a more intuitive way to talk about redundancy. If a vector can be constructed from the rest of the vectors, then this vector does not contribute any information that is not already captured by the other vectors.</li>
</ul>
</li>
<li>
<a href="https://eecs16a.org/lecture/Note3.pdf#page=2">Proof of equivalency</a></li>
</ul>
<h2 id=linear-independence>
Linear Independence
<a class=anchor href=#linear-independence>#</a>
</h2>
<ol>
<li>From the first definition of linear dependence we can deduce that a set of vectors .$\{\vec v_1, \dots, \vec v_n \}$ is linearly independent if .$\alpha_1 \vec v_1 + \dots + \alpha_n \vec v_n = \vec 0$ implies .$\alpha_1 = \dots = \alpha_n = 0 $
<ul>
<li>A set of vectors is linearly independent if it is not linearly dependent.</li>
<li>E.x. any two vectors that are multiples of one another are dependent</li>
</ul>
</li>
</ol>
<h3 id=systems-of-linear-equations>
Systems of Linear Equations
<a class=anchor href=#systems-of-linear-equations>#</a>
</h3>
<ul>
<li>Recall that a system of linear equations can be written in matrix-vector form as .$A\vec x = \vec b$, where .$A$ is a matrix of variable coefficients, .$\vec x$ is a vector of variables, and .$\vec b$ is a vector of values that these weighted sums must equal. We will show that just looking at the columns or rows of the matrix .$A$ can help tell us about the solutions to .$A\vec x = \vec b$.</li>
</ul>
<h4 id=theorem-31>
Theorem 3.1
<a class=anchor href=#theorem-31>#</a>
</h4>
<ul>
<li>If the system of linear equations .$A\vec x = \vec b$. has an <strong>infinite number of solutions</strong>, then the columns of .$A$ are <strong>linearly dependent</strong>
<div class="book-columns flex flex-wrap">
<div class="flex-even markdown-inner">
<ul>
<li>If the system has infinite number of solutions, it must have at least two distinct solutions .$\vec x_1, \vec x_2$ which must satisfy</li>
</ul>
</div>
<div class="flex-even markdown-inner">
<p>$$A\vec x_1 = \vec b$$
$$A\vec x_2 = \vec b$$
</div>
</div>
<div class="book-columns flex flex-wrap">
<div class="flex-even markdown-inner">
<ul>
<li>Subtracting the first equation from the second equation, we have</li>
</ul>
</div>
<div class="flex-even markdown-inner">
<p>$$A (\vec x_2 - \vec x_1) = \vec 0$$
</div>
</div>
<div class="book-columns flex flex-wrap">
<div class="flex-even markdown-inner">
<ul>
<li>Define alpha as &mldr;<br></li>
</ul>
</div>
<div class="flex-even markdown-inner">
<p>$$\vec \alpha = \begin{bmatrix}
\alpha_1 \\
\vdots \\
\alpha_n \\
\end{bmatrix} = \vec x_2 - \vec x_1$$
</div>
</div>
<ul>
<li>Because .$\vec x_1, \vec x_n$ are distinct, not all .$\alpha_i$&rsquo;s are zero. Let the columns of .$A$ be .$\vec a_1, \dots \vec a_n$. Then, .$A \vec \alpha = \sum^n_{i=1} \alpha_i \vec a_i = \vec 0$. By definition, the columns of .$A$ are linearly dependent.
<ul>
<li>The sum term says that, in other words, matrix-vector multiplication is a linear combination of columns:
<img src=/docs/eecs-16a/2/mmsum.png alt></li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id=theorem-32>
Theorem 3.2
<a class=anchor href=#theorem-32>#</a>
</h4>
<ul>
<li>
<p>If the columns of .$A$ in the system of linear equations .$A\vec x = \vec b$ are <strong>linearly dependent</strong>, then the system <strong>does not have a unique solution</strong>.
<div class="book-columns flex flex-wrap">
<div class="flex-even markdown-inner">
<br>
<ul>
<li>Start by assuming we have a matrix A with <em>linearly dependent columns</em></li>
</ul>
</div>
<div class="flex-even markdown-inner">
<p>$$A = \begin{bmatrix}
| & | & & | \\
\vec a_1 & \vec a_2 & \dots & \vec a_n \\
| & | & & | \\
\end{bmatrix}$$
</div>
</div>
<div class="book-columns flex flex-wrap">
<div class="flex-even markdown-inner">
<ul>
<li>By the definition of linear dependence, there exist scalars .$\alpha_1, \dots, \alpha_n$ such that .$\alpha_1\vec a_1 + \dots + \alpha_n \vec a_n = \vec 0$ where not all of the .$\alpha_i$‚Äôs are zero. We can put these Œ±i‚Äôs in a vector:</li>
</ul>
</div>
<div class="flex-even markdown-inner">
<p>$$\vec \alpha = \begin{bmatrix}
\alpha_1 \\
\vdots \\
\alpha_n \\
\end{bmatrix}$$
</div>
</div>
<div class="book-columns flex flex-wrap">
<div class="flex-even markdown-inner">
<ul>
<li>By the definition of matrix-vector multiplication, we can compactly write the expression above:</li>
</ul>
</div>
<div class="flex-even markdown-inner">
<p>$$A\vec \alpha = \vec 0$$
$$\text{where } \vec \alpha \neq \vec 0$$
</div>
</div>
</p>
<ul>
<li>Recall that we are trying to show that the system of equations .$A\vec x = \vec b$ does not have a unique solution. We know that systems of equations can have either zero, one, or infinite solutions.
<ul>
<li>If our system of equations has zero solutions, then it cannot have a unique solution, so we don‚Äôt need to consider this case.</li>
</ul>
</li>
</ul>
<div class="book-columns flex flex-wrap">
<div class="flex-even markdown-inner">
<ul>
<li>Now let‚Äôs consider the case where we have at least one solution, .$\vec x$:
<ul>
<li>Therefore, .$\vec x + \vec \alpha$ is also a solution to the system of equations! Since both .$\vec x$ and .$\vec x + \vec \alpha$ are solutions, and .$\vec \alpha \neq \vec 0$, the system has more than one solution. We‚Äôve now proven the theorem.</li>
</ul>
</li>
</ul>
</div>
<div class="flex-even markdown-inner">
<p>$$A \vec x = \vec b$$
$$A \vec x + \vec 0= \vec b$$
$$A \vec x + A \vec \alpha = \vec b$$
$$A (\vec x + \alpha) = \vec b$$
</div>
</div>
</li>
<li>
<p>Note that we can add any multiple of .$\alpha$ to .$\vec x$ and it will still be a solution ‚Äì therefore, if there is at least one solution to the system and the columns of .$A$ are linearly dependent, then there are infinite solutions.</p>
<ul>
<li>Intuitively, in an experiment, each column in matrix .$A$ represents the influence of each variable .$x_i$ on the measurements. If the columns are linearly dependent, this means that some of the variables influence the measurement in the same way, and therefore cannot be disambiguated.
<a href="https://eecs16a.org/lecture/Note3.pdf#page=5">See page five for good example</a>.</li>
</ul>
</li>
</ul>
<hr>
<h4 id=implications>
Implications:
<a class=anchor href=#implications>#</a>
</h4>
<blockquote>
<p>This result has important implications to the design of engineering experiments. Often times, we can‚Äôt directly measure the values of the variables we‚Äôre interested in. However, we can measure the total weighted contribution of each variable. The hope is that we can fully recover each variable by taking several of such measurements. Now we can ask: ‚ÄúWhat is the minimum number of measurements we need to fully recover the solution?‚Äù and ‚ÄúHow do we design our experiment so that we can fully recover our solution with the minimum number of measurements?‚Äù</p>
</blockquote>
<blockquote>
<p>Consider the tomography example. We are confident that
we can figure out the configuration of the stack when the columns of the lighting pattern matrix .$A$ in .$A\vec x = \vec b$ are linearly independent. On the other hand, if the columns of the lighting pattern matrix are linearly dependent, we know that we don‚Äôt yet have enough information to figure out the configuration. Checking whether the columns are linearly independent gives us a way to validate whether we‚Äôve effectively designed our experiment.</p>
</blockquote>
<h2 id=row-perspective>
Row Perspective
<a class=anchor href=#row-perspective>#</a>
</h2>
<blockquote class="book-hint warning">
<p>Optional!
</blockquote>
<ul>
<li>Intuitively, each row represents some measurement
<ul>
<li>If the number of measurements taken is at least the number of variables and we cannot completely determine the variables, then at least one of our measurements must be redundant (it doesn‚Äôt give us any new information).</li>
</ul>
</li>
<li>This intuition suggests that the <strong>number of variables we can recover is equal to the number of unique measurements</strong>, or the number of linearly independent rows &ndash; this formal proof will come in a later note when we talk about rank.</li>
<li>Now have two perspectives: in the matrix, each row represents a measurement, while each column corresponds to a variable. Therefore, if the columns are linearly dependent, then we have at least one redundant variable. From the perspective of rows, linear dependency tells us that we have one or more redundant measurements.</li>
</ul>
<h2 id=span>
Span
<a class=anchor href=#span>#</a>
</h2>
<ul>
<li>Span of the columns of .$A$ is the set of <em>all</em> linear combinations of vectors .$\vec b$ such that .$A\vec x = \vec b$ has a solution
<ul>
<li>Mathematically, .$\text{span}(\vec v_1, \dots \vec v_2) = \bigg\{\sum_{i=1}^N \alpha_i \vec v_i\ |\ \alpha_i \in \mathbb{R}\bigg\}$</li>
<li>i.e. the set of all vectors that can be reached by all possible linear combinations of the columns of .$A$</li>
</ul>
</li>
<li>A set of vectors is linearly dependent if any one of the vectors is in the span of the remaining vectors.</li>
<li>span, range, and column space of .$A$ all refer to the span of the columns of .$A$
<div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden>
<iframe src=https://www.youtube.com/embed/k7RM-ot2NWY style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe>
</div>
<details><summary>Two Examples</summary>
<div class=markdown-inner>
<ul>
<li>e.x. what is the span of the cols of .$A = \begin{bmatrix} 1 & 1 \\ 1 & -1\\
\end{bmatrix}$?
$$\text{span(cols of A)} = \bigg\{ \vec v\ |\ \vec v = \alpha \begin{bmatrix} 1\\ 1\\ \end{bmatrix} + \beta \begin{bmatrix} 1\\ -1\\ \end{bmatrix}; \alpha, \beta \in \mathbb{R}\bigg\} = \mathbb{R}^{2}$$</li>
<li>e.x. what is the span of the cols of .$A = \begin{bmatrix} 1 & -1 \\ 1 & -1\\
\end{bmatrix}$?
$$\text{span(cols of A)} = \bigg\{ \vec v\ |\ \vec v = \alpha \begin{bmatrix} 1\\ 1\\ \end{bmatrix}; \alpha \in \mathbb{R}\bigg\} = \text{line } x_1 = x_2$$</li>
</ul>
</div>
</details>
</li>
</ul>
</article>
<footer class=book-footer>
<div class="flex flex-wrap justify-between">
<div><a class="flex align-center" href=https://github.com/Mehvix/notes/commit/83595894d016c990d44d4699c538c266a080680d title="Last modified by Max Vogel | February 1, 2022" target=_blank rel=noopener>
<img src=/svg/calendar.svg class=book-icon alt=Calendar>
<span>February 1, 2022</span>
</a>
</div>
<div>
<a class="flex align-center" href=https://github.com/Mehvix/notes/edit/master/hugo/content/docs/eecs-16a/2.md target=_blank rel=noopener>
<img src=/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span>
</a>
</div>
</div>
<script>(function(){function a(c){const a=window.getSelection(),b=document.createRange();b.selectNodeContents(c),a.removeAllRanges(),a.addRange(b)}document.querySelectorAll("pre code").forEach(b=>{b.addEventListener("click",function(c){a(b.parentElement),navigator.clipboard&&navigator.clipboard.writeText(b.parentElement.textContent)})})})()</script>
</footer>
<div class=book-comments>
</div>
<label for=menu-control class="hidden book-menu-overlay"></label>
</div>
<aside class=book-toc>
<div class=book-toc-content>
<nav id=TableOfContents>
<ul>
<li><a href=#02-01-linear-independance-matrix-transformations>02-01: Linear (in)dependance, Matrix Transformations</a>
<ul>
<li><a href=#linear-dependence>Linear Dependence</a></li>
<li><a href=#linear-independence>Linear Independence</a>
<ul>
<li><a href=#systems-of-linear-equations>Systems of Linear Equations</a></li>
</ul>
</li>
<li><a href=#row-perspective>Row Perspective</a></li>
<li><a href=#span>Span</a></li>
</ul>
</li>
</ul>
</nav>
</div>
</aside>
</main>
</body>
</html>