[{"id":0,"href":"/e-29/0/","title":"0: Intro \u0026 Tolerancing","section":"Engineering 29","content":" 01-18: Course introduction # Overview # This class focuses on three main components \u0026ndash; manufacturing processes, dimensional tolerances, and design communication \u0026ndash; and how they interact with one another.\nThe class is made up of 9 modules:\nFundamentals Subtractive manufacturing processes Additive manufacturing processes Forming processes Joining processes Graphical visualization techniques Metrology: measuring manufactured objects Geometric dimensioning and tolerancing The future of manufacturing Why manufacturing? # In 2018, U.S. manufacturing accounted for 11.6% of the U.S. economy, 18.2% of global manufacturing output, and 8.2% of the U.S. workforce \u0026ndash; source Manufacturing output is growing, and is returning to the U.S.; output increased \u0026gt;30% between end of 2008 and 2014 67% of U.S. R\u0026amp;D is funded by industry Even when production is offshore, design is often done here anyway Automation is increasing, yet there is a shortage of skilled (human) talent Even if you don\u0026rsquo;t want to go into manufacturing industry, research and academia still require manufacturing knowledge Even if the process if outsourced, design is still done in the US. To design well, you have to have a base-level understanding of manufacturing Manufacturing output and employment are rising\nMany companies have regionalized their supply chains since the pandemic\nProcesses # In this class we will consider multiple families of processes: This is a rapidly moving field that is always adapting This class should give you a top level overview so you can evaluate novel methods Materials # In this class we will consider multiple families of materials: Materials choices influence performance For example, consider the progress of the plane: In 1903 the right brothers low-density wood with steel wire and silk In 1935 the Douglas DC3 used aluminum alloy (since it became feasible to produce and manipulate) Now the 2010 Boeing 787 Dreamliner is made up of 50 wt% composites 20 wt% aluminum 15 wt% titanium 20% lower fuel consumption per passenger mile Composite materials are two(+) materials combined together to get best of both worlds, in aviation typically stiff/strong carbon fibers embed in tough/fatigue-resistant polymers. Materials choices influence market size There isn\u0026rsquo;t always a best material; different materials fit different markets/needs Opposite side of the coin: There may be multiple valid material choices for a particular function Tolerance # Tolerancing is a formal way of specifying limits on the amount of dimensional variability allowable in manufactured parts We need a range because measurements will never be 100% precise; we need to define an acceptable range Some sources of variation Human operator changes and/or errors Tool wear Environmental changes (temperature, humidity leads to tiny expansions / contractions) Input material variability Measurement error Affordable mass-production relies on interchangeability of parts When mating parts of given designs, it should not matter which specific parts Therefore part dimensions must be consistent But no manufacturing process is perfectly consistent If you don\u0026rsquo;t understand the process of manufacturing and the capabilities of tools, then you will won\u0026rsquo;t know how to create manufacturable designs Tighter tolerances (closer tolerance limits) are generally more expensive to achieve The solid green line shows an ideal process The dotted green line shows the impact of an error shifting the distribution, shifting the tails to approach the tolerance upper / lower bound The red line shows a unsuitable process (even if it\u0026rsquo;s calibrated accurately, the poor precision causes high variance that it\u0026rsquo;s not really feasible; however, if outside of the limits an additive (or less common subtractive) could be used to ) How E29 integrates manufacturing and tolerancing # Tighter tolerances are more expensive The physics of a process determine how tight a tolerance is achievable and how much it costs Therefore we need to understand how manufacturing processes work in order to: Select a suitable process for the application Specify reasonable tolerances Geometric Dimensioning and Tolerancing: a graphical language for specifying tolerances robustly Design Communication # Important to effectively describe your ideas and designs graphically Persuade \u0026ndash; we need to be able to show are perspective Instruct \u0026ndash; we need an agreed an unambiguous way to communicate Document \u0026ndash; we need to convey how to construct our final design Seek feedback \u0026ndash; we need to ensure everyone is on the same page Drawings can be 2D or 3D representations Interpreting 2D drawings made by others Creating 2D “working drawings” with unambiguous instructions Design communication is not only graphical Oral, written Manufacturing relies on teams Teaming activities 01-20: Fundamentals of Tolerancing # See why we study tolerancing from yesterday\u0026rsquo;s notes Basic tolerance formats # Unilateral e.g. Inches: .$.500^{+0.005}_{-0.000}$, Metric: .$35^{+0.05}_0$ (notice sigfig notation) Bilateral Most common; start with nominal then you have some tolerance bounds above and below Equal or unequal deviations from nominal dimension Same number of decimal places for upper and lower limits e.g. Inches: .$.500 \\pm .005$ or .$.500^{+0.005}_{-.010}$, Metric: .$35 \\pm 0.05$ or .$35^{+0.05} _{-0.10}$ Limit Given only bounds, not the nominal value e.g. Inches: .$.250, .248$, Metric: .$35.05, 35.00$ Tolerance buildup # In the real world we have error, so the way we define dimensions have an impact Best dimensions to label depend on function That is, dimensioning should be done intentionally such that critical distances result in minimal error, e.g.suppose distance between .$X$ and .$Y$ is critical Chain is bad since the potential (and often times real world) maximum error is large The errors compound since dimensions are in reference to other dimensions that may will contain error. The more dimensions chained, the greater the possible error Baseline is better \u0026ndash; every feature references a single base. However the worst case is still significant .$X$ may be off by .$\\pm .05$ and .$Y$ may be off .$\\mp 05$, compounding to .$\\pm 0.10$! Direct is ideal Depends on which dimensions are critical (that is, .$X, Y$) Normal cumulative distribution function # Tighter tolerances (closer tolerance limits) are generally more expensive to achieve The physics of the process used determines the curve\u0026rsquo;s characteristics .$\\sigma$ is the stdiv (width) of this density .$\\mu = x_0$ is the target (average) value we give This probability density characterizes how this function is distributed and the chance a given range of values occur The area under the curve in a given range is the probability the value falls within that range Single values, i.e .$x_0$, have a 0% probability. We can only calculate ranges because this is a density function. Probability density, e.g. given by Gaussian/Normal probability density function: $$p(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}}e^{-\\frac{(x-x_0)^2}{2\\sigma^2}}$$\nWhy do we care about statistics? We want to look at a process, look at tolerances, and figure out whether it\u0026rsquo;s worth to manufacture using this process If you know the distribution of a process, you can work out the probability a given part satisfies spec limits. There is no easy, exact analytical way to integrate the normal probability density function. The probability that a randomly chosen member of a normally distributed population has a value .$\\leq x$ is $$\\int_{-\\infty}^x p(x)\\ dx = P(x) = Z\\bigg(\\frac{x-\\mu}{\\sigma}\\bigg) = \\frac{1}{2}\\bigg[1 + \\text{erf}\\bigg(\\frac{x-\\mu}{\\sigma\\sqrt{2}}\\bigg)\\bigg]$$ .$\\text{erf()}$ is the error function .$Z$ is the normal cumulative distribution function; values of .$Z$ are tabulated in a Z table Example probability of part lying between within spec limits Process capability and tolerancing # Sigma, .$\\sigma$, is the standard deviation of dimensions actually produced by a process Six sigma processes Six Sigma (6σ) is a set of techniques and tools for process improvement. [\u0026hellip;] A six sigma process is one in which 99.99966% of all opportunities to produce some feature of a part are statistically expected to be free of defects.\nSpecification limits are .$12\\sigma$ apart. Here, 2 parts per billion lie outside specification limits if process is \u0026lsquo;in control\u0026rsquo; (i.e. if mean output of process is centered between specification limits) Arose because the cost of manufacturing, specifically the process that creates an error, has a cost. This cost can grow very large, very quickly, when mass-manufacturing. You\u0026rsquo;re best off spending money improving the process so the distribution gets tighter The alternative is either (1) accepting errors (resulting in faulty products) or (2) testing all components to ensure they are \u0026lsquo;good\u0026rsquo; and tossing out the bad ones Process capability: .$C_p = \\frac{\\text{USL - LSL}}{6\\sigma}$ Classes of Fit # Tolerances should be\u0026hellip; Not too tight: tight tolerances are expensive Not too loose: otherwise function is compromised Clearance fit: designed with space left between two components e.g. a shaft with a bearing need to have some give / free space Interference (push) fit: designed to be touching You may want interference because you want the friction between the components; you want the two pieces to not move/rotate/etc How? Elastic or even plastic deformation e.g. two pieces may need to fit tightly with friction as to prevent vibrations Expansion fit: If there are large forces/torques acting on these two components so you want them very tight e.g. you may temporarily expand one component (e.x. with heat) to fit on/around the other, then it will shrink down Shrink fit: Same as expansion, but using some cooling process (e.x. liquid nitrogen) Why do this over heat? It\u0026rsquo;s typically more expensive to cool down The material may deform / weaken \u0026ndash; e.g. steel will be degraded if heated up Transition fit: complete interchangeability is compromised to allow looser tolerance on individual components. If fit type is not critical. But even then, why not choose one or the other? Because you don\u0026rsquo;t want a large gap and the materials/parts cannot withstand the force needed to assemble them with an interference fit. The pieces are just for alignment \u0026ndash; think Ikea assembly pegs; they\u0026rsquo;re just to align components. It\u0026rsquo;s easier to manufacture these parts Snap fits # Involves temporary elastic deflection which enables parts to interlock, e.g. involving bending of one component Done often with molded parts Tends to involve Cantilever (e.g. casings), Annular (e.g. pen lids, take-out soup container lids) Designed to be assembled once, and typically not disassembled (multiple times) \u0026ndash; irreversible. Relatively simple: you don\u0026rsquo;t need screws/glues/etc. \u0026ndash; useful for rapid prototyping since you don\u0026rsquo;t have to consider fasteners Takes advantage of the fact that the material has some elasticity You need to stay within the elasticity limits of the material Most 3D plastics have \u0026rsquo;enough\u0026rsquo; give You (generally) want to design such that the stress is from bending, not stretching More, additional, extra, readings Terminology Definitions # Don\u0026rsquo;t stress about memorizing these !\nMaximum material condition (MMC): The greatest allowable amount of material left on the part (max size for a shaft; min size for a hole) Minimum/least material condition (LMC): The least allowable amount of material left on the part (min size for a shaft; max size for a hole) Important with MMC since they tell us how much they\u0026rsquo;re able to \u0026lsquo;slosh around\u0026rsquo; Basic size: Exact theoretical size from which limits are derived Different form nominal since basic refers to the standard table which gives respective upper and lower bounds (MMC and LMCs) Hole basis: Basic size is minimum size of hole Shaft basis: Basic size is maximum size of shaft \u0026ndash; used when many components need to fit on to one shaft. Basic size could be chosen to be in-between hole and shaft basis Tolerance: Allowable variation of one particular dimension Fundamental deviation: Difference between basic size and the closer of the MMC and LMC Allowance: Difference between maximum material conditions of the two components Types of fit # These types are created by ANSI: American National Standards Institute Exact values are tabulated in many source RC: Running and sliding clearance fits Nine categories: RC1: Close sliding: assemble without perceptible “play” (e.g. watches) Less than a 1/1000\u0026quot;. Basically impossible for air, let alone liquids, through. RC2: Sliding fits: seize with small temperature changes (e.g. ) RC3: Precision running: not suitable for appreciable temperature differences RC4: Close running: moderate surface speeds and pressures RC5/6: Medium running: higher speed/pressure RC7: Free running: where accuracy not essential and/or temperature variations large RC8/9: Loose running Go for lower if you want minimal vibration/gaps \u0026ndash; no perceivable play. Has drawbacks: The less clearance, the easier it is to seize up \u0026ndash; especially if two components are touching and made up of different materials (different expansion/contraction rates). Susceptible to dust, you would have to seal the machine or use it in clean conditions. If you go less precise, you don\u0026rsquo;t need to go slow, cheaper operator costs, cheaper tooling RC Chart RC Table LC: Locational clearance fits Normally stationary, but freely assembled/disassembled Used when you need clearance to dis able and clean LC Chart LT: Location transition fits Accuracy of location important Small amount of clearance or interference OK e.g. ikea furniture pegs LN: Locational interference When you need friction Accuracy of location is critical FN: Force fits When you need to hold a load (typically uses temporary heating) Designed to transmit frictional loads from one part to another Example: Which type of fit? Processes, tolerances, and surface quality # How do we relate physical processes and tools to these values? From MF Ashby, Materials Selection in Mechanical Design\nRoughness # How do we define roughness? You may use tool that uses a tiny needle to \u0026lsquo;scan\u0026rsquo; the surface, measuring deflections as you go From MF Ashby, Materials Selection in Mechanical Design\nRMS roughness: root mean square of deviations over the measured surface length i.e.: .$R^2 = L^{-1} \\int_0^L y^2\\ dx$ Usually, tolerance, .$T$, lies between 5R and 1000R Generally, if you go high rotation speed and slow translational speed you get less rough surfaces RMS Roughness Example "},{"id":1,"href":"/eecs-16a/0/","title":"0: System Design \u0026 Linear Equations","section":"EECS 16A","content":" 01-18: Course Introduction # Slides Notes 0, 1A All logistics, no notes!\n01-20: Introduction to Imaging, Tomography, and Linear Equations # Slides Notes 1A, 1B System Design # We use devices, such as imagers, that provide information, such as a visual representation of a system Often, these devices don’t work alone \u0026ndash; they are part of a larger system that uses a combination of both physical sensors and signal processing techniques. When we take projections of images, we tend to need to take multiple measuring (pictures) from differing angles Otherwise we have issues with overlap and ambiguity To generate 3D models, we need these multiple perspectives We ideally want to design a system that gives us a set of linear equations Some times we can only approximate these linear equations Lots of physical processes (i.e xrays!) are exponential so we just slap a log on it .$\\hat y = p \\cdot (e^{x_1} + \\dots + e^{x_n})$ .$y = -\\log_e (\\hat y \\cdot p^{-1}) = x_1 + \\dots + x_n$ .$\\hat y$ is our measured energy value .$x_n$ is the .$n$th \u0026lsquo;pixel\u0026rsquo; .$p$ is the power of the energy source Tomography Example To solve, we need enough independent equations that do not contain redundant information, otherwise there will be multiple ambiguous solutions Different models are made up of different configurations (of the energy source and measuring sensor) and result in different system of equations We can obtain equations by moving both the energy source and measuring sensor (think document scanner) to get each individual pixel We can also move the energy source alone instead \u0026ndash; think camera pointed at image with a projector used to light up certain (group of) pixel(s) Different patterns have pros/cons \u0026ndash; speed, resolution, accuracy, number of measurements, energy use Linear Algebra # The study of linear functions and linear equations, typically using vectors and matrices Linearity is not always applicable, but can be a good first-order approximation There exist good fast algorithms to solve these problems (and lots of fun properties!) Consider .$f(x_1, \\dots, x_n) : \\mathbb{R}^n \\to \\mathbb{R}$; .$f$ is linear if the following hold\u0026hellip; Homogeneity: .$f (\\alpha x_1, \\dots, \\alpha x_n) = \\alpha f(x_1, \\dots, x_n)$ If I scale the input by a scalar (i.e. by a factor of 2) then the output should also scale by the same factor Super position (distributivity): if .$x_i = y_i + z_i$ then .$f(y_1 + z_1, \\dots, y_n + z_n) = f(y_1, \\dots y_n) + f(z_1, \\dots z_n)$ 2 possible inputs: Pass the first input through the system to get a value. Pass another input through the system, and get another value. Add those two values to get a result. 1 possible input: Pass the summation of value 1 and value 2 through the system to get a result. If the result of both approaches are equal, then distributivity holds. Otherwise, distributivity does not hold. We can account for both Homogeneity and Super position by proving the function holds under the following equation: $$\\alpha_1 f(x_1, \\dots x_n) + \\dots + \\alpha_n f(y_1, \\dots, y_n) = f(\\alpha_1 x_1 +\\alpha_n y_1, \\dots, \\alpha_1 x_n + \\alpha_n y_n)$$ where .$y_n$ is a some scalar Linear functions can always be expressed as .$f(x_1, \\dots, x_n) = c_1 x_1 + \\dots + c_n x_n$ For .$\\mathbb{R}^2$, that is, .$f(x_1, x_2) = c_1 x_1 + c_2 x_2$ We know this system is linear so it follows these two rules above. So we should set up an equation where we can apply these properties. $$ x_1 = 1 \\cdot x_1 + 0 \\cdot x_2;\\ x_2 = 0 \\cdot x_1 + 1 \\cdot x_2$$ $$\\text{Let } y_1 = 1, z_1 = 0; y_2 = 0, z_2 = 1$$ $$ \\Longrightarrow x_1 = x_1 y_1 + x_1 z_1;\\ x_2 = x_2 y_2 + x_2 z_2$$ $$ \\Longrightarrow x_1 = x_1 (y_1 + z_1);\\ x_2 = x_2 (y_2 + z_2)$$ $$\\text{Therefore, } f(x_1, x_2) = f(x_1 y_1 + x_2 z_1, x_1 y_2 + x_2 z_2)$$ $$= x_1f(y_1, y_2) + x_2f(z_1, z_2)$$ $$= x_1f(1, 0) + x_2f(0, 1)$$ $$= c_1 x_1 + c_2 x_2\\ \\blacksquare$$\nLinear Set of Equations Consider the set of .$M$ linear equations with .$N$ variables: $$\\begin{matrix}a_{11} x_1 + a_{12} x_2 + \\dots + a_{1N} x_{N} = b_1\\\\ a_{21} x_1 + a_{22} x_2 + \\dots + a_{2N} x_{N} = b_2\\\\ \\text{} \\vdots\\\\ a_{M1} x_1 + a_{M2} x_2 + \\dots + a_{MN} x_{N} = b_M\\end{matrix}$$ \u0026hellip;it can be written compactly using augmented matrix: $$\\begin{bmatrix}a_{11} \u0026amp; a_{12} \u0026amp; \u0026hellip; \u0026amp; a_{1N} \u0026amp; \\text{|} \u0026amp; b_1\\\\ a_{21} \u0026amp; a_{22} \u0026amp; \u0026hellip; \u0026amp; a_{2N} \u0026amp; \\text{|} \u0026amp; b_2\\\\ \\vdots \u0026amp; \\text{} \u0026amp; \\vdots \u0026amp; \\text{ } \u0026amp; \\text{|} \u0026amp; \\vdots\\\\ a_{M1} \u0026amp; a_{M2} \u0026amp; \u0026hellip; \u0026amp; a_{MN} \u0026amp; \\text{|} \u0026amp; b_M\\end{bmatrix}$$ An interesting thing to notice about this representation is that the symbols corresponding to our unknowns have vanished entirely! Algorithm for solving linear equations Three basic operations that don\u0026rsquo;t change a solution: Multiply an equation with nonzero scalar .$2x+3y=4$ is same as .$4x+6y=8$ In other words, no solution exists that satisfies the second equation, but not the first. Consequently, the second equation is not only implied by, but also implies the first equation. When each of two equations imply the other, we say that they are equivalent. Adding a scalar constant multiple of one equation to another Example If we have the equations.. $$(1)\\ 5a+6b=7$$ $$(2)\\ 8a+9b=10$$ \u0026hellip;we can multiply .$(2)$ by the scalar 3 and add it to .$(1)$, to obtain the new system $$(3)\\ 29a+33b=37$$ $$(2)\\ 8a+9b=10$$ Clearly, observe that any solution to the first system will also be a solution to the second, since the first system of equations implies the second. But is the reverse true? Well, observe that equation .$(1)$ can be recovered by taking equation .$(3)$ and subtracting our scalar (in this case, 3) multiplied by equation .$(2)$. In other words, our second system is, not only implied by, but also implies the first system, so it does not introduce any new solutions. Thus, replacing the first system with the second does not change the solution set of our linear system, so this operation is valid.\nSwapping equations (changing arbitrary labels, trivial) Note 1AB Extra # Affine function: a function that can be written as a sum of a linear function and a scalar constant, so though .$\\beta (x)=2x+1$ is not linear, it is still affine Notice that the definition of affine functions includes all linear functions (by setting the scalar constant to 0), so every linear function is affine, but not all linear functions are affine. These definitions mean that while all functions describing a line can be shown to be affine, not all of them are linear. This has the unfortunate consequence that, in informal conversation, affine functions may be called linear, since both describe a line. This usage, though common, is wrong!, as seen with .$\\beta (x)$ above Linear Equation: Formally, a linear equation with the unknown scalars .$x_1, x_2, \\dots x_n$ is an equation where each side is a sum of scalar-valued linear functions of each of the unknowns plus a scalar constant. Expressed algebraically, we obtain the most general form of a linear equation, where the .$f_i$ and .$g_i$ are each linear functions with a single scalar input and output, and .$b_f$ and .$b_g$ are two scalar constants: $$f_1(x_1) + f_2(x_2) + \\dots + f_n(x_n) + b_f = g_1(x_1) + g_2(x_2) + \\dots + g_n (x_n) + b_g$$ Now, recall that linear functions with a single scalar input and output can be expressed in a very particular form \u0026ndash; we know that we can write .$f_i(x) = a_i \\cdot x$ and .$g_i(x) = a_i \u0026rsquo; \\cdot x$, where all the .$a_i$ and .$a_i \u0026lsquo;$ are scalar constants. Substituting, we find that the general form of a linear equation can be rewritten as $$a_1x_1 + a_2 x_2 + \\dots = a_n x_n + b_f = a_1\u0026rsquo; x_1 + a_2 \u0026rsquo; x_2 + \\dots + a_n \u0026rsquo; x_n + b_g$$ Notice that this expression can be thought of as a “weighted sum” of the .$x_i$, where the weights are the scalar constants .$a_i$. When the weights do not depend on any of the terms (such as when the weights are constants), we call the weighted sum a linear combination of said terms. So the above expression is typically referred to as a linear combination of the .$x_i$. That is, A linear equation is one that equates two linear combinations of the unknowns plus a constant term.\nOur system will have infinitely many solutions if any two variables are ambiguous Our system will have no solutions if we have a row of zeroes followed by a non-zero That is, .$\\begin{bmatrix}0 \u0026amp; 0 \u0026amp; \u0026hellip; \u0026amp; 0 \u0026amp; \\text{|} \u0026amp; \\alpha\\end{bmatrix} \\Longrightarrow 0x_1 + 0x_2 + \\dots 0x_n = \\alpha y$ is clearly impossible (no solutions) for any non-zero .$\\alpha$ Practice questions "},{"id":2,"href":"/cogsci-c100/intro/","title":"1: Introduction","section":"CogSci C100","content":" Overview and Fields # Cognitive Science: Study of the mind and cognition that integrates a number of different academic disciplines: Neuroscientists study the mind’s biological machinery Psychologists directly study mental processes, such as perception, learning/memory, judgement and decision-making Computer scientists explore how those processes can be simulated and modeled in computers Philosophers ask critical questions about the nature of the mind \u0026ndash; what is mind? how does it interface with body? is is purely the brain? Evolutionary biologists and anthropologists speculate about how the mind evolved The job of cognitive science is to provide a framework for bringing all those perspectives on the mind together Each of the various academic disciplines that comprise cognitive science use different methods, examples: Philosophers look at where the unity of their discipline comes from A commitment to rigorous argument and analysis Particularly in the so-called analytic tradition\u0026ndash; the tradition most relevant to cognitive science Certain problems that are standardly accepted as philosophical In contrast, the unity of psychology comes from a shared set of experimental techniques and paradigms (Different branches of) Neuroscience employ different tools appropriate to the level of organization at which they are studying the brain; These tools and techniques generally vary in\u0026hellip; Spatial resolution (y-axis): the scale on which they give precise measurement (varying scale: single neurons, e.x. light microscopy; to looking at parts of the brain in general \u0026ndash; which lobe/area is there activity?) Temporal resolution (x-axis): time intervals to which they are sensitive (the frequency at which you make observations: EEG is very high, to where fMRI/PETS where the effects are measured over a longer scale) Cognitive Science includes the study of anthropology and cultural differences Conclusions derived from Western psychology experiments may not be very representative of humanity as a whole Typical research participants tend to be WEIRD: Western, Educated, Industrialized, Rich, and Democratic Summary # Ideally, Cognitive Science would involve integration of these disparate methods:\nPhilosophers \u0026ndash; Deductive reasoning Psychologists \u0026ndash; Scientific method Cognitive psychologists \u0026ndash; Modeling AI researchers \u0026ndash; Computer models Neuroscientists \u0026ndash; Case studies, lesion methods, brain imaging Roboticists \u0026ndash; Build and test machines History of Cognitive Psychology/Cognitive Science # Cognition is mental activity – the acquisition, storage, transformation, and use of knowledge\nEarly History # Attempts to understand mind go back at least to the ancient Greeks Discovered laws of learning and memory, e.g., method of loci \u0026ndash; using imagery to enhance your memory capacity Described human thinking in terms of mechanical \u0026lsquo;manipulation of symbols\u0026rsquo; Study of mind remained province of philosophy until 19th century In 1879, Wundt established first psychology laboratory One of few important dates (this is the birth date of psychology!) Studied mental processes systematically using technique of introspection Not what we would consider rigorous now adays e.x. scientist would play a note on an instrument, then ask \u0026ldquo;how does this make you feel?\u0026rdquo; Within decades, however, experimental psychology became dominated by behaviorism\u0026ndash; a view that virtually denied the existence of the mind Believe that psychology should study relation between observable stimuli and observable behavioral responses Mind was banished from respectable scientific discussion \u0026ndash; you can\u0026rsquo;t tell what\u0026rsquo;s in that black box, so ignore it! However, in 1950’s, people started to become disenchanted with behaviorism (e.x. Little Albert) + more experimental research was being done, so cognitive psychology began to emerge Result of growth of interest in memory and developmental psychology, linguistics and computer science The Magical Number Seven, Plus or Minus Two \u0026ndash; proved you could study the mind experimentally. Discovered that people universally learned things at similar ages View that mental processes can best be understood by comparison with a computer A particular cognitive process can be represented by information flowing through a series of stages Development of Computational Model of Mind # Alan Turing In article published in 1936-37, conceived of information processing as an algorithmic or rule-based calculation process (Turing machine) A Turing machine has a set of instructions (machine table) that determines what the machine will do when it encounters a particular symbol in a particular cell, depending upon which internal state it is in\nTogether with advances that were made in designing and building digital computers during and after World War II, this led to development of view that cognition involves an algorithmic process of information processing Parallel distributed processing (PDP) and connectionist models/neural networks became popular in 1990’s That is, processing can happen parallel, simultaneously \u0026ndash; contrast with serial processing approach Hold that cognitive processes operate in a parallel fashion e.x: face recognition \u0026ndash; cognitive processes can be completed even when supplied information is incomplete or faulty; your friend dyes their hair, but you still recognize them next time you see them. Research in AI and Machine Learning boomed in early 21st century due to growth in computing power and availability of large data sets We knew how to do this way before, but didn\u0026rsquo;t have the data sets nor computational power til now Classifications: Artificial Intelligence (AI): tries to design computer models that accomplish the same cognitive tasks that humans do Machine Learning: a subset of AI that allows computers to \u0026ldquo;learn\u0026rdquo; (i.e. progressively improve performance on a specific task) by creating new algorithms to produce a desired output based on structured (or unstructured) data that is provided Relies on labeled data set and human input to differentiate classifications (e.x telling the computer cats have pink noses while dogs have black, etc.) Deep Learning: a subset of Machine Learning involving numerous layers of algorithms Machine does not need to be provided with structured data \u0026ndash; doesn\u0026rsquo;t required labeled data set, harder to do Neural Networks: Networks of algorithms that are similar to the neural networks present in the human brain The Turn to the Brain in Cognitive Science # Early models of cognitive functions, such as visual perception, focused on top-down analysis and included relatively little discussion of neural implementation \u0026ndash; not based on scientific method Neuroimaging techniques that emerged in the 1980s and 1990s, such as PET and fMRI, allowed neuroscientists to begin establishing large-scale correlations between types of cognitive functioning and specific brain areas Other techniques, such as single-cell recordings, have made it possible to study brain activity in nonhuman animals at the level of the single neuron The Cerebral Cortex # This will be on the exam! Frontal lobes: involved in speaking \u0026amp; muscle movements and in making plans \u0026amp; judgments \u0026ndash; directly proportional to social network Parietal lobes: include the sensory cortex, important in spatial navigation Occipital lobes: include the visual areas, which receive visual information from the opposite visual field Temporal lobes: include the auditory areas and mystical (out of body) experiences Motor cortex: area at the rear of the frontal lobes that controls voluntary movements Sensory cortex: area at the front of the parietal lobes that registers \u0026amp; processes body sensations Limbic System # Limbic cortex: phylogenetically older part of cortex Amygdala: Two almond-shaped neural clusters that are components of the limbic system and are linked to emotion, particularly fear and aggression Hippocampus: Donut-shaped structure that is important in memory Are our behaviors determined by brain function? # Or, is brain function determined by our behaviors? (Which came first\u0026ndash; the chicken or the egg?)\nPhysiological correlates can almost always be found for psychological states If we haven\u0026rsquo;t figured it out yet, we probably will very soon Penfield found that stimulating various parts of the brain with electrodes give rise to specific thoughts, emotions, images, or motor movements Done on epilepsy patients in preparation for surgery (so they didn\u0026rsquo;t remove any parts that were crucial) Abnormal EEG patterns seen in those with schizophrenia, depression, obsessive-compulsive disorder (OCD), and attention deficit-hyperactivity disorder (ADHD) However, this does not necessarily mean that brain states cause mental states! e.x. psychotherapy and drug therapy produce similar types of brain changes (e.g., in studies of treatment of depression, OCD, and ADHD) Recalling sad memories makes your brain temporarily look like the brain of someone with depression Psychotherapy (talk therapy) produces the same brain state that meditation does Controversies in CogSci # Do the benefits of cognitive neuroscience justify the costs?\nNeuroimaging studies can be quite outrageously expensive, and many of these studies do not provide direct practical benefits To run an hour PET on someone, it costs $20-30k per subject and you need many subjects for research Some researchers claim that cognitive neuroscience has not really helped to develop psychological theories \u0026ndash; people are just wowed by pictures of brains Limitations of the experimental method # Artificiality of experiments (lack of ecological validity \u0026ndash; doesn\u0026rsquo;t apply to real-world scenarios): the more you control the environment, the less like real life it becomes Argument that the best things in life (e.g., love, beauty, truth, joy) cannot be quantified There was an awful rainbow once in heaven:\nWe know her woof, her texture; she is given\nIn the dull catalogue of common things.\nPhilosophy will clip an Angel’s wings.\n\u0026ndash; John Keats\nBelief as a confounding variable: Magellan’s diary When Magellan interacted with native people, they could not see (perceive) his large ships The idea is that the ships were so alien to their experience that \u0026ldquo;\u0026hellip; their highly filtered perceptions couldn\u0026rsquo;t register what was happening, and they literally failed to \u0026lsquo;see\u0026rsquo; the ships.\u0026rdquo; (JZ Knight, What the Bleep Do We Know?) Certain things have to be believed until they can be seen SQ3R technique # Steps: Survey: Scan material; Read headings, figures, summaries Question: Pose questions to yourself Read: Look for answers to questions as you read; Read actively, not word-by-word (key to speed-reading!) Recite: Practice rehearsal (tell someone about the material) Review: Go over answers to questions; Review material again a day or several days later Fallacies: You have to read every word. The slower you read, the higher your comprehension. It’s a \u0026lsquo;sin\u0026rsquo; to skip around when you are reading. "},{"id":3,"href":"/e-29/1/","title":"1: Fundamentals of Graphical Communication \u0026 Subtractive Processes","section":"Engineering 29","content":" 01-25: Fundamentals of graphical communication # Evolution of graphical visualization # Hand drawing Instrument drawing (using mechanical things to measure distances) 2D CAD (initially only able to draw side views) 3D CAD (solid modeling) Automatic generation of 2D working drawings Enable easy communication of measurements between designer and manufacturer Created with the assumption that manufactured objects are made up of elementary objects, geons? Now we have the ability to run algos to analyze models, removing unnecessary bits We\u0026rsquo;re moving towards computer-generated geometry that\u0026rsquo;s contained by human input # Increasingly complex geometries Topological optimization Internal lattices The way we interface with drawings has to keep up with this New interfaces Virtual and augmented reality for visualizing designs Why bother sketching by hand? # Why not go straight to CAD? Some possible reasons: There is a connection between drawing and your own creativity; a feedback loop of sorts CAD bottlenecks you to designing a certain way Find one’s own distinctive style Avoid making detailed decisions too early Keep geometries more freeform Ideas may come to mind anywhere, anytime Potentially quicker Sketch Examples Leonardo da Vinci: “helicopter” (c. 1489) Charles and Ray Eames: chair Renzo Piano + Richard Rogers: Pompidou Center Philippe Starck: lemon squeezer Aside: how is it made? # Jonathan Ive/Apple design team: iPhone Burj Khalifa: Adrian Smith Tesla Model 3: Franz Holzhausen Concept drawing for Berkeley Engineering Essentials of 2D sketching # Line types matters Solid: edges Dashed: hidden detail Chain: centerline. - . - . \u0026hellip; Faint: construction Align, but don\u0026rsquo;t touch, features Dimension lines: Fainter than edges, and not connected. Long, thin arrows. Lots of differing standards, just be consistent 3D pictorial approaches # Isometric drawing # \u0026lsquo;iso\u0026rsquo; = same, \u0026lsquo;metric\u0026rsquo; = measure Any lines parallel .$x,y,z$ on the lines are equal length Orthogonal edges of a 3D object map to: Vertical lines Lines at .$\\pm 30^\\circ$ to horizontal Enables creating reasonably realistic drawings fairly easily Dimensions in these orthogonal directions are preserved on page Dimensions in other directions are not preserved on page Circles in isometric Use construction lines for bounding box Mark midpoints Draw longer quadrants first Through holes: use construction lines for obscured circle; darken later What if circle is not on an orthogonal face? Coded plans for practicing isometric sketching # Principle: number in cell gives height of column to be drawn Codes could be given on isometric or square grid ( plan view) Square grid: viewpoint explicitly specified Different views / perspectives may obscure different features. Chose the one that minimizes information loss / ambiguity You can shade certain surface to convey shadow (and thus depth, 3D information) Plan View Draw Example Axonometric drawing # Orthogonal edges are represented by: Vertical lines Lines at .$\\pm 45^\\circ$ to horizontal Advantage: Floorplans are not skewed/distorted Disadvantage: Areas of equal orthogonal faces are not equal on drawing Can look more \u0026ldquo;distorted\u0026rdquo; # Example of axonometric drawing\nSometimes called “planometric” Popular in architecture to preserve floorplans Oblique drawings # Front view is undistorted Conveys lots of information of a single face Angles are arbitrary (here they\u0026rsquo;re ~45) Receding lines drawn at a constant angle Judgement needed to select scale for receding direction Perspective drawing # Simulates how the eye sees 3D objects: further away objects/details are smaller Much more challenging than other methods Receding lines not parallel But CAD software can generate Horizon line Vanishing point(s) One point: dimensions referenced from closest surface Two points: dimensions referenced from closest edge # Introduction to sketching tools # Digital sketching tools # Autodesk Sketchbook Now discontinued! :( OneNote is an alright alternative Import isometric grid on Layer 2 Draw on Layer 1 Google Jamboard Adobe Photoshop, Illustrator Software resource page in bCourses Analog sketching tools # Set square/drawing triangle Pencils: Various hardness/softness Mechanical vs traditional Pens Ballpoint Felt tip Technical Possible ways of enhancing 3D sketches # Shading \u0026ndash; simulate the effect of light falling on object e.x shadows, glare, Visual clarity \u0026ndash; edges bolder: example of how it clears confusion Suggesting motion, sound, texture, etc. # 01-27: Subtractive processes: types of subtractive process; mechanics of cutting # Subtractive manufacturing processes # Subtractive processes are those that begin with standard stock material (in rod, bar, sheet, plate form etc) and remove material to impart shape Types of subtractive process # Material comes in range of mediums: billet (below, left), sheet, foil, wire, pellets, etc. A subtractive process removes material from a larger piece of material to define the geometry needed Subtractive processes therefore generate waste material How we deal with this is important Take 5 minutes to brainstorm all the ways you can think of to remove material in a controlled way from a solid piece of stock to create a geometry Also think about what could happen to the waste material What are some ways of removing material? Can we re-use the extra material? How can we cut away material? Some possibilities: # Cutting \u0026ndash; taking a sharp edge to material Drilling, boring, reaming Milling Lathe operations: turning, facing, tapping Shearing/punching/puncturing Sawing Chiseling Collectively: “machining” Electric field Electrical discharge machining (EDM): electrode, wire Localized melting or vaporization Laser cutting, laser ablation Flame, plasma cutting Hot wire (typically used for plastics) Aside: is melting really subtractive? Misc Chemical methods ( Etching) Explosives (think mining) Forcing apart (using physical force) Abrasion \u0026ndash; rough, abrasive wheels rotate at high speed across the surface of the component, removing particles of the workpiece. This approach is well suited even to the hardest of materials, although control of geometry is not as great as in cutting operations, for example. Grinding, sanding, lapping, polishing, filing Abrasive jet (water jet) Sand blasting Why would we choose a subtractive process # Instead of an additive process? Instead of a forming process, e.g. casting? Some possible reasons to: # Stock materials tend to be readily available (and inexpensive compared with the specialist powders that are used in some additive processes such as selective laser melting Precision and finish are exceptionally good – probably better than any other family of processes Need strength / structure Material cannot be molded (it\u0026rsquo;s the only feasible process) Doesn\u0026rsquo;t alter heat treatment (if feed and speed isn\u0026rsquo;t too high) Short runs \u0026ndash; no specialized tooling costs Easy customization \u0026amp; More control for iterative product development – every component produced can be different if needed All you need is to generate new .gcode; versus a whole mold Why not choose subtractive? # Slow for large run sizes compared to forming processes Most processes are “serial”, meaning that each feature on the component needs to be produced sequentially, making the processes slow. This is in contrast, for example, to molding/casting operations where the entire component is produced in approximately one step. Fairly costly in comparison Large amount of waste generated (low efficiency) High operator skill is often demanded, raising processing costs Cuts across metal grains \u0026ndash; not as strong as forged or possibly cast components focus on material cutting Cutting operations rely on wedge-shaped teeth Mechanics of lathe turning of metals # To understand the key concepts of metal cutting, we focus specifically on lathe turning i.e. a reduction in the diameter of a cylindrical component using a cutting tool. We focus on the turning of metals and their alloys, although turning is also widely used to process polymeric materials and even composite materials (wood being one “composite” that is regularly turned). Cutting operations rely on wedge-shaped teeth Lectures 4 and 5 will focus on cutting-based operations; Lecture 6 will look at some of the others A close look at a metal cutting operation\nTerminology # The workpiece (or simply work — the solid material that is to be reduced in diameter) is held firmly at one end in a chuck, whose jaws are tightened against the workpiece enough that the friction between the work and the chuck is always enough to resist the torques experienced by the work during cutting. The work is rotated, using an electric motor, at angular velocity .$\\omega$, which is typically hundreds or even thousands of revolutions per minute. Let us call the axis of rotation z. If the radius of the workpiece is a at the location of cutting, then the linear, tangential velocity of the work relative to the tool is .$V_\\text{cut} = \\omega a$, provided that .$\\omega$ is expressed in radians/second (to convert from rev/min to rad/sec multiply by .$2\\pi/50$). The cutting tool is mounted on a cross-slide/carriage assembly, which enables precise control of the cutting edge along the z axis and also radially outwards from the z axis (let us call this radial direction the x axis). The carriage and cross-slide could be moved by manual screws or by computer-controlled motors with positional feedback. Once the work is rotating at its target speed, the tool is positioned slightly to the right of the end of the work and the tool is moved inwards in the x direction (towards the rotational z axis) by a distance .$d$, called the depth of cut. The tool is then moved from right to left along the workpiece at a velocity called the feed rate given by .$V_\\text{feed} = f\\omega$, where .$f$ is the feed, or the distance moved by the tool along the z axis in one revolution of the work. Feed rate is a velocity, while feed is a distance per revolution — this subtlety of terminology is important to note. Note that in almost any turning process, .$V_\\text{cut} \u0026raquo; V_\\text{feed}$ A close look at a metal cutting operation # High-speed video: cutting tool moves across surface\nTeeth are at a specific, optimal wedge angle wrt the material to shave away stock Material is sheared off the workpiece (like butter and knife) Quality of cut depends on rake angle (front angle), .$\\alpha$, of tool Rake angle vertical (.$\\alpha = 0$): material piles up and lot of heat is generated Rake angle too large: tool is fragile, can dig into material Rake angle just right: cutting happens close to tool What is “just right” for the rake angle .$\\alpha$? .$30^\\circ$ is a good magic starting value Clearance angle should be non-zero Allows you to run the tool in reverse over the material without cutting Material Removed # The metal that is cut, or shaved, away from the workpiece is called the chip, and depending on (1) how brittle or ductile the work material is, (2) the shape of the cutting tool, and (3) the speed of cutting, the chip may be either a fairly continuous spiral or a series of small chips that regularly fracture. Many small chips are desirable from a practical perspective because, unlike very long continuous chips, they do not tend to become tangled around the work and potentially scratch it. Cutting tools will often have protrusions on their front surface that are designed to break up a chip as it comes off the workpiece. So in turning, a spiral of material is being removed from the work that has initial cross-section .$d \\times f$ and with the tool sweeping through the material with velocity .$V$. Thus, the volumetric removal rate of material is .$R_\\text{MR} = Vdf$. Cutting depends on material properties # More ductile materials (aluminum, mild steel, copper etc): long, spiral-shaped chips of material More brittle materials (e.g. cast iron): comes off in short chips Basic types of chips produced in metal cutting and their micrographs:\n(a) continuous chip with narrow, straight primary shear zone;\n(b) secondary shear zone at the tool-chip interface; (c) continuous chip with built-up edge;\n(d) segmented or nonhomogeneous chip; and\n(e) discontinuous chip.\n"},{"id":4,"href":"/eecs-16a/1/","title":"1: Gaussian Elim. \u0026 Matrices + Vectors","section":"EECS 16A","content":" 01-25: Gaussian Elimination, Vectors # Slides Notes 2A, 2B Upper Triangular Systems # Consider the following equation $$x-y+2z=1$$ $$y-z=2$$ $$z=1$$ \u0026hellip;which can be represent as an augmented matrix: $$\\begin{bmatrix} 1 \u0026amp; -1 \u0026amp; 2 \u0026amp; \\text{|} \u0026amp; 1\\\\ 0 \u0026amp; 1 \u0026amp; -1 \u0026amp; \\text{|} \u0026amp; 2\\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; \\text{|} \u0026amp; 1 \\end{bmatrix}$$ These are called upper triangle matrices \u0026ndash; they are nice in that they\u0026rsquo;re easy to solve! The solution is reached when the diagonal is all one, the remaining is zero (excluding the rightmost \u0026lsquo;answer\u0026rsquo; colum) Row Echelon Form # More precisely, a matrix is in row echelon form when the following criteria are met: All nonzero rows are above all zero rows. The leading coefficient of a non-zero row is always to the right of the leading coefficient of the row above it. $$\\begin{bmatrix} 1 \u0026amp; * \u0026amp; * \u0026amp; * \u0026amp; \\text{|} \u0026amp; *\\\\ 0 \u0026amp; 1 \u0026amp; * \u0026amp; * \u0026amp; \\text{|} \u0026amp; *\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; \\text{|} \u0026amp; *\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\text{|} \u0026amp; 0\\\\ \\end{bmatrix}$$ The leading coefficient of every non-zero row (which we call the pivot, and say is in the pivot position) is 1. Some textbooks will require this third property, others don\u0026rsquo;t Reduced Row Echelon Form # Reduced Row Echelon Form: requires that, in addition to the upwards propagation of variables in step (3), we will obtain a matrix with the following properties, in addition to the two mentioned above: The matrix is in row echelon form. The leading coefficient of every non-zero row (which we call the pivot, and say is in the pivot position) is 1. Each column with an element that is in the pivot position of some row has 0s everywhere else. $$\\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; * \u0026amp; 0 \u0026amp; \\text{|} \u0026amp; *\\\\ 0 \u0026amp; 1 \u0026amp; * \u0026amp; 0 \u0026amp; \\text{|} \u0026amp; *\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; \\text{|} \u0026amp; *\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\text{|} \u0026amp; 0\\\\ \\end{bmatrix}$$ Sometimes abbreviated (especially in programming) as rref. By construction, the Gaussian elimination algorithm always results in a matrix that is in reduced row echelon form. Once an augmented matrix is reduced to reduced row echelon form, variables corresponding to columns containing leading entries are called basic variables, and the remaining variables are called free variables If there just isn\u0026rsquo;t enough information and the equations do not contradict each other, then there exist an infinite number of solutions. When this happens, choose some variable (ideally, which is in most of the equations) and then solve each equation in terms of that variable (e.x. .$z$ is in all equations, so now write .$x,y,\\dots$ in terms of .$z$). Example We start with the following system: $$x-y+2z=1$$ $$2x+y+z=8$$ $$-4x+5y = 7$$ \u0026hellip;which we can write as a matrix $$\\begin{bmatrix} 1 \u0026amp; -1 \u0026amp; 2 \u0026amp; \\text{|} \u0026amp; 1\\\\ 2 \u0026amp; 1 \u0026amp; 1 \u0026amp; \\text{|} \u0026amp; 8\\\\ -4 \u0026amp; 5 \u0026amp; 0 \u0026amp; \\text{|} \u0026amp; 7 \\end{bmatrix}$$ \u0026hellip;and we can row-reduce to upper triangle (Row echelon)\n\u0026hellip;which we can use back substitution to solve\nTomograph Example Error # In real systems, we will always have noise (error) that makes our systems slightly skewed So what if we repeat the example above, but have a measurement of .$+0.1$\u0026hellip; are there any solutions? Graphing # We can represent our solution as a set of linear equations, meaning we can represent them graphically 01-27: Vectors, Matrices, Multiplications, And Span # Slides Notes 2A, 2B Last lecture, we showed how vectors and matrices could be used as a way of writing systems of linear equations more compactly, demonstrating through our tomography example that modeling a set of measurements as a system of equations can be a powerful tool.\nIn these following notes, we are going to more thoroughly discuss how to perform computations with vectors and matrices. In future notes, we will consider additional properties of vectors and matrices and see how these can help us understand real-world systems.\nVectors # Given a collection of .$n$ real numbers such as .$x_1, x_2, \\dots x_n$, we can represent this collection as a single point in an .$n$-dimensional real space .$\\mathbb{R}^n$, denoted as a .$\\vec x$ Each .$x_i$ (for .$i$ between .$1$ and .$n$) is called a component, or element, of the vector. $$\\vec x = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}$$ The size of a vector is the number of components it contains Two vectors .$\\vec x$ and .$\\vec y$ are said to be equal, .$\\vec x = \\vec y$, if they have the same size and .$x_i = y_i$ for all .$i$ Vectors are interesting because they can represent any set of numbers Representing as vectors lets us apply a lot of nice operations to them + represent them graphically In Tomography, we can write a vector to represent the amount of light absorbed by each bottle in a row or column. E.x. color (RGB values), pictures (set of pixels), solar cycles, Electrical circuit quantities Vectors Representing State # Vectors can be used to represent the state of a system, defined as follows: State: The minimum information you need to completely characterize a system at a given point in time, without any need for more information about the past of the system.\nState is a powerful concept because it lets us separate the past from the future. The state completely captures the present\u0026ndash; and the past can only affect the future through the present E.x: Consider modeling the dynamics of a quadrotor. The state of a quadrotor at a particular time can be summarized by its 3D position, angular position, velocity, and angular velocity, which can be represented as a vector .$\\vec q \\in \\mathbb{R^{12}}$, as illustrated: Special vectors # Zero \u0026amp; One Vector: # You can usually tell the size of the zero from the context: if .$\\vec x \\in \\mathbb{R}^{n}$ is added to .$\\vec 0$, then .$\\vec 0$ must also be in .$\\mathbb{R}^{n}$ $$\\vec 0 = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}$$ $$\\vec 1 = \\begin{bmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}$$ Standard Unit Vector: # A standard unit vector is a vector with all components equal to .$0$ except for one element, which is equal to .$1$. A standard unit vector where the .$i$th position is equal to .$1$ is written as .$\\vec e_i$ The system .$e_1, \\dots, e_n \\in \\mathbb{R}^{n}$ is called the standard basis in .$\\mathbb{R}^{n}$ $$\\vec e_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix},\\ \\vec e_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ \\vdots \\\\ 0 \\end{bmatrix},\\ \\vec e_n = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 1 \\end{bmatrix}$$ When talking about standard unit vectors in the context of states, we might also use the word “pure” to refer to such states. This is because they only have one kind of component in them. Other states are mixtures of pure states. Vector Operations # Addition # Must be same size and space (e.g. complex numbers, real numbers, etc.) Properties: Many of the properties of addition you are already familiar with when adding individual numbers hold for vector addition as well. For three vectors .$\\vec x, \\vec y, \\vec z \\in \\mathbb{R}^n$ (and .$\\vec 0 \\in \\mathbb{R}^n$), the following properties hold: Commutativity: $$\\vec x + \\vec y = \\vec y + \\vec x$$ Additive identity: $$\\vec x + 0 = \\vec x$$ Associativity: $$(\\vec x + \\vec y) + \\vec z = \\vec x + (\\vec y + \\vec z)$$ Additive inverse: $$\\vec x + (- \\vec x) = 0$$ Vector addition can be performed graphically as well: Scalar Multiplication # We can multiply a vector by a number, called a scalar: Scalar: a number. In mathematics and physics, scalars can be used to describe magnitude or used to scale things (e.g. cut every element of a vector in half by multiplying by 0.5, or flip the signs of each element in a vector by multiplying by −1).\nIn general, for a scalar .$\\alpha$ and vector .$\\vec x$, this looks like this: $$\\alpha \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} = \\begin{bmatrix} \\alpha x_1 \\\\ \\alpha x_2 \\\\ \\vdots \\\\ \\alpha x_n \\end{bmatrix} $$ We can obtain the zero vector by multiplying any vector by 0: $$0\\vec x = \\vec 0$$ Properties: Associative, distributive, and multiplicative identity hold \u0026ndash; trivial As an example, we can scale a vector .$\\vec x \\in \\mathbb{R}^{2}$ by 2 or -2: Vector Transpose # .$\\vec x$ is always a column vector, so to convert (represent) a row vector, we apply the transpose: .$\\vec x^T$ If the elements of the matrix .$A \\in \\mathbb{R}^{N \\times M}$ are .$a_{ij}$ The elements of .$A^T \\in \\mathbb{R}^{M \\times N}$ are .$a_{ji}$ Matrix transpose is not (generally) an inverse! $$\\vec x = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_N \\end{bmatrix}; \\vec x \\in \\mathbb{R}^{N \\times 1}$$ $$\\vec x^T = \\begin{bmatrix} x_1 \u0026amp; x_2 \u0026amp; \\dots \u0026amp; x_N \\end{bmatrix}; \\vec x^T \\in \\mathbb{R}^{1 \\times N} $$ The transpose of a row vector is a column vector Thus, the transpose of the transpose of a vector recovers the original vector. It is important to recognize that, though the transpose of a vector contains the same elements as the original vector, it is still a different vector! That is to say, for any vector .$\\vec x$ (with at least two components), .$\\vec x ^T \\neq \\vec x$ The transpose of a matrix has a very nice interpretation in terms of linear transformations, namely it gives the so-called adjoint transformation. Vector-Vector Multiplication # By convention, a row vector can only be multiplied by a column vector (and vice versa). Multiplication is valid only for specific matching dimensions! Width of the first, matches length of the second\u0026rsquo;s transpose e.x. given .$\\vec x, \\vec y \\in \\mathbb{R}^{N\\times 1}$ We can take the transpose of .$y$ and multiply by .$\\vec x$: This is also known as inner product or dot product Commutative for real numbers (this ceases to be true when we start working with complex numbers in 16B) $$\\vec y^T \\vec x = y_1 x_1 + y_2 x_2 + \\dots + y_N x_N = \\text{some scalar} \\in \\mathbb{R}^{1\\times1}$$ Alternatively, we can take .$\\vec x$ and multiply by the transpose of .$y$ Also known as outer product Do not commute! $$\\vec x \\vec y^T = \\text{some matrix} \\in \\mathbb{R}^{N \\times N}$$ Matrices # A collection of numbers in a rectangular form Or, given .$\\mathbb{R}^{M \\times N}$, a collection of .$M$ rows and .$N$ column vectors Remark: Matrices are often represented by capital letters (e.g. .$A$), $$A = \\begin{bmatrix} a_{11} \u0026amp; \\dots \u0026amp; a_{1N} \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots\\\\ a_{M1} \u0026amp; \\dots \u0026amp; a_{MN} \\end{bmatrix}$$ A matrix is said to be square if .$M=N$ (that is, if the number of rows and number of columns are equal). Relation between scalars, vectors, and matrices A vector is a degenerate matrix, that is, .$\\vec x \\in \\mathbb{R}^{n \\times 1}$ A scalar is a degenerate vector or matrix, that is, .$a \\in \\mathbb{R}^{1 \\times 1} $ Transpose: Just as we could compute the transpose of a vector by transforming rows into columns, we can compute the transpose of a matrix, .$A^T$ , where the rows of .$A^T$ are the (transposed) columns of .$A$ $$A^T = \\begin{bmatrix} a_{11} \u0026amp; \\dots \u0026amp; a_{M1} \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots\\\\ a_{1N} \u0026amp; \\dots \u0026amp; a_{NM} \\end{bmatrix}$$ Mathematically, .$A^T$ is the .$N\\times M$ matrix given by .$A^T_{ij}$ A square matrix is said to be symmetric if .$A = A^T$ , which means that .$A_{ij} = A_{ji}$ for all .$i, j$. Special Matrices # Zero Matrix: Trivial Identity Matrix: Square matrix whose diagonal elements are .$1$ and whose off-diagonal elements are all .$0$ $$I_3 = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix}$$ Note that the column vectors (and the transpose of the row vectors) of an .$N \\times N$ identity matrix are the unit vectors in .$ \\mathbb{R}^{N}$. The identity matrix is useful because multiplying it with a vector .$\\vec x$ will leave the vector unchanged: .$I \\vec x = \\vec x$. In fact, we will see that multiplying a square matrix by an identity matrix of the same size will yield the same initial matrix: .$AI = IA = A$ Column vs Row # The interpretation of rows and columns that come from the system-of-linear-equations perspective of doing experiments. There, each row of the matrix represents a particular experiment that took one particular measurement. For a given row, the coefficient entries represent how much the corresponding state variable affects the outcome of this particular experiment. In contrast, the columns represent the influence of a particular state variable on the collection of experiments taken together. These perspectives come in handy in interpreting matrix multiplication.\nMatrix Operations # Matrix Addition # Two matrices of the same size can be added together by adding their corresponding components. For instance, we can add two matrices A and B (both in .$ \\mathbb{R}^{m \\times n}$) as follows: Matrix-Vector Multiplication # Given .$A \\in \\mathbb{R}^{M \\times N}, \\vec x = \\mathbb{R}^{N\\times 1}$, we end with some vector .$\\in \\mathbb{R}^{M\\times 1}$ Visual View Matrix-Matrix Multiplication # Matrix-Matrix multiplication involves multiplying each row vector in .$A$ with each column vector in .$B$, starting from the top row of matrix .$A$ and leftmost column of matrix .$B$. Effectively, the left matrix is multiplied by each column vector in the second matrix to produce a new column of .$AB$ Given .$A \\in \\mathbb{R}^{M \\times N},\\ B = \\mathbb{R}^{N\\times L}$, we end with some matrix .$\\in \\mathbb{R}^{M \\times L}$ We can also interpret the .$A \\vec x$ product in the context of .$A$’s columns: Matrix multiplication does not commute! That is .$A \\times B \\neq B \\times A$ In fact, both quantities can only be calculated if the number of rows in .$A$ equals the number of columns in .$B$ and the number of rows in .$B$ equals the number of columns in .$A$. Matrix Multiplication is Associative Fun fact: Computers have to do so many multiply and add operations that it\u0026rsquo;s optimized at a processor level (leaned about this is 61C) "},{"id":5,"href":"/cogsci-c100/perception/","title":"2: Perception","section":"CogSci C100","content":" Definition of sensation and perception # As humans, we are cognitive beings who\u0026hellip; Acquire information about the world around us (perception) Integrate that information with prior knowledge from our stored memory Store that knowledge in our memory so we can use it later to help us achieve our goals First step in this process of acquiring knowledge about the world involves sensation and perception Sensation: process by which our sensory receptors and nervous system receive stimulus energies from the environment and transduce them into neural impulses (transduction). The inherent stimuli. Objective Perception: process of interpreting and organizing sensory information through use of previous knowledge. What gives stimuli meaning. Subjective. Early models of object perception # Lots of machines are built on these old theories The models aren\u0026rsquo;t respected nowadays, but had useful tid-bits of information (for cogsci, ML, etc) Template matching model # Template matching model: object perception involves a comparison of the stimulus with set of templates or specific patterns stored in memory Static, unchanging Problem: cannot account for complexity and flexibility of object recognition e.g. individual differences in handwriting Feature analysis model # Feature-analysis model: discrimination of objects is based on small number of distinct characteristics of stimuli Are these two the same letter?: G M P R People are faster at deciding whether G and M are different than P and R because there are fewer similarities Supported by neurological evidence: some neurons respond only to horizontal lines, others to diagonals, etc. Problem: Cannot explain recognition of complex objects with features that move and distort (e.g., horse or kangaroo) Recognition-by-components model # Recognition-by-components model: view that an object is represented as an arrangement of simple 3-D shapes called geons Six main geons above Cup/pail composed of cylinder and curved tube geons in a particular arrangement\nEasier to tell the object from the rightmost column versus the center, despite the latter containing more lines\nDavid Marr’s Model of Visual Processing # Not on Exam\nThe image is then transformed into a 3-D sketch in which the the axes of symmetry and elongation link the object parts Symmetry axis: line that divides an object into mirror image halves Elongation axis: line defining direction along which main bulk or mass of a shape is distributed The 3-D sketch is object-centered – the object’s parts are described relative to one another and are linked on the basis of shared properties and axes This solves the object constancy problem, allowing recognition of an object presented in different orientations and under different conditions, e.g., lighting changes Prototype model # Prototype model: object perception involves a comparison of the stimulus with ideal, abstract example People are faster at identifying sparrow as a bird than penguin One of the most famous models in all of cognitive psychology (and developed at Berkeley!) It has been hypothesized that our sensory systems act primarily as a selective filtering mechanism Prototypes more easily pass this filter This filter sorts things according to a limited number of variables (e.g., warm, unpleasant, green) out of which we construct our world But prototype theory suggests that our minds can also perceive objects in a very different way\u0026hellip; That which is essential is invisible to the eye. – de Saint-Exupery\nAlternative modes of perception # Alternative modes of perception: Mindfulness is largely about seeing the “suchness” of things, that is, seeing things directly without conceptual filters What assumptions might you make about this woman if you were told she is from New England? from California? Our preconceived notions prevent us from seeing the real person in front of us Stereotyping If the doors of perception were cleansed, everything would appear to man as it is, infinite.\nTo see a World in a Grain of Sand, And a Heaven in a Wildflower, Hold Infinity in the palm of your hand, And Eternity in an hour. \u0026ndash; Blake\nNeural Networks # On Quiz 2!\nArtificial Neural Networks in Pattern Recognition Human neurons Many different neurons connect to the dendrites of each neuron Some produce excitatory effect; others produce inhibitory effect There are also different levels of intensity of these effects Around a thousand connections are connected to each neurons If the activation of the neuron reaches a certain minimum threshold, the neuron will fire 16A Notes Because circuit analysis translates to a wide range of fields, we can model many physical systems as electrical circuits, often gaining insight about the system. You may have heard of neural networks, an important machine learning tool that can be used to “learn” tasks such as image and voice recognition from examples instead of explicit programming. Neural networks are modeled after biological neural networks, which are fundamentally circuits operating on electrical signals within a brain:\nIn a general sense, studying circuits provides you with the conceptual and mathematical tools needed to analyze such networks. More broadly, circuit concepts are relevant to understanding network analysis and signal flows in systems, which can be applied to areas ranging from transportation analysis to social network analysis. ( from EECS16A Note0)\nArtificial neural networks (ANN) The nodes or neurons are organized into layers in much the same way that human neural networks are The weights attached to the connections between pairs of units in adjacent layers determine the overall behavior of the network This is similar to the way in which excitatory and inhibitory neurons of various strengths connect to a particular neuron in human neural networks The bias term indicates what the weighted sum needs to be before the node/neuron will activate This is similar to the threshold necessary for activation of a neuron in human neural networks An artificial neural network is an interconnected group of nodes, inspired by a simplification of neurons in a brain. Here, each circular node represents an artificial neuron and an arrow represents a connection from the output of one artificial neuron to the input of another.\nEx: How might a computer recognize a “9” using neural networks? # There is huge variety of ways in which people write 9’s To simplify things, we can represent the “9” by decomposing it to a grid of 28 x 28 pixels of varying shades of gray (between 0 and 1) First (input) layer of network Starts with bunch of neurons or nodes corresponding to an array of 28 x 28 pixels in the image Each node holds a number that represents the grayscale value of the corresponding pixel, ranging from 0 for black to 1 for white This is the neuron’s activation level Activations in one layer bring about activations in the next layer, which in turn bring about activations in the next layer\u0026hellip; This is loosely analogous to how, in biological networks of neuron, some groups of neurons cause other neurons to fire Second layer (or first “hidden layer”) Each neuron in the second layer might pick up on whether there is an edge in one particular region You assign a weight to each one of the connections between a particular neuron in the second layer and the neurons in the first layer Then you take all the activations from the first layer and compute their weighted sum according to the weights Could make the weights associated with almost all of the pixels 0 except for some positive weights in target region To really pick up on whether there is an edge here, could also have some negative weights associated with the surrounding pixels Sum is largest when those middle pixels are bright but surrounding pixels are darker But maybe you don’t want the neuron to light up anytime the sum is bigger than zero \u0026ndash; maybe you only want it to be active when the sum is bigger than say 10 So you add in some other number (the bias), like -10, to the weighted sum The bias tells you how high the weighted sum needs to be before the neuron starts getting meaningfully active The connections between the other layers also have weights and biases associated with them Third layer (or second “hidden layer”) When we recognize digits, we piece together various components e.x: A “9” has a loop near the top and a line on the right whereas an “8” has a loop on the top and one below Each neuron in the third layer corresponds to one of these subcomponents e.x: A particular neuron in the third layer might be activated by any generally loopy pattern toward the top These subcomponents are made up of the various edges from the second layer Last (output) layer Has 10 neurons, each representing one of the digits The activation in these neurons – some number between 0 and 1 – represents how much the system thinks a given image corresponds with a given digit Learning is about getting the computer to find a setting for all of the different weights and biases so that it will actually solve the problem at hand This is done through backpropagation Learning in Neural Nets: Backpropagation # ANNs can compute any function that can be computed by a digital computer However, it was not until the emergence of backpropagation learning algorithm that it became possible to train multilayer neural networks The strength or weight of the connections between neurons in adjacent layers varies: neural networks learn by modifying these weights Learning algorithms that are programmed into the ANN change the weights of the connections between pairs of neurons in adjacent layers in order to reduce the “mistakes” that the network makes The basic idea is that each hidden unit connected to an output unit bears a degree of “responsibility” for the error of that output unit If the activation level of an output unit is too low, then the weight between the output unit and each hidden unit connected to it is increased to decrease the error The network then assigns error levels to the next layer of hidden units, so the error is propagated back down through the network until the input layer is reached tl;dr: neural networks have to \u0026rsquo;learn\u0026rsquo; by adjusting stimuli weights. When a network gets an answer wrong, it has to recursively pop back each layer and adjust the corresponding weights (increasing correct value weights, decreasing activated incorrect value weights) Other Neural Networks Q\u0026amp;A # Q: How many neurons should there be in each hidden layer?\nA: There are a number of empirically-derived rules-of-thumb. Of these, the most commonly relied on is “the optimal size of the hidden layer is usually between the size of the input and size of the output layers” Q: How many hidden layers are needed? Are more layers better?\nA: No. Situations in which performance improves with additional hidden layers are very few. One hidden layer is sufficient most of the time. Q: Why are more hidden layers not necessarily better?\nA: Increasing the number of hidden layers much more than the sufficient number will cause the network to overfit the training set. It will learn the training data, but it won’t be able to generalize to new unseen data. Overfit Dataset: Top-down processing in object recognition # Limitations of models of object perception discussed above: assumes, in theory, perception is objective and accurate; in real life, that is often not the case\u0026hellip; What we perceive, the way we perceive, is not always what would be predicted by these models Our concepts, expectations, and beliefs play a much bigger role in perception than we usually realize Perception engages both top-down and bottom-up processing Bottom-up processing: analysis of information coming from stimuli through sensory receptors Object perception as combination of stimulus information from sensory receptors Emphasizes the importance of information coming from the outside world Top-down processing: information processing guided by higher-level processes, such as our beliefs, expectations, and memories Our knowledge, beliefs about the world inform our perceptions Emphasizes the importance of information coming from our minds Note: we use both throughout everyday situations, rarely exclusively either or Models can almost be sort of categorized/grouped by which of the two processing model they put emphasis on E.x. you see a water bottle on your desk You know it\u0026rsquo;s a water bottle since it\u0026rsquo;s physically there. You can see, and perhaps touch it You know it\u0026rsquo;s a water bottle because of it\u0026rsquo;s features which have meaning to you through living in the modern world (perhaps owning a water bottle yourself!) If you were an alien, or a homosapien from a very long time ago, you wouldn\u0026rsquo;t know that the object was a water bottle (among other things\u0026hellip;) Optional: Deductive versus Inductive Reasoning Deductive reasoning works from the more general to the more specific, i.e., “top-down” approach. In deductive reasoning there is usually a first premise, then a second premise (both of which are proven through observations), and finally an inference. Ex: All men are mortal. Smerdley is a man. Therefore, Smerdley is mortal. Inductive reasoning works the other way, moving from specific observations to broader generalizations and theories, i.e., “bottom-up” approach Inductive reasoning extracts a likely (but not certain) premise from specific and limited observations. Ex: I have a bag of many coins, and I’ve pulled 10 at random and they’ve all been pennies, therefore this is probably a bag full of pennies. “Objective reality” is often not as objective as we think\u0026hellip; # A fool sees not the same tree that a wise man sees. \u0026mdash; William Blake\nReversible figures (e.g., Necker cube; vase/profiles)\nAmbiguous figures (e.g., old woman/young woman \u0026ndash; also old people tend to see the old figure first, and vis-versa)\nEffect of expectations on perception Perceptual set brain teasers: SOAK FOLK CROAK Context effects (e.g. bea(r|n), Presidential illusion) # Effects of expectations, experience, emotional patterns, and beliefs on perception # Effects of Prior Experience on Perception Children who have been physically abused are significantly more likely to misperceive a fearful face as angry (Pollak) Cultural effects on perception What is above the woman’s head? Is this an indoor or outdoor scene? (Gregory and Gombrich, 1973) Rorscharch and Thematic Apperception Test (TAT) When angry, people more often perceive neutral objects as guns (Baumann \u0026amp; DeSteno, 2010) Effect of beliefs/preconceived notions on perception Rosenhan study on effects of psychological labeling Self-fulfilling prophecies # Self-fulfilling prophecies: People generally think that it is our experiences and perceptions that create our beliefs, but often, it is actually our beliefs that create our experiences and perceptions Our beliefs and expectations influence others’ behavior The Pygmalion effect: study found that students who were (randomly) labeled intellectual “spurters” showed significantly greater gains in IQ and academic performance after 8 months than controls Follow-up: If teacher believed that girls learn to read faster than boys, they did Children who were told they were neat and tidy became more neat and tidy than those who were told they should be neat and tidy Follow-up: children who are told that they are good at math showed greater improvements in math scores than those who were told that they should try to become good at math Those who over-idealize romantic partners as having many virtues and few faults tend to have happier and longer-lasting relationships (Miller, Niehuis, \u0026amp; Huston, 2006) Moreover, the partners who are over-idealized tended to develop those traits over time! (Sandra Murray) People live up to their expectations \u0026ndash; we tend to bring out what we focus on Our beliefs and expectations influence our own behavior Study by Mark Snyder found that when a man was led to believe that a woman found him attractive, she was more likely to act as if she did \u0026ldquo; Gus Hansen refused to acknowledge the odds and the odds disappear\u0026rdquo; Assume a virtue if you have it not. – Shakespeare\nPerceptual Constancies # Perceptual constancy: perceiving objects as unchanging (having consistent lightness, color, shape, and size) even as illumination and retinal images change Many visual illusions result from the overuse of strategies employed to achieve perceptual constancy Is Tile A or Tile B darker or are they the same color?\nIllusion results from visual system’s attempt to maintain lightness constancy: we perceive an object as having a constant color, even if changing illumination alters the wavelengths reflected by the object Shape constancy: we perceive the form of familiar objects as constant even while our retinal images of them change A door casts an increasingly trapezoidal image on our retinas as it opens, yet we still perceive it as rectangular Müller-Lyer illusion: Is line AB or line BC longer? Size-distance constancy: Our brains are used to perceiving angles as corners that are near or far away and sees the inward-facing corners as more distant and therefore smaller Are the two parallelograms the same size and shape? Ponzo illusion: Which line is longer? Moon illusion: Does the moon appear larger near the horizon or when it is high in the sky? When the moon is near the horizon we perceive it to be farther away from us than when it is high in the sky, but since the moon is actually the same size, our minds make it look bigger when it is near the horizon to compensate for the increased distance The Magical Kingdom of Salt In the Salar de Uyuni of Bolivia, the world’s largest salt flat, with no other objects in sights, the human eye loses its ability to establish a proper field of depth. The result is some bizarre pictures. Effects of color in marketing # Assume that you are considering buying condoms You enter a store and notice that the store doesn’t carry all the brands you may be familiar with, so you’re going to have to make your choice based on the product package alone You are really interested in finding a brand that is considered Durable, strong, and well built (“rugged” condition) OR Classy, attractive, and refined (“sophisticated” condition) Which would you choose? Match the colors with the following Sincerity: white, yellow.$^1$ Excitement: red, orange.$^1$ Competence: blue Sophistication: black, pink, purple Ruggedness: brown .$^1$Marginally significant Neurological disorders of visual perception # Face perception and prosopagnosia # Face recognition is “special” Single-cell recordings of monkeys show activation of particular cells in lower temporal only when full-face photos of other monkeys are presented Recognition accuracy for faces and houses: parts vs. whole Study (Tanaka and Farah, 1993) in which participants were shown series of faces with person’s name and series of houses with owner’s name Later on recognition test, they showed greater recall of Parts of houses but Whole faces Do people tend to perceive men or women more in “parts”? Women (Gervais, Vescio , Forster et al., 2012) Prosopagnosia: failure to recognize particular people by the sight of their faces After stroke, sheep rancher could not recognize people but could recognize sheep Someone would walk in the room and he wouldn\u0026rsquo;t be able to tell if it was the wife/neighbor/robber/etc Note: the eyes also play a special role in perception 70-90% of famous portrait paintings sampled from the last five centuries have an eye at or within 5% of the painting’s exact centerline (Christopher W. Tyler)\nEvery man indicates in his eye the exact indication of his rank. – Emerson\nModular Processing # Visual illusions suggest that the mind is at least in part modular (Jerry Fodor) That is, it is not solely organized in terms of faculties, such as memory and attention, that can process any type of information Rather, there are specialized information-processing modules that Respond automatically Cannot be “switched off” Modular processes are usually characterized by\u0026hellip;\nFixed neural architecture It is sometimes possible to identify determinate regions of the brain that are associated with particular types of modular processing e.x: fusiform face area for face recognition Specific breakdown patterns Modules can fail in highly determinate ways, which provide clues on the form and structure of processing e.x: prosopagnosia Other Neurological Disorders Related to Visual Perception # Visual agnosia: inability to recognize/identify visual objects despite relatively good visual perception Usually due to damage in occipital or temporal lobes “Mr. P” in Oliver Sacks’ Man Who Mistook His Wife for a Hat Man with agnosia puzzling over a picture of a cow suddenly found himself making alternating up-and-down movements with fists. He looked down at his hands and said, “Oh, a cow!” Due to some error between vision and verbal communication Visual neglect syndrome or unilateral spatial neglect: Tendency to ignore – or to be unaware of – information on one half of visual field, usually the left side Typically occurs after damage (e.g., stroke) to right hemisphere, particularly damage to the parietal and frontal lobes Relatively common, easy to test for Patients are asked to bisect each line. Their markings are typically skewed to the right, as if they do not see the leftmost segment\nPatients are asked to draw from memory or to copy an illustration (Driver \u0026amp; Vuilleumier, 2001)\nHouse Experimenter: Are the two houses the same or different?\nPatient: The same.\nExperimenter: Which house would you prefer to live in?\nPatient: The left house.\nCapgras syndrome: characterized by belief that family and/or friends are imposters Damage to pathway between visual cortex and amygdala, which regulates emotions Emotional “glow” that we normally feel around people we are close to is missing Ramachandran argues that this emotional “glow” is, to a large extent, what gives us a sense of continuity in our relationships Classified as some kind of schizophrenia Functional blindness (conversion disorder): unexplained vision loss with no organic basis Cambodian women who had witnessed horrible war atrocities became either partially or wholly blind Impairs primarily body functions / processes Psychological defense mechanism Blindsight: vision without awareness Blindness resulting from damage to visual cortex When presented with various shapes like circles and square, or photos of faces of men and women, patient could not tell (or guess) what his eyes were gazing at However, when shown pictures of people with angry or happy faces, he was able to guess the emotions expressed, at a rate far better than chance Patients are also able to correctly “guess” the identity or location of particular objects Patients report that they get a “gut” feeling that allows them to perform these tasks Blindsight patient was able to meander around all the clutter in a hallway that he was told was empty (Weiskrantz)\nA second pathway of visual perception may account for this phenomenon Two pathways of visual perception # Study looked at speed with which people were able to find a specific hidden object among a group of similar objects Participants were instructed to Passively allow the target to just “pop” into their minds OR Actively direct their attention to the target Participants in Passive Group 1 outperformed those in Group 2 (Smilek, Enns, Eastwood et al., 2006) Targets Look for the circle with just one gap, and say whether the gap is on the left or the right Use “relax” strategy, then try active search strategy Proposed explanation: Participants who were basically told to relax and go with their gut instinct used a secondary pathway of visual perception that Does not go through the visual cortex Instead simply makes a very short loop through the limbic system: the emotional, instinctual center of the brain Research evidence for existence of two pathways: Auditory cortex of rats was destroyed, then rats were exposed to tone paired with an electric shock Rats quickly learned to fear tone, though they could not “hear” it! Explanation: the sound took the direct route from ear to thalamus to amygdala, bypassing all higher avenues (Joseph Ledoux) Development of perception # Adults who were born blind and later gained vision through newly-developed surgical interventions (e.g., cataract surgery) usually have some difficulty recognizing objects At age 3, Mike May lost his vision in an explosion. Decades later, a new cornea restored vision to his right eye. Unfortunately, although signals were now reaching his visual cortex, it lacked the experience to interpret them May could not recognize expression, or faces, apart from features such as hair Yet he can see an object in motion Ended up committing suicide because he found himself in a world he didn\u0026rsquo;t (couldn\u0026rsquo;t) understand or comprehend There is a critical period for normal sensory and perceptual development Kittens reared in a cylinder with only vertical black and white stripes later had difficulty perceiving horizontal bars Kitten would play with rod only when it was held upright As if they couldn\u0026rsquo;t see the horizontal rod "},{"id":6,"href":"/eecs-16a/2/","title":"2: (In)dependence \u0026 Circuit Analysis","section":"EECS 16A","content":" 02-01: Linear (in)dependance, Matrix Transformations # Slides Note 3 Recall the simple tomography example from Note 1, in which we tried to determine the composition of a box of bottles by shining light at different angles and measuring light absorption. The Gaussian elimination algorithm implied that we needed to take at least 9 measurements to properly identify the 9 bottles in a box so that we had at least one equation per variable. However, will taking any 9 measurements guarantee that we can find a solution? Answering this question requires an understanding of linear dependence. In this note, we will define linear dependence (and independence), and take a look at what it implies for systems of linear equations.\nLinear Dependence # Linear dependence is a very useful concept that is often used to characterize the “redundancy” of information in real world applications. Closely tied to the idea of free and basic variables as we’ve already seen We will give three (equivalent) definitions of linear dependence: A set of vectors .$\\{\\vec v_1, \\dots \\vec v_n \\}$ is linearly dependent if there exists scalars .$\\alpha_1, \\dots, \\alpha_n$ such that .$\\alpha_1 \\vec v_1 + \\dots + \\alpha_n \\vec v_n = \\vec 0$ and not all .$\\alpha_i$ are equal to zero. This combination of all-zero scalars has a special name: the \u0026ldquo;trivial solution.\u0026rdquo; A set of vectors .$\\{\\vec v_1, \\dots \\vec v_n \\}$ is linearly dependent if there exists scalars .$\\alpha_1, \\dots, \\alpha_n$ and an index .$i$ such that .$\\vec v_i = \\sum_{j\\neq i} \\alpha_j \\vec v_j$. In other words, a set of vectors is linearly dependent if one of the vectors could be written as a linear combination of the rest of the vectors A set of vectors is either linearly dependent or linearly independent. More specifically, consider the sum in the first definition. If there is a solution to satisfy this equation other than to make all the scalars .$\\alpha_1 = \\dots = \\alpha_n = 0$, (that is, a nontrivial solution) then the vectors are linearly dependent. Why three (equivalent) definitions? Because each is useful in different settings. It is often easier mathematically to show linear dependence with definition (1) since we don’t need to try to “single out” a vector to get started with the proof. (2) gives us a more intuitive way to talk about redundancy. If a vector can be constructed from the rest of the vectors, then this vector does not contribute any information that is not already captured by the other vectors. Proof of equivalency Linear Independence # From the first definition of linear dependence we can deduce that a set of vectors .$\\{\\vec v_1, \\dots, \\vec v_n \\}$ is linearly independent if .$\\alpha_1 \\vec v_1 + \\dots + \\alpha_n \\vec v_n = \\vec 0$ implies .$\\alpha_1 = \\dots = \\alpha_n = 0 $ A set of vectors is linearly independent if it is not linearly dependent. E.x. any two vectors that are multiples of one another are dependent Systems of Linear Equations # Recall that a system of linear equations can be written in matrix-vector form as .$A\\vec x = \\vec b$, where .$A$ is a matrix of variable coefficients, .$\\vec x$ is a vector of variables, and .$\\vec b$ is a vector of values that these weighted sums must equal. We will show that just looking at the columns or rows of the matrix .$A$ can help tell us about the solutions to .$A\\vec x = \\vec b$. Theorem 3.1 # If the system of linear equations .$A\\vec x = \\vec b$. has an infinite number of solutions, then the columns of .$A$ are linearly dependent If the system has infinite number of solutions, it must have at least two distinct solutions .$\\vec x_1, \\vec x_2$ which must satisfy $$A\\vec x_1 = \\vec b$$ $$A\\vec x_2 = \\vec b$$ Subtracting the first equation from the second equation, we have $$A (\\vec x_2 - \\vec x_1) = \\vec 0$$ Define alpha as \u0026hellip;\n$$\\vec \\alpha = \\begin{bmatrix} \\alpha_1 \\\\ \\vdots \\\\ \\alpha_n \\\\ \\end{bmatrix} = \\vec x_2 - \\vec x_1$$ Because .$\\vec x_1, \\vec x_n$ are distinct, not all .$\\alpha_i$\u0026rsquo;s are zero. Let the columns of .$A$ be .$\\vec a_1, \\dots \\vec a_n$. Then, .$A \\vec \\alpha = \\sum^n_{i=1} \\alpha_i \\vec a_i = \\vec 0$. By definition, the columns of .$A$ are linearly dependent. The sum term says that, in other words, matrix-vector multiplication is a linear combination of columns: Theorem 3.2 # If the columns of .$A$ in the system of linear equations .$A\\vec x = \\vec b$ are linearly dependent, then the system does not have a unique solution. Start by assuming we have a matrix A with linearly dependent columns $$A = \\begin{bmatrix} | \u0026amp; | \u0026amp; \u0026amp; | \\\\ \\vec a_1 \u0026amp; \\vec a_2 \u0026amp; \\dots \u0026amp; \\vec a_n \\\\ | \u0026amp; | \u0026amp; \u0026amp; | \\\\ \\end{bmatrix}$$ By the definition of linear dependence, there exist scalars .$\\alpha_1, \\dots, \\alpha_n$ such that .$\\alpha_1\\vec a_1 + \\dots + \\alpha_n \\vec a_n = \\vec 0$ where not all of the .$\\alpha_i$’s are zero. We can put these αi’s in a vector: $$\\vec \\alpha = \\begin{bmatrix} \\alpha_1 \\\\ \\vdots \\\\ \\alpha_n \\\\ \\end{bmatrix}$$ By the definition of matrix-vector multiplication, we can compactly write the expression above: $$A\\vec \\alpha = \\vec 0$$ $$\\text{where } \\vec \\alpha \\neq \\vec 0$$ Recall that we are trying to show that the system of equations .$A\\vec x = \\vec b$ does not have a unique solution. We know that systems of equations can have either zero, one, or infinite solutions. If our system of equations has zero solutions, then it cannot have a unique solution, so we don’t need to consider this case. Now let’s consider the case where we have at least one solution, .$\\vec x$: Therefore, .$\\vec x + \\vec \\alpha$ is also a solution to the system of equations! Since both .$\\vec x$ and .$\\vec x + \\vec \\alpha$ are solutions, and .$\\vec \\alpha \\neq \\vec 0$, the system has more than one solution. We’ve now proven the theorem. $$A \\vec x = \\vec b$$ $$A \\vec x + \\vec 0= \\vec b$$ $$A \\vec x + A \\vec \\alpha = \\vec b$$ $$A (\\vec x + \\alpha) = \\vec b$$ Note that we can add any multiple of .$\\alpha$ to .$\\vec x$ and it will still be a solution – therefore, if there is at least one solution to the system and the columns of .$A$ are linearly dependent, then there are infinite solutions.\nIntuitively, in an experiment, each column in matrix .$A$ represents the influence of each variable .$x_i$ on the measurements. If the columns are linearly dependent, this means that some of the variables influence the measurement in the same way, and therefore cannot be disambiguated. See page five for good example. Implications: # This result has important implications to the design of engineering experiments. Often times, we can’t directly measure the values of the variables we’re interested in. However, we can measure the total weighted contribution of each variable. The hope is that we can fully recover each variable by taking several of such measurements. Now we can ask: “What is the minimum number of measurements we need to fully recover the solution?” and “How do we design our experiment so that we can fully recover our solution with the minimum number of measurements?”\nConsider the tomography example. We are confident that we can figure out the configuration of the stack when the columns of the lighting pattern matrix .$A$ in .$A\\vec x = \\vec b$ are linearly independent. On the other hand, if the columns of the lighting pattern matrix are linearly dependent, we know that we don’t yet have enough information to figure out the configuration. Checking whether the columns are linearly independent gives us a way to validate whether we’ve effectively designed our experiment.\nRow Perspective # Optional! Intuitively, each row represents some measurement If the number of measurements taken is at least the number of variables and we cannot completely determine the variables, then at least one of our measurements must be redundant (it doesn’t give us any new information). This intuition suggests that the number of variables we can recover is equal to the number of unique measurements, or the number of linearly independent rows \u0026ndash; this formal proof will come in a later note when we talk about rank. Now have two perspectives: in the matrix, each row represents a measurement, while each column corresponds to a variable. Therefore, if the columns are linearly dependent, then we have at least one redundant variable. From the perspective of rows, linear dependency tells us that we have one or more redundant measurements. Span # Span of the columns of .$A$ is the set of all linear combinations of vectors .$\\vec b$ such that .$A\\vec x = \\vec b$ has a solution .$\\exists \\vec x$ s.t. .$A \\vec x = \\vec b \\Longrightarrow \\vec b \\in \\text{span(cols}(A))$ That is, the set of all vectors that can be reached by all possible linear combinations of the columns of .$A$ Formally, .$\\text{span}(\\vec v_1, \\dots, \\vec v_N) = \\bigg\\{\\sum_{i=1}^N \\alpha_i \\vec v_i\\ |\\ \\alpha_i \\in \\mathbb{R},\\ \\vec a_i \\in \\mathbb{R}^{M}\\bigg\\}$ A set of vectors is linearly dependent if any one of the vectors is in the span of the remaining vectors. That is, if any one of the vectors could be represent as the combination of the remaining vectors (that is, it\u0026rsquo;s in the span of the others) On the other hand, if each vector adds another dimension to the span (contains novel information) then they\u0026rsquo;re said to be linearly independent span, range, and column space of .$A$ all refer to the span of the columns of .$A$ Two Examples e.x. what is the span of the cols of .$A = \\begin{bmatrix} 1 \u0026amp; 1 \\\\ 1 \u0026amp; -1\\\\ \\end{bmatrix}$? $$\\text{span(cols of A)} = \\bigg\\{ \\vec v\\ |\\ \\vec v = \\alpha \\begin{bmatrix} 1\\\\ 1\\\\ \\end{bmatrix} + \\beta \\begin{bmatrix} 1\\\\ -1\\\\ \\end{bmatrix}; \\alpha, \\beta \\in \\mathbb{R}\\bigg\\} = \\mathbb{R}^{2}$$ e.x. what is the span of the cols of .$A = \\begin{bmatrix} 1 \u0026amp; -1 \\\\ 1 \u0026amp; -1\\\\ \\end{bmatrix}$? $$\\text{span(cols of A)} = \\bigg\\{ \\vec v\\ |\\ \\vec v = \\alpha \\begin{bmatrix} 1\\\\ 1\\\\ \\end{bmatrix}; \\alpha \\in \\mathbb{R}\\bigg\\} = \\text{line } (x_1 = x_2)$$ 02-03: Intro to Circuit Analysis # Slides Note 11 A, B Our ultimate goal is to design systems that solve people’s problems. To do so, it’s critical to understand how we go from real-world events all the way to useful information that the system might then act upon. The most common way an engineered system interfaces with the real world is by using sensors and/or actuators that are often composed of electronic circuits; these communicate via electrical signals to processing units, which are also composed entirely of electronic circuits. In order to fully understand and design a useful system, we will need to first understand Electrical Circuit Analysis.\nThere are four main steps involved when designing information devices and systems Analog World Sensor Input Data Processing Actuation (16B) Tiny Bit of Solid-State Physics # Conductors have lots of electrons Move around very easily E.x. copper, gold, silver, water Conductors have lots of electrons But they are at an energy level where they need to be given some energy level (e.x 1 eV) to move E.x. solar cell, diodes Insulators do not let electrons pass through them E.x. capacitors have a big insulator in the middle, that is, current only goes through a capacitor when the magic smoke is released Electrical Quantities # Quantity Symbol Units What Current .$I$ Amps, .$A$ Flow of charges (e.g. electrons) in the circuit due to a potential difference Voltage .$V$ Volts, .$V$ Potential energy (per charge) between two points in the circuit Resistance .$R$ Ohms, .$\\Omega$ Material’s tendency to resist the flow of current. Voltage .$\\pm$ depends on reference point Voltage, or electric potential, is only defined relative to another point (mountain/height analogy). Similarly, in circuits, we will frequently define a reference point, called ground, against which other voltages can be measured. Current .$\\pm$ depends on direction Circuit Diagram # Collection of elements, where each element has some voltage across it and some current through it Two main elements: Notes: Points where elements meet Junctions: Points where different material meet Voltage = difference of two potential Basic Circuit Elements # Wire # The most common element in a schematic is the wire, drawn as a solid line The .$IV$ relationship for a wire is: .$V_\\text{elem} = 0$: A wire is an ideal connection with a constant, zero voltage across it. .$I_\\text{elem} = ?$: The current through a wire can take any value, and is determined by the rest of the circuit. Resistor # The .$IV$ relationship of a resistor is called Ohm’s Law: .$V_\\text{elem} = I_\\text{elem} R $: The voltage across a resistor is determined by Ohm’s Law. .$I_\\text{elem} = V_\\text{elem} / R$: The current through a resistor is determined by Ohm’s Law. Limit check\nThe slope is proportional to .$R^{-1}$, that is, the larger the resistance the lower the slope .$\\lim_{R \\to 0}$ results in an wire (above) .$\\lim_{R \\to \\infty}$ results in an open circuit (below) Open Circuit: # This element is the dual of the wire. .$V_\\text{elem} = ?$: The voltage across an open circuit can take any value, and is determined by the rest of the circuit. .$I_\\text{elem} = 0$: No current is allowed to flow through an open circuit; always zero. Voltage Source: # A voltage source is a component that forces a specific voltage across its terminals. The + and − sign indicates which direction the voltage is pointing. The voltage difference between the “+” terminal and the “−” terminal is always equal to Vs, no matter what else is happening in the circuit. .$V_\\text{elem} = V_S$ \u0026ndash; The voltage across the voltage source is always equal to the source value. .$I_\\text{elem} = ?$ \u0026ndash; The current through a voltage source is determined by the rest of the circuit. Current Source: # A current source forces current in the direction specified by the arrow indicated on the schematic symbol. The current flowing through a current source is always equal to Is, no matter what else is happening in the circuit. Note the duality between this element and the voltage source. .$V_\\text{elem} = ?$ \u0026ndash; The voltage across a current source is determined by the rest of the circuit. .$I_\\text{elem} = I_S$ \u0026ndash; The current through a current source is always equal to the source value. Rules for Circuit Analysis # See also: 26.3 Kirchhoff’s Rules Kirchhoff’s Current Law (KCL) # Node: A place in a circuit where two or more of the above circuit elements meet The net current flowing into/out-of any node is zero: $$(-i_1)+(-i_2)+i_3 = 0$$ That is, the current flowing into a node must equal the current flowing out of that node $$ i_1+i_2=i_3$$ Kirchhoff’s Voltage Law (KVL) # The sum of voltages across the elements connected in a loop must be zero Mathematically, KVL states that: $$\\sum_\\text{loop} V_k = 0$$ $$\\Longrightarrow V_A - V_B - V_C = 0$$ Height Analogy # If you walk in a circle (a loop) so that you end up back where you started, than your total change in elevation must be zero, no matter how much you go up or down. If you walk in a line, ending up somewhere different, than your total change in elevation is equal to the sum of all of the elevation changes along the way. Real World Analogy The sum of voltages across the elements connected in a loop must be equal to zero “what goes up must come down” If the arrow corresponding to the loop goes into the “+” of an element, we subtract the voltage across that element. We went “downhill” from higher voltage to lower voltage so we lost “elevation.” If the arrow goes into the “-” of an element, we add the voltage across that element This is like going “uphill” Ohm’s Law and Resistors # $$V_\\text{element} = I_\\text{element}R$$\nThe unit of .$R$ is Volts/Amperes, called Ohms .$\\Omega$. See also: 25.3 Ohm’s Law: Resistance and Resistors "},{"id":7,"href":"/e-29/2/","title":"2: Cutting-based Processes \u0026 Other Subtractive Processes","section":"Engineering 29","content":" 02-01 Cutting-based Processes # Modeling of material cutting # Chips are created and separated from the work by plastic deformation, i.e. shearing, of the work. Sheer plane is created by the tool (teal segment) Throughout the shear plane, the shear stress must equal or exceed the shear strength of the work material for cutting to proceed. The value of the shear angle is not something that we directly control, but depends on several factors. Lathe operations # To analyze chip formation, we consider the 2D y–z plane, with the x axis out of the page We assume that all plastic deformation occurs in a single plane, the shear plane, which is oriented at an shear angle .$\\phi$, to the direction of relative motion .$V_\\text{cut}$ of the tool and work. The shear plane meets the sharp edge of the cutting tool at the bottom of the cut. The area of the shear plane is .$fd / \\sin \\phi$ where .$f$ is the feed (also denoted by .$t_0$ in some texts), and .$d$ is the cut depth (out of the page; also denoted by .$w$ in some texts). Turning on a lathe # Nomenclature of cutting process # The tool itself has a carefully designed shape. Its front face is positioned at an angle .$\\alpha$, the rake angle, from a plane normal to the direction of relative tool–workpiece motion. As we will see, the rake angle will always be chosen to be greater than zero, to ensure a clean cut and to reduce the amount of mechanical work needed to produce a cut. The flank angle, meanwhile, is that between the bottom of the tool and the post-cutting workpiece surface. The flank angle needs to be greater than zero to reduce friction and any resulting scratching or fusion between the tool and the freshly machined surface. We can control the rake and flank angles by manufacturing a cutting tool to our specifications. Merchant’s circle: cutting forces # Force diagram that resolves the reaction force exerted by the tool on the work\nIt is important to consider the magnitudes and directions of the forces that are exerted by the tool on the workpiece during cutting. These forces determine the torque required to rotate the work, the amount of elastic distortion of the work or the lathe that might occur during cutting (potentially leading to inaccuracies in the manufactured component), and also whether the tool will experience a load large enough to fracture it.\nOne way to analyze these forces is with Merchant’s Circle. This Circle is a graphical device developed in the 1940s by Eugene Merchant to visualize cutting loads. The circle itself does not represent the shape of the work being cut. This is a force diagram – i.e. the lengths of lines represent magnitudes of forces, not physical distances. The directions of the lines do correspond to actual orientations in space. The horizontal axis corresponds to the cutting direction. We can use Merchant’s Circle to understand how to design and optimize the cutting process. For example, suppose that we have obtained an estimate of .$\\phi$ from experimental measurements of the chip thickness. We could then use the trigonometrical relationships illustrated in Merchant’s Circle, substituting our estimated .$\\phi$ and our known .$\\alpha$ for the tool, to estimate the friction angle .$\\beta$, which would be difficult to measure directly. This approach might be used to compare the effect on friction angle of various candidate tool materials, coatings, or lubricants.\nRelating cutting geometry to cutting forces\nMaterial properties link the geometry of the tool and cutting path to the forces involved. The material being cut needs to exceed its shear yield stress to start deforming (cutting) and creating a chip. If the material exceeds its ultimate shear strength, the chip becomes more likely to fragment into pieces as it forms (which may help clear waste material away). However, the more strain involved in chip formation for a given material, the more work has to be invested in the cutting operation. Therefore designing the cutting operation to limit shear strain can help to limit cutting energy required. Predicting shear strain .$\\gamma$ # Since we wish to limit the amount of shear strain, .$\\gamma$, occurring during cutting, we need to understand how it depends on parameters that we can control. Parameters we control: .$f, V_\\text{cut}, d, \\omega, \\alpha$. Parameters we don’t directly control, but can measure: .$c, F_c , F_t$ Parameters we don’t directly control and are hard to measure directly: .$\\phi, \\beta$, and forces other than .$F_c$ and .$F_t$ Next, we show (via geometry) how shear strain .$\\gamma$ depends on shear angle .$\\phi$ (which is hard to measure directly) and on rake angle .$\\alpha$ (which is known in advance). To predict shear angle .$\\phi$, we then have two options: Measure chip thickness, .$c$, and use geometry to work out .$\\phi$ in terms of .$f, c$ and .$\\alpha$, or Use Merchant’s shear angle relationship to express .$\\phi$ in terms of .$\\alpha$ and .$\\beta$, and find .$\\beta$ by measuring cutting and thrust forces .$F_c$ and .$F_t$ (e.g. by force sensors on the cutting tool holder) Then, we can take action to control shear strain and cutting force. Relationship between .$\\phi$ and .$\\gamma$ # Using chip thickness to .$\\approx \\phi$ # If we did not have any experimental estimate of .$\\phi$ (since it\u0026rsquo;s a function of the material\u0026rsquo;s properties) but we did have an estimate of .$\\beta$ and knowledge of rake angle .$\\alpha$ (since we can control the tool we use(\u0026rsquo;s rake angle)) and the shear strength of the work material, .$\\tau_y$, we could use Merchant’s shear angle relationship to predict both .$\\phi$ and the cutting force.\nWe can measure a chip to get .$c$ Can then use the relationship between .$\\phi$ and .$\\gamma$ to estimate .$\\gamma$ Merchant’s shear angle relationship Study Advice: # Don\u0026rsquo;t memorize this derivation! This is to show how we can do process diagnosis if we understand each step of the process and how they relate to one another Understand merchant forces, shear/friction angle, the physical relationship between these angles, and terms (cutting depth, feed (cut amount per rotation), feed rate (speed moving tool along machine)) Merchant’s shear angle relationship # The total reaction force exerted by the tool on the work, .$R$, can be resolved in any of three useful coordinate frames: The first frame is defined by the cutting direction. .$F_c$ is the cutting force, the force in the direction of relative motion of the tool and work. This force is important because the mechanical work done by the lathe is equal to the cutting force times the distance moved in the direction of cutting. The force perpendicular to the cutting force, the thrust force, .$F_t$, can contribute to bending of the work during turning and can become problematic and lead to vibration, particularly with long slender workpieces if they are not supported at both ends. The second frame is defined by the rake (front) face of the tool, and decomposes the reaction force into a frictional force .$F$ parallel to the rake face, and a normal reaction component .$N$. These loads are related by the friction angle: .$\\beta = \\arctan (F/N)$ where .$F/N$ is the coefficient of friction between the chip and the tool. This coefficient can be considerably larger than 1, particularly if chip material bonds to the front of the tool (leading to a built-up edge) and chip–tool sliding involves plastic deformation of the chip. Ideally .$\\beta$ will be as small as possible, which can be achieved with liquid lubrication of the cutting location and/or by special tool coatings which make it smooth and reduce the coefficient of friction. The third frame is defined by the shear plane, and is composed of the shear force .$F_s$ which acts within the shear plane, and the normal component .$F_n$ We assume that shear stress equals the shear strength of the workpiece material in the shear plane, and that shear stresses are lower elsewhere in the material. Also assume that the value of the shear angle .$\\phi$ is naturally such that the shear strength, .$\\tau_y$, of the workpiece material is reached in the shear plane at the lowest possible cutting force .$F_c$. If we know the material’s shear strength, the friction angle .$\\beta$, and the shear angle .$\\phi$, we can (roughly) estimate the required cutting force .$F_c$. Need to solve for .$\\phi$ in terms of parameters we can control: .$\\alpha$ (directly through tool geometry) and .$\\beta$ (indirectly through lubrication). To find value of .$\\phi$ for maximum shear stress at a given .$F_c$, set derivative .$\\frac{d\\tau}{d\\phi}$ to zero and solve: Aside: This equation assumes that all deformation occurs in the sheer plane The lower the friction angle, the higher the sheer angle, requiring larger cutting forces To estimate .$\\beta$, we can measure .$F_c$ and .$F_t$ and use the force-resolving circle, and then substitute into the expression for .$\\phi$: $$\\tan(\\beta - \\alpha) = \\frac{F_t}{F_c}$$ $$\\beta = \\arctan\\bigg(\\frac{F_t}{F_c}\\bigg) + \\alpha$$ Insights: Increasing the rake angle .$\\alpha$ (via tool design) and lowering friction angle .$\\beta$ (by application of an appropriate lubricant/coolant – e.g. water/oil mixture) both help to increase shear angle, reducing the area of the shear plane. However there are limits: for instance, if .$\\alpha$ becomes too large, .$F_t$ may become negative in which case the tool would dig into the workpiece and result in a very poor finish. There\u0026rsquo;s a limit to how high .$\\alpha$ can reasonably be, though. If it becomes so large that the angle enclosed at the tool tip is extremely sharp, the tool will become highly susceptible to cracking. Moreover, at very large .$\\alpha$ the thrust force may change direction and tend to pull the tool into the work, leading to vibration and a very poor finish. Examples of process changes This model is an approximation; alternative models exist Merchant’s circle: examples of process changes\nThe larger .$\\phi$, the shorter the shear plane and the smaller the cutting force is expected to be for a given feed, cut depth and material (reducing the cutting force is desirable). If .$\\phi \\approx 0$, the shear plane would be large and the cut would be rough. As a rule of thumb, if the angle between the shear plane and the rake face of the tool is about 90°, cutting will be of reasonably high quality as long as adequate lubrication and a reasonable cutting speed are used. Meanwhile, knowledge of .$\\gamma$ can help give a picture of how much plastic deformation is occurring in the chip between the moment of first yield and the chip leaving the cutting tool. Cutting power # The above analysis relies on having knowledge of multiple geometrical parameters, the friction coefficient, and material properties such as shear strength. These values will often not be available; another approach is to characterize metal cutting operations with a specific energy value, which is the energy required to remove a unit volume of material from the work. Specific energies are usually reported with quite a wide range (e.g. 0.4–1.1 J/mm.$^3$ for aluminum alloys) to take into account the variability of tool–material friction and tool geometries that might be used. Different workpiece materials are associated with different specific energies, .$U$ (J/mm.$^3$): the work that must be done per unit volume material removed. However, these energies, which are widely tabulated in handbooks, provide a simple way to relate cutting speed, force and power. The cutting power is simply the product of material removal rate and specific energy. The material removal rate can be computed as the product of cutting speed, feed and cut depth. Broadly speaking, the harder the tool and the softer the workpiece, the larger the feeds and speeds that can be successfully used. The ductility of the workpiece (strain at fracture) also plays a role in determining suitable feed and speed. Not all of the cutting power will go straight into plastic deformation of the work material. In a typical process about 30% of the work done would be dissipated as tool–chip friction. Additionally, the electrical power input to the lathe will exceed the cutting power required, because the motor will not be perfectly efficient and there will also be some mechanical losses in the lathe. .$U$ approximately folds in work done in the shear plane, work done against friction, and mechanical losses in the lathe – hence each material has a quite wide range for .$U$ Example values of .$U$ (J/mm.$^3$): Aluminum alloys: 0.4 – 1.1 Titanium alloys: 3.0 – 4.1 Steels: 2.7 – 9.3 Expression linking .$U$, cutting power .$P_C$, and geometry: Since .$F_C$ depends on the tool, we can find the span of possible .$f, d$ values with the equation above (which need to be sufficiently great to remove material from the work) We have discussed the concepts of shear plane, rake angle and cutting power, etc, in the context of lathe turning But these concepts apply across a whole range of metal-cutting processes, including facing processes on a lathe, as well as drilling, milling, planing, and even processes that use tools with many small cutting edges, such as sawing and rasping. In all of these processes the cutting edge shape is optimized to reduce the amount of mechanical work done to remove material, and give a clean, smooth cut. Tool temperatures and lifetimes # The basic requirement of any cutting tool is that it must be made of a material that is harder than the material it is cutting. For cutting many metals and alloys, we can use high speed steel (HSS), which is a hardened steel containing tungsten, chromium and vanadium as alloying elements. The cutting speeds that are possible with HSS, however, are lower than can be achieved with harder, higher-melting-point, tool materials. An example of such a material would be tungsten carbide. Cutting tool materials, being very hard, are very challenging to machine, and so are made either by grinding or by powder metallurgy, where powder mixtures are pressed in a mold into the shape of the required tool insert and are then baked to fuse the powder. We say tool insert because the objects made by powder metallurgy are small square items that are mounted into a larger holder. The inserts usually have multiple usable edges, so as soon as one of them is blunt or fractured, the insert can simply be rotated in the holder without re-aligning the tool in the machine. An example of a tool material produced by powder methods is “cemented carbide” which is hard tungsten carbide powder bound together with cobalt. Cemented carbides are harder wearing than conventional high-speed steel and easier to produce than pure tungsten carbide tools. Coatings are also important in tool fabrication. Titanium nitride and diamond-like carbon are examples of coatings that reduce chip–tool friction, in part by reducing the surface roughness of the tool The useful lifetime of a cutting tool is a function both of the material and of the cutting velocity at which it operates. This is because as the cutting speed increases, the rate of frictional heat dissipation rises, heating and softening the tool material, and enabling it to erode more quickly: Taylor tool life equation: $$VT^n = C$$\n.$V$ = cutting speed = .$V_\\text{cut}$ .$T$ = tool lifetime .$n, C$ are empirical constants .$n \u0026lt; 1$ and predominated by the melting temperature of the tool material Smaller .$n$ values indicate that cutting tool lifetime falls more rapidly as cutting velocity increases than materials with .$n$ approaching 1. Example .$n$ values: One advantage of using a tool material that can withstand a higher cutting speed is that one can reduce processing time. Another potential advantage is to be able to reduce the feed while not lengthening the total processing time. Smaller feeds can enable smoother finishes to be achieved, so finishing cuts will tend to have smaller feeds than roughing cuts (which remove large amounts of material quickly at the start of a turning process) Examples of tool wear # Some general effects of tool wear include: increased cutting forces increased cutting temperatures poor surface finish decreased accuracy of finished part May lead to tool breakage Causes change in tool geometry Certain tool materials do not work well with other work materials Typical recommended cutting speeds and feeds # Cermets: Composite between ceramic and metal. Used if you need an especially tough tool Carbides are hard but brittle, so they take tiny particles (typically cobalt) and mix it with a metal (tungsten often used) Talk to the Jacobs staff about this and they can teach you a lot Cutting tool design # (a) Schematic illustration of a right-hand cutting tool for turning. Although these tools have traditionally been produced from solid tool-steel bars, they are now replaced by inserts of carbide or other tool materials of various shapes and sizes, as shown in (b). The insert is the actual cutting feature.\nSurface roughness in machining # Takeaway: Roughness varies with radius .$R_n$ and feed .$V_\\text{feed} = f$ If you want a mirror finish, you have to decrease feed rate .$R_P$: Peak-to-valley roughness Example: .$R_n = 0.5 \\text{ mm; } f = 0.1 \\text{ mm}$ Actual roughness will be up to 2-3 times worse than this ideal value: built-up edge, cracking, scratching from chips, etc Turning example Autodesk computer-aided manufacturing (toolpath planning) demo Etcheverry Machine Shop manual lathe demo 02-03: Other cutting processes based on plastic deformation # Cutting-based operations other than turning: Milling Drilling Reaming Boring Broaching Tapping Punching Milling # Milling in action Etcheverry demo video (manual) Etcheverry demo video (CNC) Cut depth tends to be much greater than feed Types of milling Peripheral (plain, down) milling Tool axis is parallel to surface being machined Slab, slotting, side milling, straddle, form Face (up milling) milling Tool axis is perpendicular to surface being machined Conventional, partial, end, profile, pocket, surface contouring Both have pros/cons Teeth orientation is different (down milling puts lots of force on the teeth) Drilling # Drilling Flutes carry away material However, you end up with scratches on the side wall when drilling Reaming Reaming involves enlarge existing holes Provide better tolerance and smoother finish than drilling Reaming tools: vertical flutes Boring # Boring = “inside turning” Single point tool moving along the inside of a rotating workpiece Broaching # Tool used to get square (or non-circle) holes How keyways are put into gears Cut per tooth is analogous to the feed Example Geometry # (a) Typical parts finished by internal broaching.\n(b) Parts finished by surface broaching. The heavy lines indicate broached surfaces;\n(c) a vertical broaching machine.\nTapping # Creates an internal (though external of possible, just uncommon) threads with no pitch nor diameter You start by creating a hole a bit smaller than the minor diameter, then drive the tap in Tend to be treated to ensure they\u0026rsquo;re strong enough Notice that the diameter tapers off, so you\u0026rsquo;re progressively enlarging the hole Punching # Comes in a set including both the punch and die Creates an edge using shear forces \u0026ndash; think industrial hole puncher There has to be a gap between the punch and die; around 6-10% of the desired hole size (varies with type of material) Otherwise tool or medium can be damaged Video demo Other subtractive processes not based on plastic deformation # Laser cutting # In laser cutting, an intense beam of light imparts heat locally to the material and converts the solid either to liquid or directly to vapor to form an edge Where this melted material goes (may) matter depending on the job Extremely quick With the right type of laser you can cut many materials In ablation, a laser beam vaporizes material from the surface of a component to shape it without cutting all the way through it. A thin band of material is removed: the kerf Has some thickness, maybe a thousandth, which may matter The cut isn\u0026rsquo;t completely vertical \u0026ndash; intensity isn\u0026rsquo;t uniform, and beam may not be completely orthogonal (and even then there is some geometry of the focal-spot) http://alumni.media.mit.edu/~yarin/laser/physics.html\nElectrode discharge machining (EDM) # An electrical field is applied between a high-melting-point electrode (which creates sparks, plasma) and the material that that is to be shaped is usually submerged in water. The gap breaks down electrically and a high current flows, heating and vaporizing the work material. The electrode might be made of a wire (for profiling operations) or might be a custom-shaped electrode (e.g. made of graphite) to enable parallel transfer of a complex geometries to a workpiece. Used when you require extreme precision Wire has a constant width, there\u0026rsquo;s no focusing issues like with laser cutting Wire EDM # Not obvious in this example, but the wire can be adjusted about the .$z$-axis too Rate limitation is a function of how quickly you can cut + remove the material https://www.mdpi.com/2076-3417/10/6/2082/htm Abrasive jet # Intro video A high-pressure (≫ 100 MPa) jet of water containing fine, hard particles (usually garnet) impact a sheet of material and cut through it. It\u0026rsquo;s not the water itself, rather, there are tiny particles in the water itself which concentrate the forces enough Material must be emersed under water Surface finish will be more rough (matt texture) than EDM or laser cutting https://www.omax.com/news/blog/controlling-taper "},{"id":8,"href":"/cogsci-c100/non-visual/","title":"3: Non-Visual Perception","section":"CogSci C100","content":" Auditory Perception: Psychological Effects Of Music # Effects Of Music On Cognition # Music has been found to influence memory, decision making, and other cognitive processes Study conducted in supermarkets found that use of slow background music increased sales by 38% over use of fast music - shoppers stayed in store longer and bought more, a lot more (Milliman, R.E., 1986) Particularly effective for purchasing decisions with high affective/low cognitive involvement, e.g., jewelry, sportswear, and beer Follow-up study found this is only the case for music in minor mode (Knoeferle, Spangenberg, Herrmann et al., 2011) Study on effects of music on decision making (Hansen \u0026amp; Melzner, 2014) Group 1: Listened to tritone or dissonant, unfamiliar chords Beginning of Simpson’s theme song Group 2: Listened to perfect fifth or consonant, familiar chord Twinkle, twinkle little star Are people in Group 1 or Group 2 more likely to be swayed by aggregate, as opposed to individualized, information? Aggregate info: overall star rating on Amazon review Individualized info: actual customer reviews that appear at the bottom of the page Group 2 The Mozart Effect # Initial research Participants who spent 10 minutes listening to Mozart afterwards showed mean spatial IQ scores that were 8-9 points higher (significant, not giant) than that of controls in relaxation conditions Effect did not extend beyond 10-15 minutes Effect was specific to spatial temporal performance Follow-up studies with rats suggest that effect cannot be explained merely by “enjoyment arousal” Rats exposed to Mozart in utero and for 60-day postpartum period completed maze test significantly more quickly and with fewer errors than control rats exposed to minimalist music, white noise, or silence (Jenkins, J.S., 2001) Longer-term effects Children from low SES backgrounds who were given classical music in wind and string instruments for 18 months performed significantly better on IQ tests administered after program ended (Barbaroux, Dittinger \u0026amp; Besson, 2019) Specificity of music # No enhancement in performance on spatial temporal tests was shown after listening to minimalist or oldtime pop music In fact, one experiment found that heavy metal increased amount of time it took for mice to run a maze by 20 minutes \u0026ndash; and the mice also started killing each other! Other types of music that resemble Mozart’s in high degree of long-term periodicity, especially within the 10-60s range (e.g., Bach), also found to be effective The Mozart effect on epilepsy # Listening to Mozart has been found to decrease abnormal brain wave activity and number of seizures in patients with epilepsy (Grylls, Kinsky, Baggott et al., 2018) One study found that 23 of 29 patients showed decrease in seizure activity after listening to Mozart piano sonata Abnormal spikes fell from 62% to 21% in one patient Beneficial effects of music on neonates (early-born babies) Preterm babies who listened to music (not necessarily Mozart) in the neonatal intensive care unit had brain activity that more closely resembled that of full-term babies (Lordier, Meskaldji, Grouiller et al., 2019) Match the following: (Rentfrow \u0026amp; Gosling, 2003, 2006) classical, jazz, blues, and folk music lovers\ncountry, pop, and religious music lovers cheerful verbally intelligent outgoing conscientious open to experience Answers (2, 1, 2, 2, 1) Music is the only sensory experience that can activate all areas of the brain simultaneously! Meditation can do this too, but it\u0026rsquo;s not sensory Music Therapy # Typically used for people with Alzheimer’s, learning disabilities, autism, ADHD, Down Syndrome, intellectual disability, brain injuries, physical disabilities, chronic pain Alzheimer\u0026rsquo;s patients have shown improvements in memory when listening to their favorite songs In children, helps with emotional self expression, improves social and communication skills, as well as motor functioning Various studies with cancer patients there have found significant reductions in anxiety, pain, and tiredness after music therapy was introduced Music being played alone shows an improvement in mental and physical well being. However, it is more effective when paired with the playing of an instrument or singing along. Individual Differences In Perception # Introverts vs. Extroverts 🍋 Introverts salivate more to lemon juice \u0026ndash; both by self-report and experimentally Introverts tend to retain elevated heart rate for a longer time period after exposure to noxious odors Introverts are more sensitive to pain \u0026ndash; and are able to learn more quickly from punishment At the same time, extroverts are more likely to drink/smoke to much and end up in jail Synesthesia # Synesthesia: stimulation of one modality leads to perceptual experience in another Color grapheme (seeing specific letters or numbers in specific colors) synesthesia is most common type of synesthesia We all have some sense of underlying synesthesia: Which is a dog barking \u0026lsquo;woof woof\u0026rsquo;, doorbell ringing \u0026lsquo;ding ding\u0026rsquo;, and a furnace billowing \u0026lsquo;woosh wooosh\u0026rsquo;? Which is booba and which is keekee? Some people visualize months like this: These examples were created by synesthetes, and most people agree on them However, only synesthetes seem to be able to create these pieces Study conducted at Science Museum in London Visitors viewed two musical animations, one designed by synesthetes, the other by nonsynesthetes Then asked which animation better matched the music Participants overwhelmingly chose the synesthete-designed animation Suggests that we all unconsciously link together music and vision but only synesthetes are consciously aware of these links This sensory crossover probably occurs in the limbic system Two theories of synesthesia # Brain architecture of synesthetes is equipped with more connections between neurons, causing the usual modularity to break down Says the brain is made of many modules, and synesthetes have more connections between these modules “Feed-backward” connections that carry information from high-level multisensory areas of the brain back to single sense areas are not properly inhibited Normally, information processed in such multisensory areas is allowed to return only to its appropriate singlesense area In synesthetes’ brains, that inhibition is disrupted, allowing the different senses to become jumbled Subliminal Perception And Priming # Subliminal perception: Messages that are presented below one’s absolute threshold for conscious awareness may influence behavior Experiment in which emotionally positive scenes (e.g., kittens) or negative scenes (e.g., werewolf) were subliminally flashed before participants viewed slides of faces More positive ratings were given to faces that had been associated with positive scenes (Krosnizk, Betz, Jussim et al., 1992; Anderson, Siegel, White et al., 2012) Graduate students evaluated their research ideas more negatively shortly after viewing the unperceived scowling face of their adviser In the experimental group, students were more likely to be critical (less confident) in their ideas Advertisers have made all sorts of exaggerated claims of being able to sell products when the product is flashed very briefly, along with an image of a very attractive person. In general, not well-supported by evidence. The word \u0026lsquo;beef\u0026rsquo; was flashed at below-threshold durations while participants watched a film. They were later more hungry, but did not specifically prefer beef to other foods. Suggests that meaning not easily extracted from subliminal presentations; rather, stimuli act at an emotional level of priming task (Dijksterhuis, Aarts, \u0026amp; Smith, 2006) No real evidence that listening to subliminal message tapes during sleep effective but some research suggests some types of sensory stimuli during sleep may affect learning and memory “Cramming” during sleeping study (Antony, Gobel, O’Hare et al., 2012) Participants learned to play 2 simple piano melodies, then took a 90-minute nap While they slept, one of the melodies was quietly played on repeat Able to play cued melody more accurately than other melody when they awoke Smoking cessation study (Arzi, Holtzman, Samnon et al., 2014) When asleep, study participants were exposed to two odors, cigarette smoke and rotten fish During the following week, those who had smelled the mix of both odors lit up 30% less than those in control groups Conditioning was more effective during Stage 2 than during REM sleep Explicit olfactory aversive conditioning during wakefulness did not alter smoking behavior Priming: unconscious activation of particular associations in memory People showed either of these images and instructed to draw a nature scene; those shown the left image tended to draw scenes with water (find the duck!)\nScrambled-sentence test: Make a grammatical four-word sentence as quickly as possible out of the following five-word sets: Him was worried she always From are Florida oranges temperature Ball the throw toss silently Shoes give replace old the He observes occasionally people watches Be will sweat lonely they Sky the seamless gray is Should now withdraw forgetful we Us bingo sing play let Sunlight makes temperature wrinkle raisins After completing this test, participants walked significantly more slowly – almost as if they felt old – than control participants (Bargh, Chen, \u0026amp; Burrows, 1996) "},{"id":9,"href":"/e-29/3/","title":"3: Additive Processes: Intro \u0026 Extrusion","section":"Engineering 29","content":" The rise of additive manufacturing # Additive manufacturing or “3D printing” has been receiving enormous attention both in industry and as a tool for education and design. Something that sets additive manufacturing apart from other families of processes is the enormous rate of innovation in process technology and machine design, together with the fact that much of this innovation is done by small start-up companies and even by individuals, with the development in some cases being crowd-funded. There are huge differences between the versatility and achievable tolerances of “maker-grade” (or consumer-grade) and industry-grade tools, and in this part of the course we will describe and analyze some of the processes that are available, and provide a framework for analyzing new additive manufacturing tools as they become available.\nThe richness of innovation in machine design has been helped by the fact that the established players in 2D printing (HP, Epson, etc.) have until very recently been largely absent from 3D printing. This situation has begun to change, especially with the introduction of HP’s “Multi Jet Fusion” systems, but there is no doubt that the market is highly fragmented, and to understand it, one needs knowledge of the underlying material processing principles.\nReasons to use additive manufacturing # Additive manufacturing has conventionally been seen as a means of prototyping components that would then be mass-produced with some other, faster, process. Prototyping remains an important application, but there are many reasons why one would produce final, functional components additively. Reasons for selecting an additive process over another kind of process include:\nGeometry needs customization, \u0026ldquo;mass customization\u0026rdquo; Medical implants and prostheses; dental aligners, crowns, bridges, surgical guides, clothing, footwear Run size too small for custom tooling There are some components where machining would require too much time and/or labor to be economical, and mass-production techniques such as casting or injection molding would incur considerable tooling costs (complex injection molds can cost tens of thousands of dollars) e.x. Aircraft cabin components (e.g. ducting, seats); titanium alloy turbine blades; mounting brackets; engine fuel nozzles The desired geometry cannot be made in any other way Multi-material, graded stiffness or color Internal porosity for reduced mass The idea of “complexity as practical” or that “complexity is free” is often talked about as a distinctive advantage of additive manufacturing. Reasons for geometrically complex designs: particular aesthetic goals, to improve the aerodynamic performance of a vehicle, or to optimize mechanical properties (e.g. stiffness-to-weight ratio, or compressibility) of a component by introducing fine porous structures that mimic geometries found in nature (e.g. bone, cork, and branched tree-like structures). Supply chain is challenging in some way (geographically or temporally) Enables more decentralized/distributed production Space Station: producing spare parts on the International Space Station — see the start-up company Made in Space that is developing fused deposition modeling tools to work in vacuum and zero-gravity, as well as recycling machines for the printable material, the idea being to produce components on demand without having to wait for a new spacecraft to be launched from Earth to deliver them; Producing spare parts for military use in theaters of war, where components are frequently needed more quickly than they could be shipped, installing a full machine shop in the field may not be practical, and carrying a comprehensive array of spare parts would be cumbersome Printing of food, where freshness is important and people decide what they would like to eat just minutes before they eat it. Assembly costs can be significantly reduced. Items that would necessitate the use of many components if made with traditional manufacturing approaches could be produced in a single piece by exploiting the extra geometrical flexibility allowed by additive manufacturing, thus saving assembly costs. Less workers (human controllers) required \u0026ndash; cheaper, less (human) error Why Not: Potentially waste more material Supports are annoying to remove Working principles for additive manufacturing # New additive processes and tools enter the market every month, so any detailed description of process technology will rapidly become outdated. Perhaps the most useful way to think about additive manufacturing technology is to isolate the different functions that are involved in any additive process, and consider the multiple independent ways in which each function might be fulfilled. Printing tools could be conceived that combine those solution principles in many different ways:\nMaterial supply: Gas (semiconductor manufacturing) Solution (Electroplating) Laminae (sheets of material), i.e Fabrisonic Energy: Heat Cooling (cryo-printing, biological tissue) Plasma Spatial: moving tool versus moving work belt/string drive screen projector scanning mirrors (galvanometer scanner) This is a very rapidly developing field. Some of these seem unrealistic, and they may be, but only for now.Technology is rapidly increasing and what is unfeasible today may be feasible very soon. Think neural networks\u0026ndash; we\u0026rsquo;ve known the underlying concepts since 1873 but only now has technology become fast enough for it to become feasible Simple model for extrusion # Material must able to heat up and cool down while maintaining it\u0026rsquo;s key properties Higher the temperature, more energy required, lower the viscosity Radius matters Narrow is significantly harder Typical nozzle diameters are in the range 100–250 μm So more resolution is harder to achieve Distance to drive wheel and extruder block needs to be short enough so the material doesn\u0026rsquo;t buckle Considerations # Turning corners Need to synchronize extrusion rate If .$Q$ is too high, then you\u0026rsquo;ll end up with corner bulges Volume conservation Voids/porosity If material isn\u0026rsquo;t hot enough to remain molten, then it won\u0026rsquo;t fill the voids Thermal gradients (difference in temperatures) Can cause warping Heat the component during printing Heated beds (or \u0026lsquo;ovens\u0026rsquo; \u0026ndash; used in industrial machines) can help prevent this Types of additive process # Fused deposition modeling (FDM) # Need models for heat transfer at printing location Also called Fused Filament Fabrication (FFF) Print time depends strongly on: Machine, Component size/geometry, In-fill strategy After CAD is complete, model is ran through software (i.e. cura) which turns the model into a series of triangles, which is then used to generate some tool path that forms the model There\u0026rsquo;s some information loss throughout this process Variations at Jacobs Type A \u0026ndash; most basic \u0026amp; common Single extrusion nozzle (one material) PLA: Poly Lactic Acid \u0026ndash; plant-based, recyclable, but not industrial-grade due to it\u0026rsquo;s brittleness Ultimaker 3 PLA, PETG (tougher, bit more flexible than PLA) Dual nozzle \u0026ndash; capable of support material (or just various colors) LulzbotTaz 6 1.2 mm nozzle Stratasys Fortus 380 MC ABS (reasonably tough: lego-brick material); others possible (i.e nylon) Soluble support Markforged X7 One of the first printers that enables you to include carbon fiber-reinforced nylon Printed composite Stiffer than many aluminum alloys Industrial usage Examples: Curtain headers, internal brackets for Airbus A350; air duct components. Material: “Ultem” (polyetherimide), PEEK, nylon Used due to the ability to rapidly manufacture in bulk + light weight compared to alternatives Support structures and extrusion diameter # Stair-step effect You can fix this with post-processing processes i.e. acetone vapor, sanding Some FDM machines have two or more extrusion dies which can deposit different materials independently. One of these extrusion heads might be set up to deposit dedicated support material, which is often soluble in water or a weak NaOH solution and can thus be readily removed from the printed part A widely used rule of thumb is that an overhang with an angle of up to 45° can be printed without any support material. Stress Line Additive Manufacturing (SLAM) # Fused deposition modeling (FDM) [sometimes known as fused filament fabrication (FFF)] is usually carried out layer-by-layer but inter-layer voids or defects can reduce strength or make strength highly directional. If the filament orientation can be optimized based on knowledge of the way the part will be loaded (i.e. aligning filaments with the principal stress lines), strength could be improved. This is what SLAM aims to achieve. Extrusion nozzle path planning is more challenging than in layer-by-layer FDM. FDM of metal powder/polymer mixtures # The filament material itself is crucial in determining the performance of the printed object. Filaments with new properties enter the market all the time. Materials with widely varying elasticity are available. It is possible to buy filaments with embedded metal particles, wood particles, carbon powder or even graphene (sheets of few-atom-thick carbon with exceptional in-plane thermal and electrical conductivity). These additives control the optical properties (e.g. reflectivity), and, with a post-printing sintering step, electrical conductivity (enabling printing of e.g. circuit boards) and possibly even thermal conductivity.\nNevertheless, the working principle of FDM is that a thermoplastic material is temporarily softened inside the print head and then extruded layer by layer on to the emerging component. So it is likely that the largest constituent by volume of any FDM filament will continue to need to be a thermoplastic material. At the moment, organic thermoplastic polymers are the primary ingredient of most filament materials. However, we have already seen the working principle of FDM translated from organic thermoplastic polymers to metals and to glass, which can be extruded in a sufficiently viscous form that it holds its shape long enough to be deposited on to a component.\nA heated nozzle brings the feedstock filament close to its melting point, then an electrical current passed through the filament and into the substrate further heats the material causing it to fuse on to the substrate. It has also been demonstrated with bulk metallic glasses, which are special alloys that remain amorphous at readily attainable cooling rates – i.e. they do not crystallize and can therefore achieve a desirable combination of hardness and toughness. Meanwhile, use of arc welding in conjunction with robotics to deposit metal has been demonstrated as a lower-resolution, but much faster, way of depositing material Filament ~85 wt% metal powder; polylactic acid (PLA) binder Sinter at 980 °C (copper) or 830 °C (bronze) to fuse particles If you get high enough pressure, then the metal atoms themselves will fuse You\u0026rsquo;re not melting the object; shape is retained Same side of the coin: pores still exist \u0026ldquo;Pores are just cracks waiting to grow\u0026rdquo; due to their sharp edges Quench; pickle or polish Left: After printing; Right: After Polishing\nDirect ink writing # This is a hybrid between FDM and sintering, and involves extruding a paste containing particles of the structural material in a solvent. The material can be extruded successfully through syringe needles as small as a micrometer in diameter. Lead zirconium titanate ceramic particles in a solvent dispensed via nozzle Piezoelectric material The material paste is thixotropic, or shear-thinning, which means that when it is loaded and starts to flow, its apparent viscosity falls enabling it to be squeezed through the needle. Versus than thermally softening polymer (like squeezing toothpaste from a tube) When its shear strain rate falls again, its viscosity rises, so the extruded structure retains its shape and does not deform significantly under the action of gravity or surface tension. Once the structure has been printed, heat is applied to drive of the solvent and sinter the particles. Applications include making micro-scale sensors and actuators, and tissue scaffolds. Material flow is achieved by using a (shear- thinning) ink Features down to ~ 1 micrometer possible by extrusion through fine needles Advantages: heat not required; more material possibilities (notably ceramics) Shear stress in material, .$\\tau$, is defined by: $$\\tau = \\tau_y + K \\dot \\gamma ^n$$ .$\\tau_y$ is zero-shear-rate yield stress (Pa) .$K$ is a material constant .$\\hat \\gamma$ is shear (deformation) rate (1/s) .$n$ is shear-thinning exponent (\u0026lt;1 for this process to work) "},{"id":10,"href":"/eecs-16a/3/","title":"3: Transformations \u0026 Inverse","section":"EECS 16A","content":" 02-08: Matrix Transformations # Slides Note 4 Linear Transformations # Linear Transformation: In the previous practice set, we discussed the idea of a matrix .$A^{M \\times N}$ as a linear transformation. Effectively, in the equation .$A \\vec x = \\vec b$, the matrix itself can be considered a transformation .$f : \\mathbb{R}^{N} \\to \\mathbb{R}^{M}$ which takes a vector .$\\vec x^{N \\times 1}$ of inputs and returns a vector .$\\vec b^{M \\times 1}$ of outputs That is, matrices are operators that transform vectors Just as .$f$ is a linear transformation iff homogeneity and super position hold, matrix-vector multiplications satisfy linear transformation: $$A \\cdot (\\alpha \\vec x) = \\alpha A \\vec x$$ $$A \\cdot (\\vec x + \\vec y) = A \\vec x + A \\vec y $$ State Transformation # As such, we can think about matrices as state transformations; If we have a list of inputs representing some current state at some timestep .$n$ (given by .$\\vec x(n)$), then when a matrix .$A$ operates on that state, it transforms it into a new state at the next time step (.$\\vec x(n + 1)$). Consider a timestep to be a very small unit of time. Our systems here will be discrete, meaning that the transition of water happens exactly at each timestep, and not between timesteps Aside: But in reality, water is flowing continuously! To model this rigorously, we need linear differential equations, but for now, if the timestep we take is very small, the discrete model is quite good as an approximation. Example: Water Pulps ( Note5 ) At each time step, some portion of the water in each pump goes to itself, and some portion goes to each of the other pumps. The general state transition matrix formula for an .$n$-state system (assuming the initial and final state vectors have the same length .$n$) is as follows: $$ \\begin{bmatrix} \\vec P_{1 \\to \\dots} \u0026amp; \\vec P_{2 \\to \\dots} \u0026amp; \\dots \u0026amp; \\vec P_{N \\to \\dots} \\\\ \\end{bmatrix}$$ $$\\equiv$$ $$\\begin{bmatrix} P_{1 \\to 1} \u0026amp; P_{2 \\to 1} \u0026amp; \u0026hellip; \u0026amp; P_{N \\to 1}\\\\ P_{1 \\to 2} \u0026amp; P_{2 \\to 2} \u0026amp; \u0026hellip; \u0026amp; P_{N \\to 2}\\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots\\\\ P_{1 \\to N} \u0026amp; P_{2 \\to N} \u0026amp; \u0026hellip; \u0026amp; P_{N \\to N} \\end{bmatrix} $$ $$\\begin{bmatrix} 0.4 \u0026amp; 0.1 \u0026amp; 0.4 \u0026amp; 0.1\\\\ 0.1 \u0026amp; 0.2 \u0026amp; 0.1 \u0026amp; 0.4\\\\ 0.2 \u0026amp; 0.3 \u0026amp; 0.2 \u0026amp; 0.2\\\\ 0.3 \u0026amp; 0.4 \u0026amp; 0.3 \u0026amp; 0.3 \\end{bmatrix}$$\nNotice that all of the water goes somewhere and none comes up out of thin air; that is, the water is a conserved quantity. We don’t have any leakage or generation of water in the system. This isn’t always true, but the idea of conservation will largely hold true, especially for systems based in physical reality. We can tell if the transformation is conservative by looking at each column’s values describe the movement of water from a specific node to other nodes. If any column’s values do not sum to exactly 1, then something is being lost or created in the system as a whole. In addition, if a specific column’s sum is greater than 1, matter is entering the system through that node; conversely, if a specific column sum is less than 1, matter is leaving the system through that node. Recognize that, given information about only a single node’s column sum, we can never definitely say if the overall system is conservative or not; we only know if it might be conservative, based on other nodes. Diagram .$\\to$ Matrix: Given a state transition diagram, we can create the corresponding state transition matrix by reading the values at each arrow, noting the directionality (these are directed edges) and populating the rows one by one. Similarly, given a matrix, we can draw the appropriate number of nodes and label arrows going to/from each node with the values as indicated by the matrix. How do we go back in time? That is, we want some transition matrix .$B$ such that .$\\vec x (t-1) = B \\vec x(t)$ Flipping the direction of the edges won\u0026rsquo;t work\u0026hellip; Transpose won\u0026rsquo;t either\u0026hellip; Which leads us to\u0026hellip; 02-10: Inverse # Slides Notes 5, 6 Matrix Inverse # Purpose We know that .$\\vec x(t+1) = Q \\vec x (t)$ and want some reverse-matrix .$P$ such that .$\\vec x (t) = P \\vec x (t+1)$ $$P \\vec x(t+1) = PQ\\vec x(t)$$ $$P \\vec x(t+1) = I \\vec x(t)$$ $$\\vec x(t+1) = Q \\vec x(t)$$ $$\\vec x(t+1) = Q(P \\vec x(t+1))$$ $$\\vec x(t+1) = I \\vec x(t+1)$$ Consider .$A$ as an operator on any vector .$\\vec x \\in \\mathbb{R}^{n}$: What does it mean for .$A$ to have an inverse? It suggests that we can find a matrix that \u0026ldquo;undoes\u0026rdquo; the effect of matrix .$A$ operating on any vector .$\\vec x \\in \\mathbb{R}^{n}$. What property should .$A$ have in order for this to be possible? A should map any two distinct vectors to distinct vectors in .$ \\mathbb{R}^{n}$, i.e., .$A \\vec x_1 \\neq A \\vec x_2$ for vectors .$\\vec x_1, \\vec x_2$ such that .$\\vec x_1 \\neq \\vec x_2$. Definition: Let .$P, Q \\in \\mathbb{R}^{N \\times N}$ be square matrices (we tackle non-square in 16B) .$P$ is the inverse of .$Q$ if .$PQ = QP = I$ We say .$P = Q^{-1}$ and .$Q = P^{-1}$ Steps to solve with Gaussian Elimination are shown on slide 50 or \u0026lsquo;more\u0026rsquo; formally in Notes 6, page 3 For any .$n \\times n$ matrix .$M$, we can perform Gaussian elimination on the augmented matrix: If at termination of Gaussian elimination, we end up with an identity matrix on the left, then the matrix on the right is the inverse of the matrix .$M$ If we don’t end up with an identity matrix on the left, we will have a row of zeros, (which indicates that the rows of .$M$ are linearly dependent) and that the matrix is not invertible $$\\begin{bmatrix} \u0026amp; \u0026amp; | \u0026amp; \u0026amp; \\\\ \u0026amp; M \u0026amp; | \u0026amp; I_n \u0026amp; \\\\ \u0026amp; \u0026amp; | \u0026amp; \u0026amp; \\\\ \\end{bmatrix} $$ $$ \\begin{bmatrix} \u0026amp; \u0026amp; | \u0026amp; \u0026amp; \\\\ \u0026amp; I_n \u0026amp; | \u0026amp; M^{-1} \u0026amp; \\\\ \u0026amp; \u0026amp; | \u0026amp; \u0026amp; \\\\ \\end{bmatrix}$$\nInverse of a 2x2 matrix # You can derive this via Gaussian elimination (flip .$a$ with .$d$, negate .$b$ and .$c$, then divide by .$ad-bc$) $$A = \\begin{bmatrix} a \u0026amp; b\\\\ c \u0026amp; d\\\\ \\end{bmatrix}$$ $$A^{-1} = \\frac{1}{ad-bc} \\begin{bmatrix} d \u0026amp; -b\\\\ -c \u0026amp; a\\\\ \\end{bmatrix}$$ .$ad-bc$ is the determinant, so we can check quickly if an inverse exists for a square matrix by checking if they determinant exists See slide 8 Determinant is the area the vectors form. So if they vectors form some zero-area (or volume in 3D) then it\u0026rsquo;s not one-to-one and thus not invertible Theorems # Theorem Note 6.1 # If .$A$ is an invertible matrix, then its inverse must be unique\nSuppose .$B_1, B_2$ are both inverses of the matrix .$A$. Then we have Notice that by associativity of matrix multiplication, the left hand side of the equation above becomes We see that .$B_1 = B_2$, so the inverse of any invertible matrix is unique. $$AB_1 = B_1 A = I$$ $$AB_2 = B_2 A = I$$\n$$AB_1 = I \\Longrightarrow B_2(AB_1) = B_2 I = B_2$$ $$B_2 (AB_1) = (B_2 A)B_1 = IB_1 = B_1$$ $$\\therefore B_1 = B_2$$\nAnother important property of inverses is that the “left” inverse and the “right” inverse are equal to each other. In particular\u0026hellip; Theorem Note 6.2 # If .$QP = I$ and .$RQ = I$, then .$P = R$. The matrix .$P$ can be thought of as the “right” inverse of .$Q$ and the matrix .$R$ can be thought of as the “left” inverse of .$Q$.\nWe start the proof by noticing that we know two things .$QP = I$ and .$RQ = I$. To move ahead, we can try setting .$QP = RQ$, but we cannot proceed from here, since the multiplication by .$Q$ is on different sides. So instead we take the equation .$QP = I$ and multiply both sides on the left by .$R$. This gives $$R(QP) = R(I) = R$$ Now, using the associative property of matrix multiplication we have that $$R(QP) + (RQ)P = IP = P$$ Here we used .$RQ = I$. Combining these two equations, we have that .$R = P$, and we are done Theorem Note 6.3 # If a matrix A is invertible, there exists a unique solution to the equation .$A \\vec x = \\vec b$ for all possible vectors .$\\vec b$.\nLet’s try to prove this. To do so, we need to prove two statements: That there exists at least one solution to the equation .$A \\vec x = \\vec b$, and that There exists no more than one solution to the equation .$A \\vec x = \\vec b$. For both of the above statements, .$\\vec b$ can be any vector in .$\\mathbb{R}^{n}$ Let’s prove the first statement first. Imagine we are given a vector .$\\vec b$. Consider the candidate solution .$\\vec x = A^{−1} \\vec b$. Observe that $$A \\vec x = A(A^{-1} \\vec b) = (AA^{-1}) \\vec b = \\vec b$$ Thus, our candidate solution satisfies the equation .$A \\vec x = \\vec b$, so there exists at least one solution to that equation! Now, let’s show the second statement \u0026ndash; that no more than one solution to the equation .$A \\vec x = \\vec b$ can exist. Consider a particular solution .$\\vec x$, so .$A \\vec x = \\vec b$. Pre-multiplying both sides of this equation by .$A^{−1}$, we obtain $$A^{-1}(A \\vec x) = A^{-1} \\vec b \\Longrightarrow \\vec x = A^{-1} \\vec b$$ Therefore, if .$\\vec x$ exists, it must be the particular vector .$A^{−1} \\vec b$. In other words, there exists at most one solution to the equation .$A \\vec x = \\vec b$, so we have proven the second statement Theorem Note 6.4 # If a matrix .$A$ is invertible, its columns are linearly independent.\nLet’s prove this theorem. We know that the statement “the columns of .$A$ are linearly independent” is equivalent to the statement “.$A \\vec x = \\vec 0$ only when .$ \\vec x = \\vec 0$.” This fact follows from the definition of linear independence: by definition, if .$\\vec v_1, \\dots, \\vec v_n$ are linearly independent, then .$\\sum_{i=1}^n x_i \\vec v_i$ is only .$\\vec 0$ when .$x_i = 0$. Using the column perspective of matrix multiplication (covered in Note 3), .$A \\vec x = \\sum_{i=1}^n x_i \\vec v_i$ where .$\\vec v_i$ is the .$i$th column of .$A$. Therefore, .$A \\vec x = \\vec 0$ only when all .$x_i = 0$. Therefore, we can rephrase what we’re trying to prove as $$A^{-1} \\text{ exists } \\Longrightarrow A \\vec x = \\vec 0 \\text{ only when } \\vec x = \\vec 0$$ To prove this, assume that .$A$ is invertible. Let .$\\vec v$ be some vector such that .$A \\vec x = \\vec 0$: $$A \\vec x = \\vec 0 \\text{ left-multiply by } A^{-1}$$ $$A^{-1} A \\vec v = I \\vec v = \\vec 0$$ $$\\vec v = \\vec 0$$ Theorem Lecture # .$A$ is invertible, iff the columns of are linearly independent.\nThat is, If columns of .$A$ are liner dependent then .$A^{-1}$ does not exist If .$A^{-1}$ exists, then the cols. of .$A$ are linearly independent Proof concept: Assume linear dependence and invertibility and show that it is a contradiction From linear dependence: .$\\exists \\vec \\alpha \\neq 0$ such that .$A \\vec \\alpha = 0$: But .$\\vec \\alpha \\neq 0$, hence, .$A^{-1}$ DNE $$A \\vec \\alpha = 0$$ $$A^{-1} A \\vec \\alpha = A^{-1} 0$$ $$I \\vec \\alpha = 0$$ Thus, the following statements are equivalent: Matrix .$A$ is invertible .$A$ has linearly independent columns .$A$ is full rank .$A \\vec x = \\vec b$ has a unique solution .$A$ has a trivial nullspace The determinant of is not zero "},{"id":11,"href":"/cogsci-c100/attention/","title":"4: Attention","section":"CogSci C100","content":" Physiological/neurological measures of attention # Attention: ability to focus on one thing and ignore others Attention is required for information to be encoded Novel stimuli capture attention \u0026ndash; something that\u0026rsquo;s different Can be something loud, bright, salient or even sudden sound of silence Physiological/neurological measures of attention Neurological correlates (ERP and PET studies) Activation of anterior and posterior attentional networks (frontal lobe) Orienting response Increase in heart rate and galvanic skin response Pupil dilation Pupil dilation reflects attentional effort \u0026ndash; pupils expand when people deep in thought If you are shown picture of someone you are romantically attracted to, your pupils dilate We are also more attracted to pictures of people with expanded pupils In marketing/advertising, pupils are enlarged in models In 19C, ladies used to put belladonna in their eyes Eye movement changes Eyes tend to move toward objects of attention (even auditory attention) A number of newly developed therapies focus specifically on maintaining eye contact as a way of enhancing attunement and detecting when a person is dissociating (e.g., in treatment of PTSD and dissociative disorders) EMDR (Eye Movement Desensitization and Reprocessing) for treatment of trauma Activation of opposite hemispheres of the brain Selective attention # Selective attention: focus of conscious awareness on a particular stimulus Dichotic Listening Task: Filter Model of Attention # Donald Broadbent formulated one of the first information-processing models in psychology (1958); based on dichotic listening experiments (shadowing task): Participants are instructed to repeat message played in one ear while ignoring message played in other ear Participants are very good at filtering out irrelevant information: they fail to notice even changes in language in the unattended ear or words that have been repeated dozens of times However, they are usually able to pick out the gender of the speaker, words like “Fire,” and their own name (“cocktail party effect”) In general, people tend to consciously attend to one source of information and ignore other sources, but people vary in degree to which they focus attention and ignore distractions Results of dichotic listening experiments supports filter model of attention: everything is picked up on some level, but at any given moment, we focus our awareness on only a limited aspect of all that we are experiencing Broadbent proposed a flowchart model of selective attention to explain these results This type of flowchart model has become a standard way for cognitive scientists to describe and explain different aspects of cognition\nImplications: We can understand how a cognitive system as a whole works by understanding how information flows through the system Stimuli that we do not notice can affect us Participants were unable to recognize simple, novel tunes that had been played in unattended ear, but when asked to rate how much they liked different tunes, they preferred the ones previously played On the other hand, research suggests that we consciously take in far less information than we think, even about objects that fall within the beam of our attentional spotlight Change blindness experiment (Simons \u0026amp; Levin, 1998) # Experimenter stopped people on a college campus and asked them for directions During the conversation, two confederates walked between them carrying a large door In the few seconds in which the first experimenter was obscured, another experimenter took his place and continued the conversation Only half the participants noticed the change \u0026ndash; even when specifically asked about it Implications Relatively dramatic changes can go undetected Due to attempt to achieve object constancy in spite of changes in sensory input (e.g., shadows, occlusion) Divided Attention # Divided Attention: Performance suffers when people have to attend to different stimuli at the same time; however, performance can improve with practice on some tasks, especially if one or both of the tasks are easy or well-learned. Experienced drivers can converse with passengers while driving With practice, people can learn to read while taking dictation or categorizing words (after several months of practice) These feats of divided attention are possible because with practice, actions become automatic \u0026ndash; they no longer require much attention However, it is virtually impossible to perform two tasks that require deep cognitive processing at the same time Talking on cell phone (even hands-free) increases risk of accident 4 times Texting increases risk 23 times (!) (McEvoy, Stevenson, McCartt et al., 2005; Olson, Hanowski, Hickman et al., 2009) Also, interestingly, research has also indicated that those who think they are best at multi-tasking are actually the ones who show the most severe impairments when multitasking! (Sambonmatsu, Strayer, Medeiros-Ward et al, 2013) Internet use in class Study conducted at Michigan State University (Ravizza, Uitvlugt, \u0026amp; Fenn, 2017) found that, for every 100 minutes in class, students spent 40 minutes on non-academic website, mostly social media 5 minutes on websites that related to course material Increased non-academic internet usage in class had a negative correlation with final exam scores Automaticity: advantages and disadvantages # Stroop task # Identification of color is slower in (c) because reading of word is automatic People with phobic disorders have difficulty identifying ink color of words related to feared objects Those with eating disorders take longer to report words related to body shape Those who show an attentional bias toward suicide-related words are more likely to make a suicide attempt within the following 6 months (Cha, Najmi, Park et al., 2010) Evan Longoria’s catch # What are some advantages to automatic processing? Efficiency, multitasking, allows us to pay attention to higher levels of processing (you don\u0026rsquo;t think about the letters you type, rather, the content of what you\u0026rsquo;re trying to convey) What are some potential drawbacks? Lack of focus (car crashes), miss the rich and vividness of life “Raisin” meditation Theories of attention # Bottleneck theory # Quantity of information to which we can pay attention is limited Not widely supported by research You can multitask in many ways Automatic vs. controlled processing # Automatic processing Easy or well-practiced tasks Processing is parallel and involuntary i.e scanning list of student names for your own name Controlled processing Difficult or unfamiliar task Processing is serial i.e scanning list of student names for three unfamiliar names Visual search study: automatic processing occurs only when target and irrelevant items belong to different sets Find the two: 3 7 6 2 D 2 M R Feature-integration theory # Distributed attention (a) Can be used to register single features automatically (searching for blue X among red X’s and O’s or searching for feature present) Automatic, parallel processing Focused attention (b) Used to search for combinations of features and for a feature that is missing (searching for blue X among red X’s, red O’s and blue O’s or searching for feature missing) Controlled, serial processing E.x. is it easier to find the slanted green line in (a) or (b)? Attention-Deficit/Hyperactivity Disorder # Attention-deficit/hyperactivity disorder: difficulty sustaining attention, often fails to pay close attention to details on tasks, easily distracted by extraneous stimuli, fidgets and squirms constantly, always “on the go,” often interrupts others in conversations and games Timing issue; subjects feel like they need to get their idea out or it will disappear Treatment: Medications: Ritalin and other stimulants Supposed to attempt psychotherapy but that isn\u0026rsquo;t the case nowadays (you get much better results) Idea is that people with ADHD are constantly under-aroused, that\u0026rsquo;s why they fidget, so stimmys artificially bring them to a normal baseline Parenting skills training classes \u0026ndash; parents taught to support/ignore certain behaviors Behavioral therapy: focusing on organization, scheduling, etc. Alternative therapies (e.g., biofeedback training) Research has indicated that the more kids with ADHD wiggle and fidget, the better they do on cognitive tests (Sarver, Rapport, Kofler et al., 2015) Biofeedback # Biofeedback: form of operant conditioning in which person is trained to bring autonomic processes under conscious control A signal, such as a tone or light, is made to come on whenever a certain desirable physiological change occurs Person is instructed to try to keep the signal on for increasing periods of time Technique has been successfully used to teach people to reduce blood pressure, produce more regular heartbeats, decrease incidence of headaches and increase attention Neurofeedback is now also being used to treat ADHD and chronic pain fMRI neurofeedback for ADHD increases activation in the right inferior frontal cortex and is associated with clinical symptom improvement (Rubia, Criaud, Wulff, et al., 2019) Reinforcement can be used to teach people to regulate many physiological responses that they are normally not aware of\nTV # Connection between ADHD and TV viewing/video games Many parents say, “My son can’t possibly have ADHD because he can sit for hours concentrating on a video game, so there is clearly nothing wrong with his attention span.” In fact, a child’s ability to stay focused on a screen, though not anywhere else, is actually characteristic of ADHD Not clear though whether fascination with the screen may be a cause or an effect of attention problems - or both Evidence that TV watching may increase attention problems: Study found that television shows impaired children’s executive functioning, particularly fantastical shows (Lillard, Drell, Richey et al., 2015) Probably due in large to habitual reliance on bottom-up processing, that is, attention-grabbing external stimuli Herbert Krugman found that In less than one minute of television viewing, person\u0026rsquo;s brainwaves switches from beta waves to primarily alpha waves (lower arousal state associated with ADHD) When person stops watching television and begins reading a magazine, the brainwaves revert to beta waves When watching TV, all the information is spoon-fed to you. When you read, you are forced to use your imagination to interpret the media Recent study published in JAMA Pediatrics found that after controlling for age, gender and income, 3-5 year old children with higher use of screen-based media Had lower measures of structural integrity and myelination in neurons Scored lower on cognitive tests (Hutton, Dudley, Horowitz-Kraus et al., 2020) Extra: American Academy of Pediatrics recommendations:\nChildren under 18 months should avert their eyes from TV and screen media at all times For children 2 to 5, screen time should be limited to no more than 1 hour per day with the parent present "},{"id":12,"href":"/e-29/4/","title":"4: Additive Processes: Light-based, etc.","section":"Engineering 29","content":" Processes based on light-sensitive liquids # Stereolithography (SLA) # Also called Digital Light Printing (DLP) Thermoforming molds are most commonly made with stereolithography, i.e orthodontic aligners Used because the molds need complete geometric customizability A tray, bath or vat of a photo-sensitive liquid (a resin) is locally crosslinked (solidified) with a scanning laser beam or projected light pattern (i.e Photoinitiator), usually violet (~405 nm) or ultraviolet (~365 nm) Unwanted bonding can be avoided by making the base oxygen-permeable (e.g. by making it out of an elastomeric material, as in the Carbon 3D systems) since oxygen inhibits the photocrosslinking reaction and an oxygen-rich layer will form in the resin just above the permeable window because of air diffusing through the window Covalent bonds form between the molecules in the liquid, forming a solid material (this reaction is called photocrosslinking) Absorption of photons as they pass through the liquid limits crosslinking to a small layer (typically tens of μm thick) at a time Parts are more isotropic (versus FDM), have almost matte finish Two machine design approaches: Right: Light shines up through a window; part gradually drawn up out of a tray of resin Requires more liquid Projection Printing (below): Light shines down onto liquid surface; platform moves down into the vat as the part builds up Printed object drawn upwards out of tray of resin Separation from tray: peeling or O2-inhibited dead layer In older SLA configurations, the platform is submerged in the resin, the illumination comes in from above the resin surface, and the platform moves down into the resin bath after each layer was printed. Sometimes a mechanical ‘wiper’ is used to distribute a thin, uniform layer of uncured resin on top of the component before the next exposure step In new designs, the illumination is shone through a window in the bottom of the resin tray, and the platform moves upwards, pulling the object out of the tray as it prints. This approach has the advantage of requiring a less deep resin tray, and eliminating the need for a wiper, but means that the machine designer must ensure that the resin does not bond to the window at the base of the resin tray. Resin solidification in stereolithography # Beer-Lambert model: Absorption of the illuminating light means that exposure dose .$E(z)$ (measured in W/m.$^2$) of the light is at its highest at the point where the light enters the resin, and decreases exponentially with depth below resin surface .$z$: $$E(z) = E_0 e^{-z/D_p}$$ This curve (a property of the dye/resin) follows a first-order exponential decay This is common across most materials that absorb light; for each unit traveled by the photon, there\u0026rsquo;s a finite probability of getting absorbed. E.x. 10% of photos are absorbed in the first 10 microns, 10% of the remaining photons in the next 10 microns, etc. A newer, potentially faster, approach for processes that rely on photocrosslinking is to expose each 2D layer in one single exposure step, rather than scanning a beam of light. The digital micromirror devices that are an integral component of modern video projectors can be used to pattern the photocrosslinking wavelength of light. Even with this layer-by-layer approach, there is still a need to Whether 3D printing will always involve this slicing approach is an open question. Since slicing often imparts anisotropic mechanical properties to printed components and limits their layer-to-layer strength, there may be a strong case for developing full 3D control over the material deposition process, so that, for example, the deposition head would move in the x, y and z directions simultaneously. Path planning to avoid collision between the machine and the component would be a significant challenge slice a 3D object into multiple 2D images. .$D_p$ is the penetration depth, a resin property The resin at a particular location will photocrosslink when the total illumination dose exceeds a critical value called the curing dose The depth into the resin at which the curing dose is reached is termed the curing depth .$E$ is the dose: the time integral of power intensity over the layer time Measured in J/m.$^2$ In projector-based systems, the source is .$E_0 = I_0\\ t_\\text{layer}$ .$I_0$ is the illumination irradiance/power intensity, which is proportional to the energy of the incoming particles per area W/m.$^2$ In laser-scanning systems, .$E_0$ is a function of illumination power intensity (W/m.$^2$), beam diameter w (m), and scanning speed, v (m/s): Gelation just occurs at the curing depth, .$D_c$ where the resin receives the curing dose, .$E_c$: $$E_c = E_0 e^{-D_c/D_p} \\Longrightarrow D_c = D_p \\ln \\bigg( \\frac{E_0}{E_c} \\bigg)$$ Cured material may not be very strong at curing depth, so layers typically overlap in .$z$ direction to ensure material is all well-gelled Thermal and light curing are done to finish these surfaces Hydrodynamic stresses in dead zone # This is the central rate-limiter in stereolithography Suction pressure forms around bottom of part Proportional to the square of the part width Thermal management for high print speeds # Oil cools down print, enables meters per hours printing SLA vs FDM # Type Pros Cons SLA Can reduce layer thickness to much smaller levels than FDM (see especially the Carbon 3D system which claims sub-micrometer layer thicknesses) Only a single resin material can be patterned per object Can use digital micromirror devices to print an entire layer in one flash – potentially higher throughput than FDM Mechanical properties of photocrosslinkable resins lag behind those of FDM filaments With thinner layers, can get more isotropic mechanical strength than in layered FDM FDM Very wide range of printable filament materials now available, with specialist mechanical, thermal, optical, and electrical properties Anisotropic mechanical strength because of layered deposition – weaker layer-to-layer than within layers Apparatus can be very affordable (a few hundred dollars in some cases) Surface roughness comparable to extruded filament diameter – cannot directly print shiny surfaces Computed Axial Lithography # CAL: Developed at Cal! A layer-free tomographic approach to photopatterning involving synthesis of 3D light dose Benefits: higher speed, the ability to print into more viscous materials, and the ability to avoid the use of solid supporting structures for delicate geometries. Enables a wider range of resin materials (notably higher viscosity resins) There a discrete number of layers, but then end up being \u0026lsquo;smeared\u0026rsquo; so they are relatively smooth Poly Jet printing # Also called ink-jet printing Used when a high degree of material heterogeneity is needed in a single object (e.g. in color, or mechanical stiffness) These have been commercialized, for example, by the company Stratasys with the brand name “PolyJet” Inkjet-dispensed polymer inks are built up layer by layer and photocured (crosslinked). Layers down to ~16.$\\mu$ thick. Elastic modulus and color can be varied spatially by mixing inks A wide range of mechanical properties (including elastomers) are now possible Rigid glassy polymers to soft rubber-like performance possible Note that these are “simulated” materials: the printed inks are photocured whereas the final production material may be, e.g., thermoplastic Typical machines can carry about five different material cartridges at a given time, and can tune material properties by depositing finely interspersed patterns of droplets of these materials. Far more expensive than FDM or basic SLA tools. Electrically conductive materials are beginning to emerge Available in Jacobs Hall: Objet260 Connex3 Objet350 Connex3 Interchangeable ink cartridges Soft, rubbery, tacky: Tango range ~ few MPa Young’s Modulus Rigid: Vero (white, black, yellow, magenta, cyan, clear available) Digital ABS ~ few GPaYoung’s Modulus Soluble support: FullCure705 Remove by hand, water or NaOH solution Powder/binder methods # Selective laser Sintering (SLS) # Use a heat source (usually a scanning laser beam) to sinter powders of the structural material directly, without a binder Can be applied to thermoplastic polymers (esp. Nylon), metals, and even ceramics A thin layer of the powder is rolled across the printing bed, and then a laser is scanned across the layer, delivering enough heat that the powder particles melt only at their surfaces. Atoms or molecules at the contacting interfaces between particles diffuse faster in the elevated temperature and cause adjacent particles to connect to each other. Powder particles are heated enough that surfaces melt\u0026ndash; atoms/molecules at their surfaces interdiffuse, bonding the particles. Because the powder is not fully melted, parts usually retain some porosity Can weaken the component compared to continuous bulk material The pores can serve as stress concentrators and promote crack propagation across the component The surface finish can be rough — the roughness is comparable to the metal particle size Selective Laser Melting (SLM) # Also called Direct Metal Laser or Directed Energy Deposition Laser (infrared light) actually melts the powder Removes porosity by melting the particles Thermal gradients can be a problem Computer generators now account for thees gradients and deformations Changes pattern order and energy distribution There has been a move away from metal SLS towards complete melting of the metal powder that is being fused. Greater control of grain structure and lower porosity than with selective laser sintering Fully dense structures achievable Scanning laser beam completely melts the powder during processing No binders being employed but rather the structure of the component being built up directly from metal. These processes may be layer-by-layer or, increasingly commonly, via a direct spray Isn\u0026rsquo;t feasibly monetarily right now Examples of components # 3D printed gas turbine blades Polycrystalline nickel superalloy Layer by layer powder fusion Survived 13,000 rpm at 1250 °C Application: pressure sensor housing in General Electric jet engine compressor Made using SLM Co-Cr alloy 19 of them in the GE LEAP engine Replaces a casting process Development time up to a year faster than using casting Example of remanufacturing with SLM Used when material is very expensive and not recyclable/reusable Electron-beam melting # Like SLM, produces fully dense parts Operates in vacuum to avoid metal oxidation and so that the electrons can follow a straight path without colliding with air molecules. In contrast, SLM uses an inert gas atmosphere Vacuum allows higher temperatures to be used (\u0026gt; 800 K) Reduces oxidation and porosity due to adsorbed gases Materials more limited than SLM Examples: Ti grade 2, Ti6Al4V, Inconel 718, CoCrMo Features down to a few micrometers are possible although printing times are slow compared with SLS or even SLM Hybrid subtractive/additive manufacturing # In the last few years hybrid machines have been demonstrated that combine a subtractive machining center (lathe, mill) with an additive capability The idea is to start with the smallest possible piece of stock material, deposit additional features of a component (e.g. flanges) additively on to that stock, and then machine back to the final shape, benefiting from the higher precision and surface finish of turning and milling. A machine made by DMG-Mori is now on the market and uses a metal spray together with a laser to deposit material, rather than a powder bed approach. Powder-binder method # One approach to printing objects from powders (which may include metal, polymer or ceramic powders) is to selectively dispense a binder (a sprayable ‘glue’, usually a polymer) on to layers of dry powder, holding the powder together in specific locations. After printing, the part is removed from the surrounding unbound loose powder. For polymeric parts, the binder can be colored and constitutes part of the final object. For metal or ceramic printing, the binder is subsequently be driven off (vaporized) with high heat after the whole part has been printed, and the powder is also sintered together by the heat (see below for explanation of sintering).\nPolymer powder and deposited binder (may be colored). Left: schematic illustration of the threedimensional-printing process. Source: After E. Sachs and M. Cima. Can also be applied to binders with metal powders, which are later sintered/fused (Desktop Metal, Markforged…) Emerging methods # Directly fuse thermoplastic powders layer-by-layer Something like a 3D photocopier Powdered materials # Polymer powder production Ball milling: grind below glass transition Metal powder production Solid-state reduction: crush ore, pass through furnace Atomization:rapidly freeze molten spray Electrolysis: deposition in powder form (e.g. Cu) Chemical, e.g. precipitation from solution Size distributions typically Gaussian SLM: typical average ~ 30 microns The more spherical, the more easily it flows Some powders more porous Design for additive manufacturing # Stretch-dominated lattices e.g. octet truss structure One of the stiffest structures known Produced, e.g., by projection microstereolithography Tuning material properties with structure # Printed cellular structures can offer new classes of material Current challenges with 3D printing # Speed – for a single material and non-freeform geometry, machining is still probably quicker; injection molding is certainly faster Lower mechanical strength – directionality, porosity, surface flaws Surface finish – roughness often of many microns Resolution – from ~ 0.25 mm for basic FDM down to a few microns for SLA/SLS/SLM. Sub-micrometer resolution possible for two-photon SLA. Expense of input material – fine filaments, powders, inks: some proprietary consumables Potential future applications of 3D printing # Food? Clothes? Human organs? Houses? What are the challenges of 3D printing? Intellectual property; forgery Difficulty of regulating production (e.g. of weapons; organs) Carbon footprint (energy input) of 3D-printed components vs traditionally manufactured components Overall will 3D printing create or destroy jobs? "},{"id":13,"href":"/eecs-16a/4/","title":"4: Vector Spaces \u0026 Eigenstuff","section":"EECS 16A","content":" 02-15: Vector Spaces: Null Spaces and Columnspaces # Slides Notes 7 8 Important Jargon # Rank a matrix .$A$ is the number of linearly independent columns Nullspace of a matrix is the set of solutions to .$A \\vec x = 0$ A vector space is a set of vectors connected by two operators: .$+, \\times$ \u0026mdash; page 48 A vector subspace is a subset of vectors that have “nice properties” \u0026mdash; page 50 A basis for a vector space is a minimum set of vectors needed to represent all vectors in the space Dimension of a vector space is the number of basis vectors Column space is the span (range) of the columns of a matrix Row space is the span of the rows of a matrix Vector Spaces # A vector space .$\\mathbb{V}$ is a set of vectors and two operators .$+, \\cdot$ that satisfy: Vector Addition\nAssociative: .$\\vec u + (\\vec v + \\vec w) = (\\vec u + \\vec v) + \\vec w$ Commutative: .$\\vec u + \\vec v = \\vec v + \\vec u$ Additive Identity: There exists an additive identity .$\\vec 0 \\in \\mathbb{V}$ such that .$\\vec v + \\vec 0 = \\vec v$ Additive Inverse: There exists .$- \\vec v \\in \\mathbb{V}$ such that .$\\vec v + (-\\vec v) = \\vec 0$. We call .$-\\vec v$ the additive inverse of .$\\vec v$. Closure under vector addition: The sum .$\\vec v + \\vec u$ must also be in .$\\mathbb{V}$ Scalar Multiplication\nAssociative: .$\\vec \\alpha(\\beta \\vec v) = (\\alpha \\beta) \\vec v$ Multiplicative Identity: There exists .$1 \\in \\mathbb{R}$ where .$1 \\cdot \\vec v = \\vec v$ Distributive in vector addition: .$\\alpha (\\vec u + \\vec v) = \\alpha \\vec u + \\alpha \\vec v$ Distributive in scalar addition: .$(\\alpha + \\beta)\\vec v = \\alpha \\vec v + \\beta \\vec v$ Closure under scalar multiplication: The product .$\\alpha \\vec v$ must also be in .$\\mathbb{V}$. ... for any .$\\vec v, \\vec u, \\vec w \\in \\mathbb{V}; \\alpha, \\beta \\in \\mathbb{R}$ These can be grouped by axioms of closure, addition, and scaling shown on slide 10 For example .$ \\mathbb{R}^{n}$ is the vector space of all .$n$-dimensional vectors. In fact, the set of all matrices the same size is also a vector space .$ \\mathbb{R}^{n \\times o}$ since it fulfills all of the properties above as well In this class we will generally only deal with vector spaces containing vectors in .$\\mathbb{R}^{n}$. Subspaces # A subspace .$\\mathbb{U}$ consists of a subset of .$\\mathbb{V}$ in vector space (.$\\mathbb{V}, \\mathbb{F}, +, \\cdot$). .$\\mathbb{U} \\subset \\mathbb{V}$ and have 3 properties Contains .$\\vec 0 $, i.e., .$\\vec 0 \\in \\mathbb{U}$ Closed under vector addition: .$\\vec v_1, \\vec v_2 \\in \\mathbb{U} \\Longrightarrow \\vec v_1 + \\vec v_2 \\in \\mathbb{U}$ Closed under scalar multiplication: .$\\vec v \\in \\mathbb{U}, \\alpha \\in \\mathbb{F} \\Longrightarrow \\alpha \\vec v \\in \\mathbb{U}$ Examples on slide 13 Intuitively, a subspace is a closed subset of all the vectors in .$ \\mathbb{V}$. Any linear combination of vectors in the subspace must also lie in that subspace. Just as basis and dimension are defined for vector spaces, they have equivalent definitions for subspaces. Basis for a Subspace: set of linearly independent vectors that span the subspace (minimal set of subspace-spanning vectors) Subspace Dimension: number of vectors in subspace-basis Basis # Basis: Given a vector space .$\\mathbb{V}$, a set of vectors .$\\{\\vec v_1, \\dots \\vec v_n\\}$ is a basis of the vector space if it satisfies the following properties: .$\\vec v_1, \\dots, \\vec v_n$ are linearly independent vectors .$\\text{span}(\\{\\vec v_1, \\dots, \\vec v_n\\}) = \\mathbb{V} \\Longrightarrow \\forall \\vec v \\in \\mathbb{V}, \\exists \\alpha_1, \\dots, \\alpha_{n-1} \\in \\mathbb{R}$ such that .$\\vec v_1 = \\alpha_1 \\vec v_2 + \\dots \\alpha_{n-1} \\vec v_n$ Minimum set of vectors that spans a vector space\nA basis of a vector space is the minimum set of vectors needed to represent all vectors in the vector space. If a set of vectors is linearly dependent and “spans” the vector space, it is still not a basis \u0026ndash; we can remove at least one vector from the set and the resulting set will still span the vector space Basis is not unique # Intuitively, think about multiplying one of the vectors in a given basis by a nonzero scalar will not affect the linear independence or span of the vectors. We could alternatively construct another basis by replacing one of the vectors with the sum of itself and any other vector in the set. Mathematically, suppose that .$\\{\\vec v_1, \\dots, \\vec v_n \\}$ is a basis for the vector space we are considering. Thus .$\\{\\alpha \\vec v_1, \\dots, \\vec v_n \\}$ where .$\\alpha \\neq 0$ is also a basis because, just as we’ve seen in Gaussian elimination row operations, multiplying a row by a nonzero constant does not change the linear independence or dependence of the rows. We can generalize this to say that multiplying a vector by a nonzero scalar also does not change the linear independence of the set of vectors. In addition, we know that .$\\text{span}(\\{ \\vec v_1, \\dots, \\vec v_n \\}) = \\text{span}( \\{\\alpha \\vec v_1, \\dots, \\vec v_n \\} )$ Any vector in .$\\text{span}(\\{ \\vec v_1, \\dots, \\vec v_n \\})$ can be created as a linear combination of the set .$\\text{span}(\\{ \\alpha \\vec v_1, \\dots, \\vec v_n \\})$ by dividing the scale factor on .$\\vec v_1$ by .$\\alpha$. We can use a similar argument to show that .$\\{\\alpha \\vec v_1, \\dots, \\vec v_n \\}$ is also a basis for the same vector space. To generalize, for .$\\mathbb{R}^{N}$, any .$N$ (and only .$N$) linearly independent vectors form a basis\nDimension # Dimension: The dimension of a vector space is the number of basis vectors. Since each basis vector can be scaled by one coefficient, the dimension of a space as the fewest number of parameters needed to describe an element or member of that space. The dimension can also be thought of as the degrees of freedom of your space \u0026ndash; that is, the number of parameters that can be varied when describing a member of that space. A vector space can have many bases, but each basis must have the same number of vectors:\nSuppose a basis for the vector space we’re considering has .$n$ vectors. This means that the minimum number of vectors we can use to represent all vectors in the vector space is .$n$, because the vectors in the basis would not be linearly independent if the vector space could be represented with fewer vectors. Then we can show that any set with less than .$n$ vectors cannot be a basis because it does not have enough vectors to span the vector space \u0026ndash; there would be some vectors in the vector space that cannot be expressed as a linear combination of the vectors in the set. In addition, we can show that any set with more than .$n$ vectors must be linearly dependent and therefore cannot be a basis. Combining the two arguments, we have that any other set of vectors that forms a basis for the vector space must have exactly .$n$ vectors! Column Space # The range/span/column space of matrix .$A \\in \\mathbb{R}^{m \\times n}$ \u0026ndash; which we can represent as a set of vectors .$\\{ \\vec a_1, \\dots \\vec a_n \\}$ \u0026ndash; is a set of all possible linear combinations: $$\\text{span}\\big(\\{\\vec a_1, \\dots, \\vec a_n\\}\\big) = \\Bigg\\{\\sum_{i=1}^N \\alpha_i \\vec a_i\\ |\\ \\alpha_1, \\dots, \\alpha_n \\in \\mathbb{R} \\Bigg\\} = \\big\\{A \\vec x =\\ \\vec x \\in \\mathbb{R}^{n}\\big\\}$$ That is, the column space of a matrix .$A \\in \\mathbb{R}^{m \\times n}$ is the span of the .$n$ columns in .$A$ It\u0026rsquo;s the space of all outputs that the operator can map to. Thinking about .$A$ as a linear transformation from .$ \\mathbb{R}^{n} \\to \\mathbb{R}^{m}$, the column space is effectively the set of all outputs that this matrix can transform input vectors to Note that in the general case, input vectors and output vectors can be different lengths The column space describes all possible output vectors .$\\vec b = \\mathbb{R}^{m \\times 1}$ It can be shown that .$\\text{span}(A)$ forms a subspace of .$ \\mathbb{R}^{m}$ Note that .$\\text{span}(A)$ is not necessarily .$ \\mathbb{R}^{m}$ Row Space # Similarly, the row space is the span of the .$n$ rows Rank # The rank of .$A$ is defined as the dimension of the column space of .$A \\in \\mathbb{R}^{m \\times n}$ .$\\text{rank}(A) = \\text{dim(span(A))}$ .$\\text{dim(span}(A\\text{)) ≡ dim(col(}A))$ It’s all too easy to confuse an actual space consisting of vectors, like a matrix range describing the output (column) space, with the dimension of that space, which is just a single scalar number. Keep them straight! .$ \\text{rank}(A) = \\text{dim(span}(A)) \\leq \\text{min}(m, n)$ This is at most .$m$, but certainly can be less, since an arbitrary .$A \\in \\mathbb{R}^{m \\times n}$ is not guaranteed to have columns whose span is all of .$ \\mathbb{R}^{m}$ Consider the simple counterexample of the zero matrix .$\\vec 0 \\in \\mathbb{R}^{m \\times n}$, which maps all .$n$-dimensional input vectors to the .$m$-dimensional all-zero vector. In general, using the column-wise representation of matrix-vector multiplication we can show that .$\\text{rank(}A)$ is the number of linearly independent columns in .$A$. Any output vector can be represented as a linear combination of the columns of .$A$. But some of these columns might themselves be linear combinations of other columns, which means we can replace any redundant column with a weighted sum of the other columns. By removing all redundancies, we find that a matrix with .$k \\leq \\text{min}(n, m)$ linearly independent column vectors can \u0026ldquo;unlock\u0026rdquo; exactly .$k$ dimensions in the output. Thus, we find that .$\\text{rank}(A)$ also equals the number of pivots in the RREF of .$A$. Since each pivot must belong to a row and a column, the number of pivots in .$A \\in \\mathbb{R}^{m \\times n}$ is limited by the smaller dimension. For a tall matrix .$m \u0026gt; n$, the columns are the limiting dimension; for a wide matrix .$n \u0026gt; m$ the rows are. Null Space # The null-space of .$ \\mathbb{R}^{m \\times n}$ is the set of all vectors .$\\vec x \\in \\mathbb{R}^{m}$ such that .$A\\vec x = 0$ $$\\text{null}(A) = \\big\\{\\vec x\\ |\\ A \\vec x = \\vec 0, \\vec x \\in \\mathbb{R}^{m}\\big\\}$$ That is, the set of all inputs that get mapped to .$\\vec 0$ by .$A$ $\\text{dim(null}(A))$ can be interpreted as the number of input directions for which the output is \u0026ldquo;compressed\u0026rdquo; down to zero. We know that it can be at most .$m$, since all of the input vectors have .$m$ components. It\u0026rsquo;s the set of vectors not in columns space, that is, the number of linearly dependent columns: $$m - \\text{dim(span}(A)) = \\text{dim(null}(A))$$ The loss of dimensionality from the input space to the output space shows up in the nullspace. .$\\vec 0$ is always in the null space — trivial Null space This wouldn\u0026rsquo;t hold if we had affine (instead of linear) functions Null space DNE when the determinant is not zero \u0026ndash; see last week Procedure to Compute a Null-Space # Computing the nullspace of .$A$ requires us to solve .$A \\vec x = \\vec 0$ \u0026ndash; the procedure is as follows: Put .$A$ in RREF. Initialize the set .$\\mathbb{S} = \\{ \\vec 0 \\}$. Check each column for leading entries and find the number of .$F$ree and .$B$asic variables. if .$F = 0$, stop and skip to the last step. if .$F \\neq 0$, repeat the following for each free variable: Set that free variable to .$1$, and all others to zero. Solve .$A \\vec x$ under these conditions; add the solution vector to .$\\mathbb{S}$. Conclude that .$\\text{null}(A) = \\text{span}(\\mathbb{S})$. Example is given on page 37-38 Rank-Nullity Theorem # How is the number of free variables related to the total number of columns in a matrix .$A \\in \\mathbb{R}^{m \\times n}$? Well, each column of a matrix either contributes a \u0026ldquo;new direction\u0026rdquo; to the output or it is redundant with other columns and their already-discovered directions. In other words, each of .$n$ columns adds a dimension to .$\\text{span}(A)$ or to .$\\text{null}(A)$. Therefore, the following holds: $$\\text{dim(span}(A)) + \\text{dim(null}(A)) = n$$ $$\\text{rank}(A) + \\text{dim(null}(A)) = n$$\n02-17: Eigenvectors, values # Eigenvectors and Eigenvalues # Consider a square matrix .$ A \\in \\mathbb{R}^{n \\times n}$. An eigenvector of .$A$ is a nonzero vector .$\\vec x \\in \\mathbb{R}^{n}$ such that $$A \\vec x = \\lambda \\vec x$$ where .$\\lambda$ is a scalar value, called the eigenvalue of .$\\vec x$.\nThat is, an eigenvector represents a sort of stability point: vectors aligned with an eigenvector will not change direction under a linear transformation .$A$ Rather, they will simply be scaled by some factor. Note that eigenvectors are a property of the matrix itself and do not depend on the specific vector being transformed (input) The eigenvalue describes this stretching or compressing factor for vectors aligned with an eigenvector This means any vector that’s \u0026lsquo;some\u0026rsquo; multiple of the eigenvector, when it’s transformed by .$A$, will become a scaled version of itself that\u0026rsquo;s a \u0026lsquo;some\u0026rsquo; multiple of the eigenvalue Example on page 45 Geometrically, an eigenvector, corresponding to a real nonzero eigenvalue, points in a direction in which it is stretched by the transformation and the eigenvalue is the factor by which it is stretched Because these two terms are so commonly used in conjunction, we often refer to an eigen(value/vector) pair Note that scaling a given eigenvector for an eigenvalue will still produce a valid eigenvector, since the vector’s direction will not be changed Given non-invertible .$A \\in \\mathbb{R}^{n \\times n}$ there are at least .$1$ and at most .$n$ eigenvalues Given non-invertibility, some .$\\lambda_i = 0$, so we have at least 1 eigenvalue. Indeed, all eigenvalues can be .$0$ (such as for .$\\vec 0$)! However, non-invertibility does not place any other restrictions on our set of eigenvalues, so all other .$n-1$ eigenvalues can be distinct from this .$\\lambda_i$ Properties to know A matrix is uninvertible iff .$0$ is an eigenvalue (because there exists a vector .$\\vec v$ such that .$A \\vec v = \\vec 0$. A scalar times an eigenvector is still an eigenvector: .$(A(c \\vec v) = c A \\vec v = c \\vec v = (c \\vec v))$ Eigenvectors with distinct eigenvalues are linearly independent (eigenvectors in the same span have the same eigenvalue) .$A^{-1} \\vec v = \\lambda^{-1} \\vec v$ .$A \\vec v = \\lambda \\vec v \\Longrightarrow A^{-1} A \\vec v = A^{-1} \\lambda \\vec v \\Longrightarrow \\vec v = \\lambda A^{-1} \\vec v \\Longrightarrow \\lambda^{-1} \\vec v = A^{-1} \\vec v$ Determinants # The determinant is a quantity we can define for any square matrix The determinant is nonzero if and only if the matrix is invertible and the linear map represented by the matrix is an isomorphism For a .$2 \\times 2$ matrix, the formula is: The absolute value of .$ad − bc$ is the area of the parallelogram, and thus represents the scale factor by which areas are transformed by .$A$. $$\\text{det}\\Bigg( \\begin{bmatrix} a \u0026amp; b \\\\ c \u0026amp; d \\\\ \\end{bmatrix}\\Bigg) = ad - bc$$ If linearly dependent, some vectors will lie on top of each other so the area will be zero Similarly in 3D, if any column vectors are multiples of each other, we \u0026ldquo;squash\u0026rdquo; a volume into a plane or a line. That is, determinant of any square matrix is zero if the columns are linearly dependent. For a .$3 \\times 3$ matrix, the shape is a parallelipiped, and we form hyper-volumes in higher dimensions. We can calculate determinants for higher dimension matrices as well, as described here The determinant of the transpose of .$A$ equals the determinant of .$A$: .$\\text{det}(A) = \\text{det}(A^\\text{T})$ \u0026lsquo;Proving\u0026rsquo; this geometrically on a whiteboard is fun, try out the .$2 \\times 2$ case Therefore, if .$A^\\text{T}$ has an eigenvalue .$\\lambda$, then .$A$ also has the eigenvalue .$\\lambda$ because .$\\text{det}(A - \\lambda I) = \\text{det}(A^\\text{T} - \\lambda I)$ This implies that in all the properties mentioned above, the word \u0026ldquo;column\u0026rdquo; can be replaced by \u0026ldquo;row\u0026rdquo; throughout For example, viewing an .$n \\times n$ matrix as being composed of .$n$ rows, the determinant is an .$n$-linear function. See this article which relies on the idea of elementary matrices (not covered) or this more advanced, out-of-scope stackoverflow post that deals with 16B-level topics (and beyond) Computing Eigenvalues and Eigenvectors # Solving this equation for nonzero (nontrivial) solutions .$\\vec x$ will yield our eigenvectors: $$A \\vec x = \\lambda \\vec x \\label{a}\\tag{1}$$ $$\\Longrightarrow (A - \\lambda I_n) \\vec x = \\vec 0_n$$ $$\\Longrightarrow A\u0026rsquo; \\vec x = \\vec 0_n$$ Note that .$A$ and .$I_n$ are fixed and only 1 parameter here that can vary in this equation: .$\\lambda$ We want to find values of .$\\lambda$ such that .$A\u0026rsquo; = (A − \\lambda I_n)$ has linearly dependent columns That is, we want to find values of .$\\lambda$ that cause the determinant of .$A\u0026rsquo;$ to become zero Linear dependence of the columns creates a nontrivial null-space for .$A'$ .$N$th order characteristic polynomial with .$N$ solutions Work-through on page 6 $$A\u0026rsquo; = \\begin{bmatrix} a_{11} - \\lambda \u0026amp; a_{12} \u0026amp; \u0026hellip; \u0026amp; a_{1n}\\\\ a_{21} \u0026amp; a_{22} - \\lambda \u0026amp; \u0026hellip; \u0026amp; \\vdots \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots\\\\ a_{n1} \u0026amp; \\dots \u0026amp; \u0026hellip; \u0026amp; a_{nn} - \\lambda \\end{bmatrix}$$ For the .$2 \\times 2$ case determinant is $$\\lambda^2 - (a+d)\\lambda + (ad - bc) = 0$$ Solving this quadratic equation, we can find the .$\\lambda$ values. Then, for each .$\\lambda_i$ we find, we revisit .$\\eqref{a}$ and solve for the corresponding .$\\vec x_i$ The polynomial on the left hand side of the above equation is known as the characteristic polynomial for the matrix .$A$. If a matrix has repeated eigenvalues, they may (or may not) have distinct eigenvectors. In general, multiplicity of eigenvalues will result in the same multidimensional eigenspace (Aside) \u0026hellip;except if the matrix is defective An .$n \\times n$ matrix is defective iff it does not have .$n$ linearly independent eigenvectors That is, a defective matrix always has fewer than .$n$ distinct eigenvalues, since distinct eigenvalues always have linearly independent eigenvectors In which case the rank is decreased Eigenspace # If the eigenvectors are distinct, they form an eigenspace $$E_\\lambda = \\text{null}(A - \\lambda I_n) = \\text{null}(A\u0026rsquo;)$$\nThat is, null space of some matrix is the set of all vectors that satisfy .$\\eqref{a}$ OR all of the eigenvectors that correspond to some eigenvalue OR the eigenspace that corresponds to the eigenvalue Exactly like the concept of span; the eigenspace is a subspace, the span of all eigenvectors for that eigenvalue (including .$\\vec 0$) Any input vectors that lie in this space (for 2 distinct eigenvectors, a plane; for 1, a line) will be scaled by the shared eigenvalue under a linear transformation. Eigenspace is a subspace, which means it\u0026rsquo;s closed under scalar multiplication Thus, if two vectors are related by a scalar, they must both lie in the same eigenspace of .$\\lambda_i$: .$E_{\\lambda_i}$ That is, the eigenvectors that make up some eigenspace aren\u0026rsquo;t unique But, every eigenvector can only correspond to one eigenvalue If for some .$A$, where .$A \\vec x_1 = \\lambda_1 \\vec x_1$ and .$A \\vec x_1 = \\lambda_2 \\vec x_1$ then .$\\lambda_1 = \\lambda_2$ Thinking about a physical diagram may help clarify; .$\\lambda_1 \\neq \\lambda_2$ would require some single initial state or vector aligned with .$\\vec x_1$ to be scaled by two different values upon being transformed by .$A$ This cannot happen, so the two eigenvalues cannot be distinct Aside: Complex eigenvalues can exist as well; they are much harder to visually understand, but mathematically, we find them using the exact same process as before. Mean-product formula # Mean-product formula is a nicer way of solving this, versus finding the roots of the polynomial $$m \\pm \\sqrt{m^2 - p}$$ $m$ is the mean of the trace, which is the same as the mean of the eigenvalues $p$ is the product of the eigenvalues, which is the determinant Theorems # Theorem 9.1 # Given two eigenvectors .$\\vec v_1$ and .$\\vec v_2$ corresponding to two different eigenvalues .$\\lambda_1$ and .$\\lambda_2$ of a matrix .$A$, it is always the case that .$\\vec v_1$ and .$\\vec v_2$ are linearly independent.\nProof on page 9 Theorem 9.2 # Let .$\\vec v_1,\\vec v_2, \\dots , \\vec v_m$ be eigenvectors of an .$n \\times n$ matrix with distinct eigenvalues. It is the case that all the .$\\vec v_i$ are linearly independent from one another.\nProof on page 10 Proposition 1 # If an square .$n \\times n$ matrix .$A$ isn\u0026rsquo;t invertible, then it has some eigenvalue .$\\lambda_i = 0$\nIf a matrix is not invertible, then the dimension of its null space isn\u0026rsquo;t necessarily greater than 0 because there must be a linearly dependent row or column. If this is true, then there\u0026rsquo;s a non-zero vector .$\\vec x$ such that .$A \\vec x = 0 \\vec x$ If the matrix is not invertible, it has a nontrivial null-space. Then, by definition, there is some nonzero .$\\vec x$ for which .$A \\vec x = 0 \\vec x = \\vec 0_n$. We pattern match to .$\\eqref{a}$ and notice the equation is exactly the same if we multiply the right by .$\\vec x: A \\vec x = 0 \\vec x$. This is kind of like pulling a scalar .$0$ out of .$\\vec 0$, leaving .$0 \\vec x$. Now, we clearly see .$\\lambda = 0$ Proposition 2 # For an invertible .$A$ with some eigenvalue .$\\lambda$, .$A^{−1}$ has eigenvalue .$ \\frac{1}{\\lambda}$\nWe can start at .$\\eqref{a}$: left-multiply both sides by .$A^{−1}$ to get .$\\vec v = A^{-1} \\lambda \\vec v \\Longrightarrow A^{-1} \\vec v = \\frac{1}{\\lambda} \\vec v$. Pattern match to .$\\eqref{a}$ again and we’ve shown the statement is true Note: Given invertibility, all .$\\lambda_i \\neq 0$ so this is always true States # Steady States # We know that a steady state .$\\vec x^*$ of a transformation matrix .$A$ is defined to be such $$A \\vec x^* = \\vec x^* $$ In other words, it is an element of the eigenspace of .$A$ corresponding to the eigenvalue .$\\lambda = 1$. The above equation tells us that if we start at a steady state, then we will remain unaffected by the transformation matrix over time. Therefore, to solve for the steady-state of a system represented by .$P$, we solve .$\\eqref{a}$, substituting .$\\lambda = 1$ Note that this amounts to solving for the null-space of .$(P − I_n)$. Great walk-through on page 53 Predicting Behavior for General Initial States # Given a system and an initial state, can we predict how it’ll dynamically change over time? We saw in the Page Rank example that we seem to approach a sort of steady-state after many timesteps, but under what conditions does this happen?\nSimpler Case: .$\\vec x (0) = \\alpha \\vec v$ # Suppose our initial state is actually a perfect multiple of an eigenvector of the system. Over time, upon repeated applications of .$A,$ we accumulate factors of .$\\lambda$; ultimately, .$A^n \\vec x = \\alpha (\\lambda^n \\vec x)$ \u0026ndash; as we can see derived to the right $$\\vec x (0) = \\alpha \\vec v$$ $$\\vec x (1) = A\\vec x (0) = \\alpha \\lambda \\vec v$$ $$\\vec x (2) = A\\vec x (1) = \\alpha \\lambda^2 \\vec v$$ $$\\vdots$$ $$\\vec x (n) = A\\vec x (n-1) = \\alpha \\lambda^n \\vec v$$ Based on this pattern, we notice the following behaviors based on the value .$\\lambda$ as .$n \\to \\infty$: .$\\lambda \u0026gt; 1: \\vec x (n) \\to \\infty$ \u0026ndash; Diverge, exponential growth. .$\\lambda = 1: \\vec x (n) \\to k\\vec v$ \u0026ndash; Converge, steady-state. .$0 \u0026lt; \\lambda \u0026lt; 1: \\vec x (n) \\to \\vec 0$ \u0026ndash; Converge, exponential decay. .$\\lambda = 0: \\vec x (n) = 0 \\vec v = \\vec 0$ \u0026ndash; Converge(?), instantaneous disappearance. .$\\lambda \u0026lt; 0$: Take .$|\\lambda|$ and refer to the appropriate case above. But recognize the sign switches at each timestep General Case: .$x(0) = \\alpha_1 \\vec v_1 + \\alpha_2 \\vec v_2 + . . . + \\alpha_n \\vec v_n$ # This form says that the initial state is now not a scalar multiple of just one eigenvectors, it’s a linear combination of all of them Note that this is still not fully general; we assume here that all initial states are in the span of the eigenvectors of .$A$, which isn’t guaranteed. But this case devolves into the previous one; we can simply treat each element individually, apply the techniques from the Simpler Case, and put them back together. The final form is as follows: $$\\vec x (n) = \\alpha_1 (\\lambda_1^n \\vec v_1) + \\alpha_2 (\\lambda_2^n \\vec v_2) + . . . + \\alpha_n (\\lambda_n^n \\vec v_n) \\label{b}\\tag{2}$$ Given a matrix .$A$ and some initial state .$\\vec x$, how can we actually get to this equation format? First, we solve for the .$(\\vec v_i, \\lambda_i)$ pairs of .$A$ Then, we use Gaussian elimination to find the .$\\alpha_i$’s; Putting eq. .$ \\eqref{b}$ in matrix form yields:\n\u0026hellip;and we compute the inverse of the matrix of eigenvectors, arriving at:\n$$\\vec x(0) = \\begin{bmatrix} | \u0026amp; | \u0026amp; \u0026amp; | \\\\ \\vec v_1 \u0026amp; \\vec v_2 \u0026amp; \\dots \u0026amp; \\vec v_n\\\\ | \u0026amp; | \u0026amp; \u0026amp; | \\\\ \\end{bmatrix}\\begin{bmatrix} \\alpha_1 \\\\ \\alpha_2 \\\\ \\vdots \\\\ \\alpha_n \\\\ \\end{bmatrix}$$\n$$\\begin{bmatrix} \\alpha_1 \\\\ \\alpha_2 \\\\ \\vdots \\\\ \\alpha_n \\\\ \\end{bmatrix} = \\begin{bmatrix} | \u0026amp; | \u0026amp; \u0026amp; | \\\\ \\vec v_1 \u0026amp; \\vec v_2 \u0026amp; \\dots \u0026amp; \\vec v_n\\\\ | \u0026amp; | \u0026amp; \u0026amp; | \\\\ \\end{bmatrix}^{-1} \\vec x (0)$$\nLet’s assume that we drop any terms corresponding to .$\\alpha_i = 0$ since those terms are, well, zero. Then, with what remains, we can make some intuitive observations: .$|\\lambda_i | \u0026gt; 1: \\vec x (n) \\to \\infty$ \u0026ndash; Diverge, even if other components in the linear combination decay, the state itself \u0026ldquo;blows up\u0026rdquo; to .$\\infty$ as this component overshadows all others. .$|\\lambda_i| = −1: \\vec x (n) \\to\\ ?$ \u0026ndash; Diverge because that component oscillates forever. .$−1 \u0026lt; \\lambda_i \\leq 1: \\vec x (n) \\to \\vec x^*$ \u0026ndash; Converge, that is, steady-state! Each .$i$th term either decays to zero (.$|\\lambda_i| \\leq 1$) or stays the same (.$|\\lambda_i| = 1$), such that .$\\vec x^* = \\sum_{i,\\lambda_i=1} \\alpha_i \\vec v_i$. We can normalize this steady-state if we want proportions (column values sum to 1) rather than absolute numbers Some Useful Information # If a matrix has .$n$ distinct real eigenvalues, their .$n$ associated eigenvectors are all linearly independent. An eigenspace for a given eigenvalue is the span of all eigenvectors, including .$\\vec 0$, and is also a subspace by definition. Say we calculate an eigenvector for an eigenvalue; we can pick any scalar multiple of the result and this will still be an eigenvector, since scaling a vector does not change its direction. This follows from the scalar multiplication property of subspaces. A given eigenvector can only be associated with one eigenvalue, since a vector can only be scaled by some single value upon being transformed by a matrix. But, a eigenvalue can be associated with multiple eigenvectors, the span of which form an eigenspace. If a matrix has some .$\\lambda = 0$, then for some .$\\vec x, A\\vec x = \\lambda\\vec x =\\vec 0$, so .$A$ has a nontrivial null-space. Therefore, it is not invertible. If a matrix has some .$\\lambda = 1$, then any initial state that is aligned with the corresponding eigenvector is a steady-state. More generally, any initial state for which .$\\lambda = 1$ comprises part of the linear combination potentially has a nonzero steady-state, so long as all other .$|\\lambda_i| \u0026lt; 1$. The rotation matrix (that rotates any vector by .$\\theta$ degrees counterclockwise) is: $$A(\\theta) = A_R = \\begin{bmatrix} \\cos\\theta \u0026amp; -\\sin\\theta \\\\ \\sin\\theta \u0026amp; \\cos\\theta \\\\ \\end{bmatrix}$$ "},{"id":14,"href":"/cogsci-c100/sleep/","title":"5: Sleep \u0026 Dreams","section":"CogSci C100","content":" Sleep and cognition # The National Sleep Foundation’s Sleep IQ Test During sleep, your brain rests. Boredom makes you feel sleepy, even if you have had enough sleep. Resting in bed with your eyes closed cannot satisfy your body’s need for sleep. Snoring is not harmful as long as it doesn’t disturb others. Everyone dreams nightly. Raising the volume of your radio will help you stay awake while driving. Sleep disorders are mainly due to worry or psychological problems. The human body never adjusts to night shift work. Most sleep disorders go away even without treatment. Answers F F T F T F F T F Why do we sleep? Fact that all vertebrates sleep, including some that would seem to be better off without it, suggests that sleep is essential Indus dolphins have evolved to take mini-naps of 4-60 seconds (totalling seven hours), bottle-nose dolphins (and some migratory birds too) have evolved to sleep half their brain at the time Sleep requirements Most people need at least 7-8 hours of sleep a night Some notable exceptions: Leonardo da Vinci slept a mere 90 minutes a day, in catnaps of 15 minutes every four hours Salvador Dali liked to doze off, sitting up with a spoon in his hand. As he fell asleep, spoon would fall and clatter to the ground, and he would wake rejuvenated. Thomas Edison and Winston Churchill also thrived on catnaps There are people who are able to function quite well on one or two hours\u0026rsquo; sleep a night Effects of sleep deprivation on cognition # Sleep deprivation causes irritability, fatigue, impaired concentration and creativity, greater vulnerability to accidents Surprisingly, people are oftentimes unaware that their concentration, judgment, etc. are impaired (\u0026lsquo;LSD Effect\u0026rsquo;) Sleep deprivation Impairs functioning of the prefrontal cortex, which has a negative impact on attention, memory, and decision making (Wu, Gillin, Buchsbuam et al., 2006) Reduces neuroplasticity and the proliferation of cells in hippocampus, which can result in memory impairments (Fernandes, Rocha, Rocha et al., 2015) Study by Matthew Walker \u0026amp; Robert Stickgold (2006) found that without sleep, the brain is 40% less able to make new memories Impairs process of making broad connections and gaining creative insight Participants were presented with a task where discovery of a hidden rule greatly improved speed of performance (Wagner, Gais, Haider et al., 2004; Ellenbogen, Hu, Payne et al., 2007) Sleep dramatically increased the likelihood of grasping the hidden rule Sleep causes our brain to create new links, which is why we often wake up with solutions to previously unresolvable problems Sleep also plays an important role in emotional regulation (Walker \u0026amp; van der Helm, 2009) After a night of no sleep, brain scans show a shutdown of the medial prefrontal cortex, which normally helps keep our anxiety in check (Simon, Rossi, Harvey et al., 2019) Research following youth through time found that sleep loss predicts depression rather than vice versa (Gregory, Rijksdijk, Lau et al., 2009) Sleep deprivation can produce hallucinations, delusions, and paranoia Peter Tripp’s wake-a-thon Sleep debt can also cause metabolic and hormonal changes that mimic aging and lead to diabetes, obesity, hypertension, and memory impairment Typical 18-year-old looks like 60-year-old in ability to metabolize glucose after 2 weeks of restricted sleep Sleep deprivation depresses the immune system and is associated with shorter lifespan “Poor sleep will make you fat and sad, and then kill you”\nIn adolescents # Teens now average nearly 2 hours less sleep a night than 80 years ago Insufficient sleep in adolescence increases risk of high blood pressure, heart disease, diabetes, risk-taking behavior, depression, and car accidents Teens who sleep less than 8 hours are 3x more likely to make suicide attempts For each hour of sleep lost, odds of an adolescent’s being obese rises by 80% Compared to A and B students, those who receive C’s, D’s, and F’s in school obtained about 25 min less sleep per night, went to bed an average of 40 minutes later, had more irregular sleep/wake schedules (greater weekend delays of sleep schedule) Sacrificing sleep time to study actually worsens academic performance, by making it harder the next day to understand class material or do well on a test Also, a regular full night’s sleep can “dramatically improve athletic ability” Strengthens neural connections that consolidate “muscle memories” Treatment for insomnia # Treatment of underlying physical/psychological problem Behavioral treatment (CBT-I) Sleep restriction: go to bed and get up at the same time everyday Stimulus control: associate your bed with sleep Relaxation response training Many studies have indicated that CBT is just as effective as sleep meds and has more lasting overall benefit Other helpful practices: # Exercise daily (preferably in late afternoon), but not right before bed No caffeine after 5PM Relax and dim lights hour or two before bedtime Using screens in a dark room was associated with worse sleep outcomes than using them with the lights on Children who used screens at bedtime consistently scored lower on quality-of-life tests Eat foods with more fiber and less simple carbs and saturated fat More fiber and less saturated fat led to more slow-wave (deep) sleep More sugar and simple carbs was associated with more frequently awakenings Consume milk, banana, or sunflower seeds right before bed to raise serotonin levels Identify problematic thoughts and determine how to deal with them In medical students # According to the Center for Disease Control, being awake more than 24 hours impairs performance as much as having a blood alcohol level of .10% - which is legally drunk Many interns work 80 hours per week, typically in shifts of 30 hours Medical students trained on traditional schedule, rather than alternative schedule with fewer weekly hours, made 5.6 times more diagnostic errors (Landrigan, Rothschild, Cronin et al., 2004) Sleep Debt \u0026amp; Napping # Sleep debt: “The brain keeps an accurate count of sleep debt for at least two weeks” Potential benefits of napping Federal Aviation Administration study found that pilots who took a 20-minute nap early in their 9-to-12-hour flight were significantly more alert as measured by reaction-time task Research indicates that a 45-minute nap improves alertness for 6 hours 1-hour nap improves alertness for 10 hours Napping can be an effective way to cope with sleep crisis However, naps can potentially disrupt nighttime sleep, so it is generally recommended that those with chronic insomnia limit naps to 20-30 min at most or to refrain from napping altogether Neurophysiological functions of sleep # Slow wave sleep lowers the brain\u0026rsquo;s metabolism and permits it to rest Regions that show the highest levels of activity during the day show the lowest levels of activity during slow-wave sleep Stimulation of person\u0026rsquo;s hand with vibrator caused activation of contralateral somatosensory cortex: more delta activity was shown in that region the next night On the other hand, regions involved in consolidation of memories show an increase in activity during sleep The same patterns of activation of hippocampal neurons that occur during learning are repeated when animal is sleeping The greater the hippocampal activity during sleep after a training experience, the better the next day’s memory Also, the glymphatic system pushes cerebrospinal fluid through the brain to flush out toxins during sleep Sleep clears brain of damaging molecules associated with neurodegeneration REM sleep and dreams # Stages of sleep # Waking state Alpha activity: regular, medium-frequency waves of 8–12 Hz; associated with a state of relaxation Beta activity: irregular, low-amplitude waves of 13–30 Hz; associated with a state of arousal or alertness Stage 1 Sleep Theta activity: transition between sleep and wakefulness (nodding off, hypnagogic stage) Stage 2 Sleep Characterized by sleep spindles: bursts of waves, thought to be important in cognitive processing and\u0026hellip; K-complexes: sudden sharp waveforms when you hear a loud noise\u0026ndash;you brains senses remain active Stages 3 and 4 Sleep (slow-wave sleep) Delta activity: regular, low-frequency, high-amplitude waves of less than 4 Hz; occurs during deepest stages of sleep Person cannot be easily awakened, and when awakened, s/he acts groggy and confused (you don\u0026rsquo;t think you\u0026rsquo;re sleeping in stage 3) Sleep cycles # Each cycle of REM and non-REM sleep lasts approximately 90 minutes Most slow wave sleep occurs during first half of night Later cycles contain more stage 2 and REM sleep 90-minute cycles of rest and activity ( basic rest-activity cycle) have also been found for activities such as infant feeding, eating, drinking, smoking, heart rate, oxygen consumption, etc. It is actually impossible to totally deprive someone of sleep for prolonged period EEG shows that people start to have “microsleeps” of two or three seconds Dreams and REM sleep Dreams occur during REM sleep: even those who claim they don’t dream, if awakened during REM, will usually recall a dream when awakened from REM Dreams also occur in non-REM (NREM) sleep However, REM sleep is associated more with “true dreams” that have vivid sensory and motor imagery and sense that “you are there”; NREM dreams tends to focus more on a thought (e.g., solving a problem), image, or emotion Amount of time spent in REM during each cycle increases as night progresses Dreams during second half of sleep become more bizarre and emotionally intense; lucid dreams tend to occur in early morning hours Dreams are quickly lost from memory unless we catch them and think about them immediately upon awakening To improve dream recall, keep a dream diary Functions of dreams # Why Do We Dream? Theories through the times\u0026hellip; Freud’s wish-fulfillment theory of dreams: Dreams are a safety valve that discharges otherwise unacceptable feelings/desires Hobson \u0026amp; McCarley’s activation-synthesis hypothesis Dreams serve no purpose \u0026ndash; just side effect of random firing of neurons that serve to develop and preserve neural pathways through stimulation Not scientific, can\u0026rsquo;t disprove \u0026ndash; same with Freudian views Pons sends randomly generated electrical signals to the frontal cortex (activation). The frontal cortex then weaves these signals into a coherent story (synthesis) Modern psychodynamic view Dreams reflect what is going on in our lives, our hidden impulses and desires, and underlying conflicts Dreams give clues to the solution of our underlying conflicts and problems Cognitive development theory of dreams The neural activation associated with dreaming aids cognitive development Peak of REM sleep occurs in 30-week-old fetus, who spends almost 24 hours a day in this state Memory consolidation view Participants who were trained on visual search task or word learning, then deprived of REM sleep, performed significantly worse than control participants the next morning REM sleep increases following stressful experiences or periods of intense learning (e.g., graduate student studying for Qualifying Exams) Hypnagogic dreams and learning and memory These dreams are characterized by: High rate of incorporation of memories of events from the day or from older related memories When researchers had participants play Tetris for 7 hours, then repeatedly awakened them during their first hour of sleep, three-fourths reported experiencing images of the game’s falling blocks A preference for emotionally salient material but without high dream affect Hypnagogic dreams normally lack the bizarreness, selfrepresentation, emotions, and narrative complexities common to REM dreams Slow-wave sleep early in the night may aid in consolidation of declarative memory tasks while REM sleep may enhance the processing of emotional memories REM sleep involves activation of the amygdala and limbic forebrain structures \u0026ndash; structures that are associated with regulation of emotions Participants deprived of REM sleep show Greater amygdala and less frontal activation after viewing emotionally distressing film Less reduction in negative affect upon second viewing (Rosales-LaGarde, Armony, del Rio-Portilla et al., 2012) You can\u0026rsquo;t process new emotions when sleep deprived “Sleep knits up the raveled sleeve of care.” - Shakespeare\nCase Studies: Unconscious problem solving Great discoveries and inventions have often come to people through dreams The tune for \u0026ldquo;Yesterday\u0026rdquo; came to Paul McCartney in a dream Man who discovered the structure of the benzene molecule had dream of snake chasing its own tail In 1845, Elias Howe had an idea for inventing a sewing machine, but he couldn’t figure out how to connect the needle to the thread. Then one night he dreamt that he was taken prisoner by a group of natives who were dancing around him with spears. The spears all had holes near their tips… Madame Walker, the first female American self-made millionaire, founded and built a highly successful African-American cosmetic company in the 1890’s that made her a millionaire many times over: the formula for her original product, which included ingredients that could only be obtained from Africa, came to her in a dream Content of dreams # Repetition dimension continuum: traumatic dreams =\u0026gt; recurrent dreams =\u0026gt; repetitive themes within long dream series =\u0026gt; frequent elements in dreams All of these may be viewed as attempts by the dreamer to work through emotional preoccupations, “fixations,” “hang-ups” Evidence that dreams can be an effective way of working through emotional preoccupations, fixations, or “hang-ups”: Patients with depression are more likely to be in remission a year later if: They show high eye movement density and high affect strength during their first REM period They report more negative dreams at the beginning of the night and fewer negative dreams at the end of the night This research suggests that negative dreams occurring at the end of the night are indicative of person’s failure to work through problems and self-regulate mood These dreams can potentially be addressed in therapy, particularly as the last dreams of the night are those that a person is most likely to recall Traumatic dreams (PTSD) Repetition of the traumatic event in all its emotional detail and horror Dreams change over time as person recovers, incorporating other elements and becoming less like the exact experience Decline in traumatic dreams if dreams are discussed in groups (e.g., with other vets) Those who have recovered often suffer a relapse to the old dream content when faced with new stressors Traumatic dreams are about emotional events that people cannot resolve or “assimilate” Recurrent Dreams and Nightmares # Recurrent dreams About 50% to 65% of college students report that they have experienced a recurrent dream Recurrent dreams are mostly unpleasant and may take the form of nightmares There are usually only a few themes that make up most nightmares for most people Most frequent content theme of recurrent dreams is being attacked or chased Recurrent dreamers tend to score significantly lower on measures of wellbeing than either former recurrent dreamers or non-recurrent dreamers Dream work as a way to resolve problem of recurrent dreams Recurrent dreams may be watered-down versions of traumatic dreams Those who are prone to nightmares… Often do not recall any obvious traumas Instead, they tend to be relatively normal people who work mainly as artists, teachers, and therapists \u0026ndash; tend to be creative and service oriented Usually extremely sensitive from childhood: open, vulnerable, with “thin boundaries” Research has also indicated that people with thin boundaries have dreams that are more vivid, detailed, and emotional than those with thick boundaries Theory that nightmare sufferers are highly sensitive people for whom many everyday experiences are in effect highly traumatic Repetitive Dream Themes # We tend to think of dreams as irregular and infinitely varied, but this is not in fact true \u0026ndash; content analysis has revealed that dreams are much more repetitive than most people think… Dorothea: Kept dream journal from 1912 (when she was 25 years old) to 1963 Analysis revealed that same basic themes appeared throughout 50 years: Eating or thinking of food appeared in 1 out of every 5 dreams Loss of an object, usually her purse, occurred in 1 out of every 6 dreams She was in a small or disorderly room, or her room was being invaded by others, in 10% of her dreams Mother appeared in another 10% of her dreams She was going to the toilet in 1 out of every 12 dreams She was late, concerned about being late, or missing a bus or train in 1 out of every 16 dreams Six themes appeared in about 70% of her dreams Jeffrey Moved from one coast to the other, left his wife, “came out” as gay, and retired from his teaching position However, as in case of Dorothea, dream themes/elements tended to remain constant over 25 year period Research has indicated that themes and emotions of dreams do not tend to change much over the course of a person’s life However, if person experiences period of profound change, dreams can change dramatically Dream analysis # Content analysis: systematic analysis of dream elements and themes Analysis of dream elements in tens of thousands of dreams in countries across the world has allowed for the establishment of certain “norms” Domhoff and his colleagues at the UC Santa Cruz have meticulously cataloged and posted more than 20,000 dreams (dreambank.net) Cross-cultural differences Americans tend to dream more about animals and food while Brazilians have more sexual and emotional dreams Far lower levels of physical aggression in dreams of Dutch men and women than among American men and women Mirrored by the fact that the US is one of the most violent industrialized nation in the world and the Netherlands one of the least violent, according to crime statistics Another study found the following rates of aggression in dreamers from different regions of the US: East Coast: 40% Midwest: 10% West Coast: 22% Most common dreams among college students \u0026ndash; percentage of students who have had each of following types of dreams: Theme % Falling 83% Being attacked or pursued 77% Trying repeatedly to do something 71% Schools, teachers, studying 71% Sexual experiences 66% Arriving too late 64% Eating 62% Gender differences Men tend to dream about sex and violence Women tend to dream more about weddings Continuity hypothesis of dreams: “the dream life reflects the waking life” People generally dream about the same people, places, objects, activities, wishes, and fears that dominate their waking-life Psychoanalysis # Content analysis and psychodynamic view of dreams Frequency with which a particular dream character or dream activity appears reflects intensity of dreamer’s preoccupation with it \u0026ndash; dreams are a way of dealing with “emotional preoccupations,” “fixations,” or “unfinished business” That is why dreams are more often unpleasant than pleasant 8 in 10 dreams are marked by negative emotions Barb Sanders Content analysis of series of over 3000 dreams recorded between 1960’s (when she was a teenager) to late 1990’s Sanders married, had three children, got divorced, was involved in a series of relationships with different men, earned a Masters’ in a helping profession, worked in a community college setting, and became an actress and director in a local theater company \u0026ndash; but her dream themes for the most part did not change Characters and social interactions (these are usually the most psychologically revealing aspect of a detailed content analysis): Mother Appears in 7.7% of dreams \u0026ndash; more than any other character Percentage of aggressive to friendly encounters with her is 72% \u0026ndash; way above the national norm and her own baseline Sanders described her mother as “an angry, isolating person… sharp and critical and negative and physically distant” Favorite brother Percentage of aggressive to friendly interactions with him is 25% – almost the mirror opposite of her relationship with her mother When you tend to have a good dream about someone, then you tend to have positive interactions with them; and vice-versa Use of Content Analysis in Clinical Diagnosis Analysis of 1,368 dreams from a four-year period reported by a man in his mid-thirties who was undergoing psychotherapy \u0026ndash; analyst (Calvin Hall) knew nothing about subject beyond his age and fact that he was seeing a therapist Repeated themes: Wide range of sexual practices and objects Friendly and sexual interactions with children, with a greater focus on girls than boys Urinating and defecating Women with penises or beards, or who disguised themselves as men Various kinds of holes, openings, and tunnels, which the dreamer usually entered and explored in some way with great curiosity Based on this analysis, Hall concluded that man had poor impulse control, confusion about gender, and a particular concern with the nature of female genitals (symbolic interpretation of holes and tunnels) \u0026ndash; and that he was a child molester whose primary desire was to look at the genitals of little girls All of these inferences proved correct according to the therapist and the dreamer Also, total lack of any reference to father in dreams, in contrast to a disproportionate number of dreams about his mother and sister Led Hall to suspect either absence of a father or a traumatic experience with father To check for the latter, Hall searched for possible symbolic substitutes for father \u0026ndash; correctly concluded from following dream that the dreamer had himself been molested as a child by the father: “A bull that seemed to have human intelligence came behind me and held me against him. I did not like his advances and I sensed that he wanted to have sexual relations with me. So I broke away from him.”\nDream work # Basic technique: Recall dream in as much detail as possible as soon as you awaken To enhance dream recall, try not to move: stay in same position Try to fully recall emotions of the dream \u0026ndash; not the day ahead Keep a scribble pad by your bed and take notes Interpret dream In general, dream dictionaries are of limited value: we all have our own unique symbols Focus on emotional interpretation \u0026ndash; not academic interpretation (What did you feel in dream? Where do you feel like that in your life?) \u0026ldquo;Dreams are all about emotions (activation of the limbic system) In our dreams, we may access immensely positive emotions that we may never even have experienced before in waking life Focusing on those emotions can potentially cause situations to manifest that naturally elicit those emotions Extra: Jungian Dream Analysis # Jung maintained that the figures in our dreams represent aspects of ourselves that we have disowned Through projection, we see in others what we fail to see in ourselves These projections may be positive or negative Dream work allows us to own and integrate all of these various aspects Jungian dream analysis involves analysis of archetypes (basic personality features) and symbols Archetypes: Self: unites all other archetypes Persona: public image (mask) Shadow: aspects of ourselves (positive as well as negative) that we wish to deny Anima: our feminine aspect Animus: our masculine aspect Symbols: Animals: your own traits, good and bad Dogs: symbol of the masculine Cats: symbol of the feminine Vehicles: the direction you are heading in life and your body Chase: time for you to set out on your destined path, but you are refusing to let go of elements in your life that are hindering your quest Children: something new, different and joyous. May also represent innocent parts of yourself sometimes, and at other times, immaturity and childishness. Death: pertains to change. May also symbolize confronting fear, usually fear of death or change. Falling: fear of losing respect or status; or of financial difficulties, fading physical vitality, or losing someone\u0026rsquo;s love. House: represents you Rooms: different aspects of yourself Doors: opportunities Lost: you are lost in your life, adrift. Something is gone from your life - love, career, spirituality. Naked: inadequacy: you don\u0026rsquo;t feel prepared for some event, or for life itself. This dream may have an element of comedy - lighten up! Water Calm water: good times ahead, clear sailing Rough waters: caution, reconsider your actions To drown can be a warning Lucid dreaming # Lucid dreams: dreams in which one is aware that one is dreaming and is thus able to direct the course of the dream Lucid dream practice originally derived from meditative traditions The ultimate aim of dream yoga is not just to have fun controlling reality; rather, lucid dreams are brief, spontaneous realizations of the state of mind sought in meditation practice These dreams involve a letting go of the ordinary sense of self and a bonding with the dream experiences themselves There is a “special” feeling of immediacy and vividness: everything becomes much more “real, clear, and somehow present” There is a sense of clarity and exhilaration that is based on a transformation of the way we normally attend to things and in our self-concept Practice dream recall Research has shown that people who recall dreams at least once a night report having at least one lucid dream a month Learn to recognize your most frequent or characteristic dreamsigns \u0026ndash; elements of dreams that indicate that you are dreaming (e.g., miraculous flight, purple cats, malfunctioning devices, and meeting deceased people) Get ample sleep The relative likelihood of lucid dreaming continuously increases with each successive REM period If you sleep 8 hours, the probability of your having a lucid dream during the last 2 hours of sleep is more than twice as great as the probability of your having a lucid dream in the previous 6 hours Napping Trick you can use if you can’t afford to spend 8 hours in bed: get up one hour earlier than usual, stay awake for 30 to 60 minutes, then go back to sleep During the wakeful period, read about lucid dreaming, practice reality checks and then do MILD as you are falling asleep Study found 15 to 20 times increased likelihood of lucid dreaming for those practicing the nap technique Test different sleeping positions Practice reality testing throughout the day Ask yourself “Am I dreaming?” and test your state… Tips on reality testing: The pinch test doesn’t really work Try flying Find some writing or a digital watch and read it once, look away, then reread it, checking to see if it stays the same In dreams, text changes 75% of the time it is re-read once; 95% of the time that it is re-read twice Try to turn on a light \u0026ndash; this usually cannot be done in a dream In general, things are much more changeable in dreams than in waking life: oftentimes all you have to do is look around for unusual transmutations Lastly, anytime you find yourself seriously suspecting that you just might be dreaming you probably are Induction techniques # Use autosuggestion/dream inoculation Imagine as vividly as possible that your surroundings are a dream During the day, think continuously that “all things are of the substance of dreams” Strengthen desire/intention Firmly resolve to recognize dreaming Tell yourself, “Tonight I will have a lucid dream,” “Tonight I will fly” – particularly in the early morning hours or during an awakening in the latter part of your sleep period Visualize yourself recognizing dreaming Imagine yourself carrying out an intended dream action Paul Tholey claims that most participants who consistently practice the reality testing and intention techniques will experience at least one lucid dream every night MILD # Mnemonic (mind) Induction of Lucid Dreams technique Preliminary training: prospective memory exercise Look for certain pre-specified targets each day for at least a week and do a reality test as soon as you notice the target When you awaken from a dream period, recall as many details as possible from your dream See yourself becoming lucid: Imagine that you are back in the dream from which you have just awakened, but this time you recognize that it is a dream Focus your intent: tell yourself “Next time I’m dreaming, I want to remember I’m dreaming” – and really mean it! Repeat procedure till you fall back asleep LaBerge found that with autosuggestion, he had a lucid dream on only 1 out of 6 nights in the lab; with MILD, he had one or more lucid dreams on 20 out of 21 nights in the lab Sleep paralysis # Experience can be terrifying Person feels they cannot move May feel like a great weight is holding them down; hallucinations may appear Neurological explanation: During REM, the voluntary muscles of the body are paralyzed Independent neural systems cause muscular paralysis, blockade of sensory input, and cortical activation – sometimes these don’t turn on or off at the same time, resulting in sleep paralysis Sometimes people panic when they experience sleep paralysis and struggle to move or to fully wake up, but such reactions are actually likely to stimulate the limbic areas of the brain and cause the REM to persist Solution: Remember it is a dream and therefore harmless Relax and adopt an attitude of interest and curiosity about what happens Dreams that proceed from paralysis experiences are often quite intense and wonderful About 20% of people who experience sleep paralysis say they really enjoy it because they\u0026rsquo;re able to enjoy it Preventing Premature Awakening # How to prevent premature awakening Remain calm – relax and engage with the dream rather than withdrawing into your inner joy of accomplishment Look at the ground or at your hands: this may help stabilize dream Concentrate on the senses other than vision, such as hearing and touch (listening to voices or music, touching your body or an external object) Load the perceptual system so it cannot change its focus from the dream world to the waking world Spinning technique Spin like a top (or fall backwards) While spinning, remind yourself that the next thing you see will probably be a dream and do a reality test wherever you seem to arrive The expectation of possible awakening often leads to a \u0026ldquo;false awakening\u0026rdquo; in which you dream of waking Possible reason spinning technique works: it engages the vestibular and kinesthetic senses, discouraging the brain from changing state from dreaming to waking Odds in favor of continuing the lucid dream: After spinning, about 22 to 1 After hand rubbing (another technique designed to prevent awakening), about 13 to 1 After \u0026ldquo;going with the flow\u0026rdquo; (a \u0026ldquo;control\u0026rdquo; task) 1 to 2 To stay in a dream, create sensation of motion If you do awaken, play dead Remain perfectly motionless and deeply relax your body – there is a good chance that REM sleep will reassert itself and you will have an opportunity to enter a lucid dream consciously Awakening at Will # Yell – this directs your attention away from the dream and may actually activate vocal muscles of sleeping body You can activate your vocal cords even when sleeping Fixate your gaze on a stationary point: this will generally cause fixation point to blur, followed by dissolution of the entire dream scene and an awakening within 4 to 12 seconds Q\u0026amp;A # Q: Won’t all these efforts and exercises for becoming lucid lead to loss of sleep? And won’t I feel more tired after being awake in my dreams? A: Yes, lucid dream practices may result in some loss of sleep. However, how tired you feel after a dream depends on what you did in the dream – if you battled endlessly with frustrating situations in a non-lucid dream, you probably will feel very tired afterwards On the other hand, a particularly exciting flight over a glamorous landscape can leave a person emotionally vitalized for several days: “I customarily wake [from a lucid dream] with a cheerful ‘afterglow’ that carries me through the day” In general, people who find lucid dreams exhausting or unpleasant are not able to exercise much control over their dreams Q: Might lucid dreaming be dangerous for some people? A: In general, for people who are not “neurotic beyond the bounds of normality,” it is completely harmless. Q: How long does it take to learn lucid dreaming? A: This varies a lot with the individual. It may take a few days to a few months to induce a first lucid dream with these practices. In general though, it will take years to get to the point where one is able to have lucid dreams at will. Transforming nightmares # You can resolve re-occurring nightmares by lucid dreaming and choosing to face the fear Lucid dream therapy (LDT) Client is trained in progressive muscle relaxation, then rehearses recurrent dream in as much detail as possible Client selects a part of the recurrent dream that is emotionally and/or visually salient and during which he can imagine carrying out a particular task Client imagines performing this task (which can be as simple as looking at one’s hands) in the dream while saying that he is dreaming Later, during an actual dream, this action will cue that the experience is a dream Client is instructed to practice exercise at home just before going to sleep Lucid dream therapy for treatment of nightmares may be particularly effective with children Children tend to have spontaneous lucid dreams much more frequently than adults Children who discover they can control their dreams never have to be told to go to bed! “Bedtime became exciting because of this new world I had discovered where anything was possible and I was the Boss.”\nIntroduction to lucid dreaming can be a wonderful gift to give to a child Other uses of lucid dreams # Dream activities produce the same physiological effects as performance of those activities in the waking state Thus, rehearsal of tasks can be effective in the dream state Also, lucid dreaming may potentially be useful for facilitating the functioning of the immune system and physical body generally Ex: Woman suffering from inorgasmia was able to orgasm in lucid dream – and this permanently cured her disorder! Same circuits activate when doing something while dreaming as doing that same something in real life Inception # What aspects of lucid dreams presented in the movie are real and what aspects are not? Real: It can be very difficult to tell if one is dreaming or awake It is primarily emotions that create dreams What characterizes REM/dream sleep neurologically is principally activation of the limbic system Unreal: Time does not speed up in dreams It is unclear that shared dreams can occur Lucid dreams do not tend to manifest as intense nightmares since by definition one is aware that it is only a dream and can exert at least minimal control over the dream "},{"id":15,"href":"/e-29/5/","title":"5-6: Forming Processes","section":"Engineering 29","content":" Before # So far in the class, we have seen a wide range of additive and subtractive processes which are capable of creating components in metals, alloys, polymeric materials, ceramics, and composites. The one thing that these processes all have in common is that they are serial — meaning that one feature is produced after another in a predefined sequence. This serial nature inherently limits productivity and may require considerable operator skill. High operator skill, which leads to high overhead costs, is especially needed when a tool such as a lathe is manually operated, but may be needed even if the tool is computer numerically controlled, because work still has to be properly mounted and the machine correctly aligned to it. Subtractive processes have been used for centuries, both in the mass-production of precision components, and in one-off custom and prototyping jobs. Additive manufacturing has until recently been seen as a purely prototyping tool, although increasingly is being adopted for short-to-medium production runs.\nCasting, and injection molding, on the other hand, form all features in a component almost simultaneously by forcing molten material into a mold, and potentially offer a faster, more affordable route to mass production. Injection molding is suitable only for mass production because of the high expense of the molds needed, and production runs of \u0026gt;10,000 are usually needed for economic operation. Casting comes in many flavors, from high pressure die-casting — which is really the equivalent of injection molding for processing metals — to gravity-driven sand-casting, which is used both for long production runs and for one-off fabrication jobs because of its ability to create very large components (\u0026gt; 1 m in some cases) with reasonable simplicity. The higher throughput of casting comes at the expense of lower achievable tolerances than machining (considerable fractions of a millimeter, as opposed to tolerances of a few micrometers). Forming processes such as casting are thus often followed by subtractive secondary processing to bring key features within tolerance. In this module we will highlight some of the attributes of casting and injection molding.\nForming of Polymers (Molding Processes) # Injection molding # An injection molding machine consists of a mechanism to melt and extrude the polymer, and a molding unit. Injection molding is the workhorse of plastics manufacturing and can achieve cycle times of under a minute. Most plastics that are injection molded are thermoplastics, meaning that they can be reversibly changed between solid and fluid states by heating and cooling. Injection molding, however, can also be used to inject materials that covalently crosslink when heated (thermosetting plastics), making components that are more heat-resistant than thermoplastics. Injection molding has even been adapted to mold metal powders which are then sintered inside the mold (powder injection molding).\nPolymer Melting # In conventional injection molding, the material to be molded enters the machine as solid pellets, which are drawn into a rotating, heated screw whose diameter decreases along its length to compress the material, drive out air voids, and turn it into a continuous molten flow. This flow is then extruded from a nozzle into a clamped mold, which is typically machined from tool steel but may contain inserts of other materials for specific applications.\nVery thick material (viscosity) isn\u0026rsquo;t an issue due to large pressures used (Pressures typ. \u0026gt; 100 MPa) Pellets go into hopper where they\u0026rsquo;re carried along and melted The inside diameter of the screw gets larger so the pellets get compressed as they\u0026rsquo;re pushed along the screw until all air is driven out and it\u0026rsquo;s a constant stream Wire mesh ensures only \u0026rsquo;liquid\u0026rsquo; passes (removes any debris/dust/funky filament) Machinery # The core is the part that is inserted into the mold (surrounding case) These two components constrain geometry Must have a very tight fit due to very high pressures (hydrolics is commonly used) Process Stages # Heat up and screw the molten plastic to the tip of the core Plunge the core into the mold and inject the molten material Hold for a few seconds to let the material to solidify Retract the core and bring back the mold to release the new piece Cycle times approx. 10 s to 1 minute The mold is held at a temperature that is high enough to allow the polymer to fill the mold before solidifying, but cool enough that cycle time is not excessively long. The molded material needs to fall to below its glass transition temperature, below which the material becomes rigid, before it is ejected from the mold. Ejection is usually automated, with mechanical ejector pins emerging from the mold to push the molded component into a collection bin.\nFeatures # Connectors are placed as to minimize material used and heat; fractals can be used to optimize these patterns\nHot Runners # Overmolding and insert molding # Multiple polymers Rigid and rubbery Can make hinges between rubber and rigid material Enables tooth brush bristles to be held in You need a precise temperature in the range of the material so it deforms enough to bind with another surface while not permanently deforming Different colors Encase metallic objects \u0026ndash; object is molded around for.. Rigidity Strength Abrasion resistance Way to circumvent having to 3D printed threads Physical limitations on geometry # As soon as coming in contact with surface, the polymer looses heat Thus, penetration depth is limited by how quickly the polymer \u0026lsquo;freezes\u0026rsquo; You can increase pressure and temperature (thus viscosity) to circumvent this \u0026ndash; but there are of course tradeoffs (longer heat up time, \u0026lsquo;maximum heat\u0026rsquo; material can withstand) For very tiny surface the surface geometry starts to matter too (e.x. income-angle may matter, as does mold material and how it interacts with the polymer) Defects # Flash \u0026ndash; extra material squeezed out around edges, typically not a big deal Flow lines \u0026ndash; may matter for certain applications Short shot \u0026ndash; not enough material injected Typically because extruder doesn\u0026rsquo;t output enough or because mold is pulled away too soon Misrun \u0026ndash; material doesn\u0026rsquo;t make it to end of material Warping \u0026ndash; uneven section thickness, causing non-constant heat density and thus warping The more rigid geometry, the more differential thermal contraction can happen before warping occurs Why ribs are used commonly used \u0026ndash; small regions of strips of material is, from a structural POV, better than evenly distributing it across the surface Trapped air \u0026ndash; solved by adding very tiny venting channels for air to escape Can fail if clogged with tiny debris, causing flash Silver streak \u0026ndash; polymer flowing over surface and solidifying too early Sink marks \u0026ndash; mass below surface pulls downwards, resulting in tiny dip Caused by ribs (supports) in particular! Visible in polished surfaces \u0026ndash; thus you can avoid it by giving the surface a finish Design of Molds # Cost of molds drives economic batch size ≳ 10,000 parts Design considerations Draft angle: typically 1 − 2° Air vents Advanced features Multi-core molds Active water cooling Materials with very high thermal conductivity e.g. Cu-Be alloys For large moldings where achieving rapid enough cooling of the component is a challenge, high-thermal-conductivity materials such as copper–beryllium alloys may be used for part of the mold.\nMaterial shrinkage needs to be compensated for in the sizing of the mold Examples of typical linear contractions between solidification and room temperature: Material Approx. linear shrinkage (%) ABS 0.6 Nylon 2.0 \u0026ndash; crystaline PC 0.7 \u0026ndash; amorphic PE 2.5 PS 0.4 PVC 0.5 Watercooling system around mold\nSurface texturing methods # Polishing Sanding Grinding Blasting \u0026ndash; for more complex geometry Sand or glass beads EDM Chemical etching Ferric chloride, nitric acid Nanoscale Features # Nanopatterned hard mold coatings such as chromium nitride have recently been introduced to enable injection molding of nanostructures giving butterfly winglike structural color \u0026ndash; structural color\nOther Polymer Forming Processes # Reaction Injection Molding # For if you aren\u0026rsquo;t making many copies (so creating a mold isn\u0026rsquo;t worth while) Materials are reactive when they come in contact \u0026ndash; think of epoxy Enables thermosets, not just thermoplastics, to be molded Compression Molding # A lower-throughput but simpler and more affordable polymer molding technique is compression molding, in which a charge of the material to be formed is placed directly into the mold, and the mold halves are then brought together. The simplicity of the technique lends it to modest-sized production runs, and it is particularly suitable for thermosetting polymers, which crosslink when heated and can be removed from the mold at the molding temperature, because they do not need to be brought below a glass transition to acquire rigidity\nMuch simpler apparatus; tens of MPa (rather than hundreds) Relies on a thermosetting material to solidify part Flash is common Extrusion # Wide range of length-scales possible FDM printing (~0.1 mm) Large pipes and construction sections (\u0026gt; 1 m possible) \u0026ldquo;Keeping mass as far away from center is a good way to optimize usage\u0026rdquo; Before leaving die, materials which are extruded outwards slightly because the high pressures cause the extrusion to shrink down Hollow sections possible with specially designed mandrels E.x aluminum alloys (80-20), PVC are common extrusion materials Blow Molding # Blow molding is very widely used to produce components with thin side walls, such as plastic bottles. An extruded tube of softened thermoplastic polymer is clamped between two halves of a mold and air pressure inside the tube is increased. This expands the material until it conforms to the mold, stretching the material and making it thinner as it does so.\nBetter than injection molding for very thin walls Extrusion followed by inflation Or, can use injection molded preform (e.x. plastic bottle threading) Forming of metals and alloys (Casting processes) # Reasons to use casting # You have an established component design which needs to be mass-produced: casting can create many parts in less time than machining or additive manufacturing. Geometric versatility: with appropriate mold design, sophisticated re-entrant geometries can be produced in about the same time as a simple geometry. Processing time does not increase strongly with geometric complexity, as it does in machining. The need to process high-melting-point (“refractory”) alloys: melting point is correlated with hardness, making machining increasingly difficult as melting temperature increases. A highprecision casting technique such as investment casting can be a good option for achieving demanding tolerances with refractory alloys. A desire to minimize material wastage: in casting, very little material is used beyond that required for the actual component, and any surplus (e.g. for risers and runners, as discussed below) can be re-melted and re-used. In contrast, in subtractive manufacturing much of the stock may be machined away as chips which are laborious and may be somewhat energy-intensive to recycle. Variables in casting Material insertion: gravity- or pressure-driven? Mold material: expendable or permanent? Pattern material: expendable or permanent? Sand Casting # Gravity-fed Heat loss and shrinkage is a concern, so extra risers are included (which also serve as vents) Manual process that creates large objects Few-hundred runs a year, takes a few hours per run If the mold halves do not mate perfectly, molten metal may travel along their interface, leading to flash, which is usually thin enough to be easily broken off or abraded/machined away In sand casting, a single-use mold is produced by compacting a sand mixture around a pattern, which has the same shape as the final required component. The pattern might be made by machining timber or metal, or by additive manufacturing. The mold is prepared in two halves, the cope (top) and drag (bottom), with the sand being contained within a metal flask that has two interlocking parts. The pattern is removed from the compacted sand mixture before the mold is closed up. If re-entrant or hollow components are to be cast, one or more cores are introduced into the mold before the cope and drag are brought together. These cores are also single-use and may be made from sand that is bound with organic material such as gelatin. The mold needs to have several features in addition to the cavity that will contain the actual cast component. First we need a downsprue, or sprue, into which the molten metal can be poured. Second we need runners which are horizontal channels that carry the molten metal from the sprue to the mold cavity. Thirdly we need one or more risers, which allow air to escape from the mold as the metal enters, and are also designed to be the last part of the metal to solidify. Risers can thus provide a small amount of surplus material to compensate for the volume shrinkage that occurs when the metal solidifies. Risers need to be strategically positioned around the mold cavity to supply this material where it is needed. The design of sprues, runners and risers requires skill and experience, and is one of the key competitive advantages of successful foundries. Photos Materials # Sand, or silica, is used as a mold material because of its high melting point (~1600 ˚C) relative to that of the material being cast (e.g. 660 ˚C for aluminum). Sand on its own, however, will not hold the shape of the mold. Green sand is a sand, clay, and water mixture which holds together via the capillary forces between particles and by particle jamming between the larger sand particles and the much smaller clay particles. Green sand has the advantage that it can simply be shaken off the cast component after solidification, and recycled many times.\nIf a mold cavity has particularly large unsupported regions, a stronger mold material may be needed. In this case the clay and water may be replaced by a stronger binder, which may thermally or chemically cure to make a solid mold material. Such molds are more robust but are not reusable.\nAn alternative approach is to use loose sand and compact it under vacuum between plastic sheets to make it solid. The plastic is vaporized as soon as the molten metal hits it.\nOptions for sand-based molds: Green sand Typically SiO2, particles a ≥ 10µm in diameter Mixed with 7-10% clay (e.g. kaolinite, 2SiO2.Al2O3.2H2O). Typically \u0026lt; 2 µm diameter Bound with 2-4% water Clay particles promote mechanical jamming of sand particles and with the water bind sand together Shake off and sieve to re-use Dry or chemically bound sand Mix sand with an organic binder May require heat to cure: 200 − 320 ℃ Or may cure via a chemical reaction Enables larger unsupported mold cavities than green sand But sand harder to re-use: needs grinding down Why use sand? It is refractory: i.e. high melting point – higher than the metal/alloy being poured. e.g. silica: melts at 1600 ℃; pure aluminum: melts at 660 ℃ Gravity casting: pouring # As molten metal is poured by hand from the crucible in which it has been heated, it accelerates under gravity and flows into the mold cavity. The height of the sprue is crucial in determining the speed with which the metal enters the cavity, and hence how long it takes to fill. If filling is insufficiently fast, there is a risk that metal will solidify before filling is complete; if, however, it is too fast, the momentum of the molten metal may dislodge sand from the walls of the mold, leading to the inclusion of sand impurities in the cast component and weakening the casting.\nThe speed can be estimated by considering the amount of gravitational potential energy that is converted to kinetic energy during the pouring process. The other important concept here is continuity, where we assume that the molten metal is incompressible (constant volume). So molten metal will flow at a higher velocity along narrower parts of a runner, for example.\nBernouilli’s Law (conservation of energy): # We can usually neglect head losses and pressure changes: Continuity (conservation of mass): # Continuity of mass and energy together explain why the downsprue is sometimes tapered: to prevent a pressure drop and aspiration of air (or sand) from the sidewalls into the molten metal. Mold filling time: Solidification # Solidification occurs when sufficient heat has been conducted out of the metal into the surrounding mold (or from the top of a sprue or riser to the air). The outside of the component will usually solidify earliest because of its proximity to the sand, leading smaller grains or crystals of metal to form near the surface of the component and larger, columnar grains to be directed radially towards the center of the component. This inhomogeneous grain structure may give undesirable mechanical properties which may need to be corrected by subsequent heat treatment or by machining back the surface.\nA widely used model for solification time is Chvorinov’s Rule, in which the solidification time is expressed as the product of a mold constant, .$C_M$, and a geometrical term, .$(V/A)^2$. The mold constant is a purely material-dependent constant depending on the properties of the alloy being cast and the mold material. The geometrical term is the square of the .$V$olume to surface .$A$rea ratio of the component being cast. The reason the exponent is taken to equal 2 is that most of the heat leaves the cast material by conduction (rather than convection or radiation), so heat is transported by diffusion. The relevant timescale thus increases as the square of the relevant linear dimension for heat transport.\nTotal solidification time .$T_{TS}$ is composed of the sum of: Pouring time, .$T_{MF} = V/Q$ Solidification time, .$T_S$ .$T_{TS} = T_{MF} + T_S;\\ \\ T_S \\gg T_{MF}$ (heat) Chvorinov’s rule: $$T_{TS} \\approx C_m \\left(\\frac{V}{A}\\right)^n$$ .$V$: Mold Volume .$A$: Surface Area .$VA$ is geometry dependent only .$C_m$: Mold constant (material property) .$n \\approx 2$: heat transport from molten metal is largely by conduction; so is governed by diffusion of atomic vibrations – consider Fick’s laws of diffusion Solidification begins from the outside of the mold cavity Small “chill crystals” form (which can be machined away) Columnar grains grow inwards Equiaxed grains near center Shrinkage # Two types of shrinkage are relevant in casting. The first is solidification shrinkage, which is a volumetric reduction associated with the phase change from liquid (amorphous) to solid (ordered, crystalline). A good question to ask about solidification shrinkage is whether the material is effectively solid or liquid while this shrinkage is happening, as its state during shrinkage will determine the shape of a component after the shrinkage occurs. To answer this question, we can envision the liquid-to-solid phase change happening through the nucleation and growth of solid metal crystals within the cooling molten metal. Thus, up until the moment when solidification is complete, we can reasonably think of the material as still being fluid, so that gravity will cause it to fill the walls of whatever container it is in. Thus, solidification shrinkage will not be uniform in all directions, but rather will predominantly be in the vertical direction (see the shrinkage question in the homework on casting).\nThe second form of shrinkage is thermal contraction, a type of shrinkage that you have probably already encountered in other contexts. Here, the solid cast component reduces in dimensions linearly with temperature, as its temperature falls from the melting point down to the temperature at which it is extracted from the mold.\nCast components exhibit shrinkage: Solidification shrinkage (due to liquid-to-solid phase change: disordered liquid becomes a denser, more ordered lattice structure). May lead to unwanted porosity if additional molten metal cannot enter the region vacated by the shrinking metal. Thermal contraction (reduction in solid volume as component cools from melting point to room temperature). May lead to hot tearing/hot cracking since the material is relatively soft just below its melting point. Possible defects and remedies # Defect Description/cause Possible remedy/ies Porosity Small voids in casting; caused by solidification shrinkage .$^1$ Appropriately placed risers to supply material during solidification; chills Misrun Metal solidifies before reaching the extremities of the mold cavity Avoid high-aspect-ratio features in design; pour metal at higher temperature; use heated mold (e.g. cast iron); redesign sprue/runner/gate and/or add additional sprue locations Hot cracking/tearing Differential thermal contraction of solidified material: material which has just solidified, but is still soft because of its high Chills to promote faster cooling of thicker sections so molten regions are not trapped inside the casting Pin holes Air entrapped in molten metal flow More careful pouring; appropriate sprue design Flash Metal escapes along parting line between two mold halves More careful packing of sand; post-processing of casting to remove flash Cold shut Metal flows through multiple paths in mold and solidifies before the metal from the different paths meets Same remedies as for misrun Cold shot Splattered, solidified metal entrained in flow Slower pouring Sand wash Erosion of mold surface in metal flow Reduce pouring velocity/height Sand blow Embedded sand particles in casting Careful design of sprue taper .$^1$: Voids can appear in the more massive regions of a casting which solidify later than the surrounding material, or the surface of a casting sinks, producing aVdefect known as a pipe. Some of the effects of solidification shrinkage can be counteracted by introducing chills, which are more thermally massive and conductive regions of a mold, made, e.g., of steel and embedded in the mold in close proximity to the more massive regions of a cast component to accelerate cooling.\nOther Casting Processes # Vacuum molding: a sand-casting variant # Sand held in place under compressive stress No need for binders – sand is easy to recover Plastic burns away quickly when molten metal poured Low-pressure casting # Rather than pouring the molten metal directly into the mold, it is held in a ladle beneath the mold, and pneumatic pressure is used to drive the metal through a tube into the mold. This has the advantage that it can be automated, so that filling time can be made highly repeatable. It also helps to prevent the oxides that form at the surface of the molten metal from being injected to the mold — in gravity casting, some of these oxides risk entering the mold during pouring and weakening the cast component. The mold in low-pressure casting is typically metallic (e.g. cast iron) and reusable.\nAdvantages over sand casting: Metal enters cavity at a more controlled velocity Mold is reusable: better surface finish Molten metal is taken from center of the bath – scum (oxides) from surface does not enter the mold Hot Chamber # Suitable for metals with lower melting points (e.g. Zn, Sn, Pb, in some cases Mg) Plunger system exposed to high temperatures: the metal being cast must not chemically attack this mechanism Pressures around 7-35 MPa Production rates up to 500 parts per hour High-Pressure Die Casting (Cold Chamber) # Die casting comes in two main types: hot chamber and cold chamber. The choice of machine design depends on the melting point of the metal to be cast, with the hot chamber approach more suited to lower-melting-point materials for which it is feasible to build a plunger assembly that can be immersed in the molten material. The pressures in die casting range from several MPa to over 100 MPa, which is comparable with plastic injection molding and enables molten metal to be driven rapidly into the fine geometries in a mold. Casting dies are made of alloys with higher melting points than the metal to be injected, and require highly precise machining.\nSuitable for metals with higher melting points (e.g. Al and alloys, brass, Mg alloys) since plunger mechanism is not immersed in the melt Pressures around 14-140 MPa are needed because the molten metal cools more rapidly These high pressures can cause flash Not as fast as hot-chamber die casting because of the ladling process Rubber Plaster Molding # Rubber-plaster molds at General Foundry “Chills” Investment casting # Involves coating a single-use wax pattern with a slurry which is a suspension of ceramic particles in liquid. The slurry is baked to solidify it, the pattern is melted out, the metal cast and the mold then broken off after solidification. The pattern can be produced to extremely fine tolerances (e.g. by 3D printing or machining), and the slurry particles are far finer than sand particles, so the surface finish achievable in this way is far superior to that of sand casting.\nPrepare wax pattern Several components may be connected on a “tree” Coat in fine-particle refractory material (ceramic slurry) Build up and solidify refractory coating Melt out wax Pour molten metal and solidify Break mold away from component Other forming processes # Drawing # Pull solid material through a die, instead of pushing as in extrusion Strain-hardens the material as well as reducing diameter Deep drawing # Rapid way of forming thin-walled, 3D metal shapes Sinks Drinks cans Pressing # Deformation of sheet metal between tools/dies # Vehicle body panels Introduce stiffening features, curvature Forging # Deforming metallic materials at significant fraction of melting temperature Pressing nearly molten metal with a die to form a shape 60-70% melting point Can be open- or closed-die (left and right below, respectively) Rolling # Reduce thickness of metallic sheet between stacked rollers Can be a continuous process Material is generally near but below melting point Cold rolling possible for small thickness reductions: smooth finish "},{"id":16,"href":"/eecs-16a/5/","title":"5: Basis \u0026 Circuit Analysis","section":"EECS 16A","content":" 02-22 Basis # Note 10 Slides Change of Basis # Previously we’ve seen that a basis for a vector space is a minimal spanning set of vectors. We can also define the standard basis vectors, e,x. the standard basis for .$ \\mathbb{R}^{3}$ is the set .$\\mathbb{E}$: $$\\mathbb{E} = \\big(\\hat i, \\hat j, \\hat k \\big)$$ $$\\dots \\equiv ( \\vec e_1, \\vec e_2, \\vec e_3)$$ $$\\dots \\equiv \\left( \\begin{bmatrix} 1\\\\ 0\\\\ 0\\\\ \\end{bmatrix}, \\begin{bmatrix} 0\\\\ 1\\\\ 0\\\\ \\end{bmatrix}, \\begin{bmatrix} 0\\\\ 0\\\\ 1\\\\ \\end{bmatrix}\\right) $$ We can represent any set of vectors that form as basis as a linear combination of the standard .$ \\mathbb{R}^{3}$ basis vectors\nWhen you plot a vector, the basis is normally implicit as the standard basis vectors Change Of Basis Operation: Preserves the geometrical vector but modify its coordinates such that when plotted in the new basis, the original and final vector are physically the same Such that we\u0026rsquo;re essentially re-expressing the coordinates of some .$\\vec v$ (formed with basis .$\\mathbb{E}$) in some new basis .$\\mathbb{E}'$ We know that the basis vectors define the linear transformation matrix: $$\\begin{bmatrix} | \u0026amp; \u0026amp; | \\\\ \\vec e_1 \u0026rsquo; \u0026amp; \\dots \u0026amp; \\vec e_n \u0026rsquo; \\\\ | \u0026amp; \u0026amp; | \\\\ \\end{bmatrix}$$ Thus, we need to solve for the scalars that multiply each of the new basis vectors, .$v_i \u0026rsquo; $, to produce the same physical vector .$\\vec v$ as before: $$\\begin{bmatrix} | \u0026amp; | \\\\ \\vec e_1 \u0026rsquo; \u0026amp; \\vec e_2 \u0026rsquo; \\\\ | \u0026amp; | \\\\ \\end{bmatrix}\\begin{bmatrix} v_1 \u0026rsquo; \\\\ v_2 \u0026rsquo; \\\\ \\end{bmatrix} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\end{bmatrix} = \\vec v $$ Generally, we can say that if given a vector .$\\vec v$ expressed with coordinates in the standard .$n$-dimensional basis .$\\mathbb{E}$, then we can solve for the coordinates .$\\vec v \u0026rsquo; $ in a different .$n$-dimensional basis .$A$ with .$\\vec v \u0026rsquo; = A^{-1} \\vec v$ .$A$ is formed with the columns of the new basis. To do the opposite (basis .$A \\to \\mathbb{E}$), we apply .$A$: .$\\vec v = A \\vec v \u0026rsquo; $ Finally, suppose that we’re given .$\\vec v$ with coordinates in an arbitrary basis .$P$ (matrix with columns .$\\vec p_1, \\dots, \\vec p_n$), and we want to change to another arbitrary basis .$Q$ with columns .$\\vec q_1, \\dots, \\vec q_n$. We must apply the transformation .$Q^{-1} P$. This follows from first transforming .$\\vec v$ to the standard basis from .$P$, and then transforming to the new basis .$Q$ The only question is whether we preserve the coordinates (which are the mathematical representation) and change the physical vector in accordance with the new basis, or preserve the physical vector and find the new coordinates. Page 57 has pretty figures Matrix Change of Basis # We can apply our knowledge to linear transformations to understand what it means to change the basis of a matrix\nGiven transformation .$T \\in \\mathbb{R}^{n \\times n}$ with input .$\\vec v_1$ and output .$\\vec v_2$, we can apply our column-wise interpretation of the matrix-vector product, but now, we must take care of the fact that our vectors may lie in a different basis than .$E$. Suppose we have basis vectors .$\\vec a_1, \\dots, \\vec a_n$ forming .$A \\in \\mathbb{R}^{n \\times n}$, and vectors .$\\vec v_1, \\vec v_2$ represented in this basis: .$\\vec v_i = v_{i,1} \u0026rsquo; \\vec a_1 + \\dots v_{i,n} \u0026rsquo; \\vec a_n$ Each .$v_{i,k}$ is the .$k$th element of the vector .$\\vec v_i$ We can also represent .$T$ in this basis as .$T \u0026rsquo; $ We start with .$\\vec v_1 = A \\vec v_1 \u0026rsquo; $ and .$\\vec v_2 = A\\vec v_2 \u0026rsquo; $. Since .$T\\vec v_1 = \\vec v_2$ by definition of linear transformation .$T$, we can plug in: .$TA\\vec v_1 \u0026rsquo; = A\\vec v_2 \u0026rsquo; \\Longrightarrow A^{−1} T A \\vec v_1 \u0026rsquo; = \\vec v_2 \u0026rsquo; $. Clearly, just as how we had .$T\\vec v_1 = \\vec v_2$, we have an analogous transformation in the new basis: .$T \u0026rsquo; \\vec v_1 \u0026rsquo; = \\vec v_2 \u0026rsquo; $, where .$T \u0026rsquo; = A^{−1} T A $. .$T \u0026rsquo; $ is equivalent to .$A$, then .$T$, then .$A^{−1}$\nDiagonalization # Why bother with change of basis? In practical applications, matrix operations form the core of some very computationally heavy algorithms \u0026ndash; we need to be able to easily invert matrices, raise them to high powers, and multiply them commutatively; all of these can be accomplished with diagonal matrices, where all entries not on the diagonal are zero An identity matrix of any size, or any multiple of it (a scalar matrix), is a diagonal matrix. Its determinant is the product of its diagonal values First, we must choose a diagonalizing basis .$A$ that consists of the .$n$ eigenvectors of .$T$. This is only possible if .$T$ is diagonalizable, which requires it to have .$n$ linearly independent eigenvectors Note this is different from the .$n$ columns being independent! Then, to figure out how .$T$ transforms .$\\vec v_1$, we write .$\\vec v_1 = \\vec v_{11} \u0026rsquo; \\vec a_1 + \\dots + \\vec v_{1n} \u0026rsquo; \\vec a_n$ $$T \\vec v_1 = T \\vec v_{11} \u0026rsquo; \\vec a_1 + \\dots + T v_{1n} \u0026rsquo; \\vec a_n$$ $$\\dots = v_{11} (T \\vec a_1) + \\dots + v_{1n} (T\\vec a_n)$$ $$\\dots = v_{11} (\\lambda_1 \\vec a_1) + \\dots + v_{1n} (\\lambda_n \\vec a_n)$$ Using the middle matrix (diagonal!) as a sort of \u0026ldquo;sifting\u0026rdquo; matrix; each row only has one nonzero value, so we only multiply the .$v_{1i} \\vec a_i$ value by the corresponding .$\\lambda_i$: $$T \\vec v_1 = \\begin{bmatrix} | \u0026amp; \u0026amp; | \\\\ \\vec a_1 \u0026amp; \\dots \u0026amp; \\vec a_n \\\\ | \u0026amp; \u0026amp; | \\\\ \\end{bmatrix}\\begin{bmatrix} \\lambda_{1} \u0026amp; \u0026amp; \\\\ \u0026amp; \\ddots \u0026amp; \\\\ \u0026amp; \u0026amp; \\lambda_{n} \\end{bmatrix}\\begin{bmatrix} v_1 \u0026rsquo; \\\\ v_2 \u0026rsquo; \\\\ \\vdots \\\\ v_n \u0026rsquo; \\\\ \\end{bmatrix}$$ Notice now we have an equation of the form: $$T \\vec v_1 = AD \\vec v_1 \u0026rsquo; \\Longrightarrow T\\vec v_1 = ADA^{-1} \\vec v_1 \\Longrightarrow T = ADA^{-1}$$ Here, .$D$ is the diagonal matrix containing the eigenvalues of the transformation One useful result is that .$T^k = AD^k A^{-1}$ (expand and group .$A^{-1}A = I$) Since .$D$ is easy to raise to high powers (.$D^k$ contains all diagonal entries raised to the .$k$th power), the computation of numerous transformations becomes much easier To raise a diagonal matrix to a power, one can raise each element to that power 02-24 Intro to Circuit Analysis # Slides Note 11 A B We will label nodes instead of junctions because all of the junctions that are connected to each other by wires can be labeled with a single voltage variable .$u$. A set of such junctions connected to each other only via wires is defined as a node. Nodes have the same voltage at every point on them. Circuit Analysis Algorithm # Select a reference ( ground) node .$u_1 = 0$ This node will have 0 potential \u0026ndash; all voltages are measured relative to this node Arbitrary selection \u0026ndash; any node can be chosen for this purpose. In this example, we choose the node at the bottom of the circuit diagram. Label Nodes .$[u_2, \\dots, u_{n}]$\nFirst lets look at the nodes with voltage set by Voltage Sources. Voltage sources set the voltage of the node they are connected to. In the example, there is only one source, .$V_S$, and we label the corresponding source .$u_1$ (names are arbitrary, but must be unique). Now we label all remaining nodes in the circuit except the reference \u0026ndash; in this example there are two, .$u_2$ and .$u_3$. Label currents .$[I_1, \\dots, I_k]$ through non-wire elements The direction is arbitrary (top to bottom, bottom to top, it won’t matter, but stick with your choice in subsequent steps). Then mark the element voltages following the passive sign convention. Label element potentials based on passive sign convention. The element voltage for .$I_S$ is not marked in the example since it will not be needed in the calculations below. Same for the voltage source. There is no harm in marking those, too. KCL Equations Write KCL equations for all nodes with unknown voltage, .$u_2$ and .$u_3$ in the example. See Week 2 At .$u_2$ we get (sum of all currents entering the node equals sum of currents exiting): $$I_{R_1} = I_{R_2} + I_{R_4}$$ and similar for .$u_3$: $$I_{R_4} + I_{I_S} = I_{R_3}$$\nElement IV Relationships Find expressions for all element currents in terms of voltage and element characteristics (e.g. Ohm’s law) for all circuit elements except voltage sources. In the example there are five unknown elements: .$R_1, R_2, R_3, R_4, I_S$ Find expressions for element currents for all elements (except the voltage source) using their characteristics. Applying Ohm’s law to the two resistors, we find that\n$$I_{R_1} = \\frac{V_{R_1}}{R_{1}}$$ $$I_{R_2} = \\frac{V_{R_2}}{R_{2}}$$ $$I_{R_3} = \\frac{V_{R_3}}{R_{3}}$$ $$I_{R_4} = \\frac{V_{R_4}}{R_{4}}$$ $$I_{I_S} = I_S$$ and we have .$u_1 = V_S$\nSolve .$A\\vec x = \\vec b$ \u0026ndash; see page 12 and/or slide 14 .$\\vec x = [I_1, \\dots, I_k, u_1, \\dots, u_n]^T$: The unknown element variables we\u0026rsquo;re solving for .$\\vec b \\in \\mathbb{R}^{k+n}$: The vector of elements values we\u0026rsquo;re solving for Units are either .$V$olts or .$A$mps .$A \\in \\mathbb{R}^{k+n \\times k+n}$ Steps: If there are .$n$ nodes (including the ground node), use KCL on .$(n−1)$ nodes to fill in .$(n−1)$ rows of .$A$ and .$\\vec b$ $n-1$ because we know that one of the nodes is ground If there .$k$ non-wire elements, use the IV relationships of each non-wire element to fill in the remaining .$k$ equations (rows of .$A$ and values of .$\\vec b$). Branch Current # Sometimes we want to solve for branch currents: These are easily obtained from the node voltages and element equations. For example above, the current .$I_{R_4}$ through resistor .$R_4$ is $$I_{R_4} = \\frac{V_{R_4}}{R_4} = \\frac{u_2 - u_3}{R_4}$$\nVoltage Divider # Passive sign convention # The passive sign convention dictates that positive current should enter the positive terminal and exit the negative terminal of an element. As long as this convention is followed consistently, it does not matter which direction you arbitrarily assigned each element current to; the voltage referencing will work out to determine the correct final sign. When we discuss power later in the module, you will see why we call this convention “passive.”\nTrivial Junctions # Trival junction: A junction connecting only two elements. KCL dictates that the current entering the junction must be equal to the current exiting. Since there are only two elements, it follows that the two currents must be equal (as long as we label the direction of current flow to be the same – if not, the currents will simply be opposite in sign). Therefore, another simplification to our analysis procedure is to label the currents only in the non-wire elements in our circuit (Sometimes these currents are called branch currents) When we use KCL, we can now consider nodes (instead of junctions) i.e. the current flowing into the node is equal to the current leaving the node. "},{"id":17,"href":"/cogsci-c100/consciousness/","title":"6: Consciousness","section":"CogSci C100","content":" The mind-brain problem: What is mind? # The nature of the mind-body or mind-brain connection was a philosophical question of importance in the early days of cognitive science What is mind? Is it something that is physical? Is a body necessary to have a mind? We don\u0026rsquo;t talk too much about philosophy since it\u0026rsquo;s inherently not empirical / verifiable It\u0026rsquo;s all that was possible 50 years ago Now we have neuroscience! Different viewpoints # Monism # There is only one kind of substance in the universe\nIdealism: Everything – including the material world – is actually mind Materialism: Everything that exists – including mind – is physical In some fundamental sense, the mind just is the brain, so that everything that happens in the mind is happening in the brain Aristotle: The brain is like a lump of clay; the different thoughts the mind can take on when it undergoes different patterns of activity are like the shapes the clay can assume Most cognitive scientists hold this view Dualism # Belief in the existence of both mental (e.g., “soul”) and physical substances\nThe mind and brain are two separate and distinct things Religious viewpoint Few cognitive scientists are dualists Functionalism # What makes something a thought, desire, pain (or any other type of mental state) is solely its function, or the role it plays, in the cognitive system of which it is a part More specifically, the identity of a mental state is said to be determined by its causal relations to sensory stimulations, other mental states, and behavior Ex: pain as a state that tends to be caused by bodily injury; to produce the belief that something is wrong with the body and desire to be out of that state; to produce anxiety Suppose that, in humans, there is some distinctive kind of neural activity (e.g., C-fiber stimulation) that meets these conditions, then humans can be in pain simply by undergoing C-fiber stimulation However, theory permits creatures with very different physical constitutions to have mental states as well, e.g., silicon-based states of hypothetical Martians It is also logically possible for non-physical substrates to give rise to mental states, e.g., some sort of energy field Functionalism is actually officially neutral between materialism and dualism, but it tends to be associated today with materialism, and specifically, the view that each type of mental state is identical with a particular type of neural state This type of “species-chauvinism” is a modern phenomenon due in large to an increased emphasis on neuroscience in the last 25 years Intelligence And The Physical Symbol System (PSS) Hypothesis # One of central ideas of philosophy of artificial intelligence Proposed in 1975 by computer scientists Herbert Simon and Allen Newell Holds that all intelligent behavior essentially involves transforming physical symbols according to rules GEB Ch 1-3 ++ A physical symbol system is basically an abstract characterization of a digital computer Statement of hypothesis: A physical symbol system has the necessary and sufficient means for general intelligent action\nImplications: Anything capable of intelligent action is a physical symbol system Since humans are capable of intelligent action, the human mind must be a physical symbol system Since a physical symbol system is sufficient for intelligence, machines can be constructed that are intelligent John Searle’s Chinese Room # Imagine a person who does not understand Chinese in a closed room Person receives pieces of paper through one window and passes out pieces of paper through another window The pieces of paper have symbols in Chinese written on them In the room is a huge instruction manual that tells the person in the room which pieces of paper to pass out depending on which pieces of paper he receives To all intents and purposes, the person in the room is responding in Chinese But he does not in fact understand Chinese So what does it really mean to “understand” something, to be fully “conscious”?\nTries to show that the physical symbol system hypothesis is completely mistaken Describes a situation in which symbols are manipulated to produce exactly the right outputs, but where there seems to be no genuine understanding and no genuine intelligence Searle also thinks that the Chinese room argument reveals a fundamental problem with the so-called Turing Test\u0026hellip; Turing test # Proposed by Alan Turing in 1950 as a criterion for whether a machine is displaying real intelligence If an observer is communicating with a machine and cannot tell the difference between it and a human being, then that would show that the computer was genuinely intelligent Participants: Human interrogator (judge), one human responder, and one “machine” responder Neutral communication: No visibility or other clues (e.g., all three are responding through computer terminals, so no handwriting or “voice” clues) Interrogation: The interrogator asks the other agents (human and machine) a series of questions Resolution: After a fixed time interval the interrogator tries to decide which is the “human” participant Rebuttal to the Chinese room argument # Rebuttal says that\u0026hellip; The Chinese room does not understand Chinese, but only because it is disembodied The ability to understand Chinese involves, at a minimum, being able to carry out instructions given in Chinese, to coordinate with other Chinese speakers, and to carry on a conversation In order to build a machine that could do all this, we would need to embed the Chinese room in a robot, providing it with some analog of sensory organs, vocal apparatus, and limbs Then the system could be said to understand Chinese and behave intelligently Searle’s response to robot reply The basic problem still remains: simply manipulating symbols cannot create meaning There must be more to genuine thinking than simply manipulating symbols according to rules Aside: Does Google Translate understand Language? Not really. We just feed it enough data such that it can make fairly-accurate predictions. It doesn\u0026rsquo;t understand language in that it can\u0026rsquo;t distinguish the meaning. So can some system, given enough resources (power, time, information, etc)? Then again, humans wouldn\u0026rsquo;t understand an Alien language\u0026hellip; we still have un-deciphered languages written by humans! What is consciousness? # Consciousness is generally defined in psychology as “awareness of our environment and our perceptions, images, and feelings” However, exactly what consciousness is is perhaps the most hotly debated issue in the modern philosophy of mind What is consciousness? Does it exist in all creatures? Is there some part of the brain or some particular pattern of neural activity that gives rise to consciousness? Some philosophers, like John Searle, have argued that consciousness is an emergent property of a physical brain That is, it may not be fully explained by an understanding of its component parts More recently, neurologists have also jumped into this debate\u0026hellip; Neural correlates of consciousness # Neuroscientists generally hold that consciousness results from the coordinated activity of a population of neurons But which neurons? What exactly are the neural correlates of consciousness (NCC), i.e., the minimal set of neural events sufficient for a specific conscious experience? (Christof Koch) Currently, there are two main theories: Global Neuronal Workspace Theory (Baars, Dehaene \u0026amp; Changeux) Integrated Information Theory (Guilio Tononi) Global Neuronal Workspace Theory # Global workspace theory: Explains how information is made accessible for high-level cognition, action, and speech\nWhen we are conscious of something, many different parts of our brain have access to that information E.x. the language/motor/planning module will all have access to this information When we act unconsciously, that information is localized to the specific sensory motor system involved Ex: When you type fast, you do so with little conscious awareness, so that, if asked how you do it, you would not know Information is localized in brain circuits linking your eyes to rapid finger movements The modules don\u0026rsquo;t know the meaning of the information given to them (they\u0026rsquo;re unconscious) Low level, implies that this is subconscious Global workspace theory maintains that consciousness forms when specialized programs or modules access a shared repository of information or “blackboard” Data written onto this blackboard becomes available to a host of subsidiary processes, such as working memory, language, the planning module, etc. Consciousness emerges when incoming sensory information, inscribed onto the blackboard, is broadcast globally to multiple cognitive systems\nGlobal Neuronal Workspace (GNW) Theory # Evolved from global workspace theory The network of neurons that broadcasts messages widely is hypothesized to be located in the frontal and parietal lobes Theory proposes a distributed network of high-level processors, most likely in the prefrontal, parieto-temporal, and cingulate cortices Because the blackboard has limited space, we can only be aware of a little information at any given instant Attention makes low-level modular information available for conscious control in the global workspace filter model of attention Once that information is broadcast on the network and is globally available, it becomes conscious Cortical regions important in consciousness # Frontal and parietal # Evidence that network of neurons that broadcast messages widely is located in the frontal and parietal lobes: Various types of nonconscious processing are associated with deficits in these areas, including\u0026hellip;. # 1. Hypnosis # Hypnosis is associated with Decreased activity in the dorsal anterior cingulate When hypnotized, you are in a state so absorbed in listening that one is not thinking about anything else No selective attention Reduced connections between the dorsolateral prefrontal cortex (motor actions) and the default mode network (awareness of one’s actions), which includes the medial prefrontal and the posterior cingulate cortex \u0026ndash; why you do not recall being hypnotized 2. Repression and Dissociation # Repressed memories Recovered memories of child abuse Dissociative identity disorder (multiple personality) Condition in which two or more identities or personalities alternate in control of a person’s behavior One personality can be diabetic, near-sighted, or allergic to a substance and the other not The two personalities can have different brain waves, vital signs and hormonal levels Some personalities are aware of the other personalities Research indicates that in repression/dissociation, the prefrontal cortex (executive control) disengages processing in the hippocampus (memory) Participants asked to memorize word pairs, e.g., ordeal-roach or steam-train Respond condition: Participants were shown cue word and asked to recall the matched word Suppress condition: Participants were shown cue word and asked to actively suppress the matched word Word suppression was associated with activation of prefrontal cortex to disengage processing in hippocampus Suppressing matched word reduced later recall of word Takeaway: Brain is actually more active when avoiding recalling a memory than during recall itself - Perhaps why mindfulness is impactful\n3. Lucid dreaming # Whereas dreams are unconscious, lucid dreams are conscious Lucid dreaming: Neuroimaging data is scant but preliminary results suggest that prefrontal and parietal regions are also involved in lucid dreaming Currently, there is only one fMRI study contrasting lucid and non-lucid REM sleep and it is a case study (Dresler, Wehrle, Spoormaker et al., 2012) Few people who can lucid dream at will =\u0026gt; few potential subjects Interestingly though, the results of this study converge with MRI studies that have evaluated individual differences in lucid dreaming frequency (Baird, Castelnovo, Gosseries et al., 2018) Compared to non-lucid REM sleep, lucid REM sleep is associated with increased activity in Prefrontal cortex (metacognition and self-reflection) Parietal cortex and the precuneus (self-referential processing, episodic memory, and experience of agency) Occipital and inferior temporal regions (visual processing) Lucid dreams are oftentimes associated with increased visual vividness and clarity of the dream scene 4. Unilateral spatial neglect # Visual neglect syndrome or unilateral spatial neglect: Tendency to ignore – or to be unaware of – information on one half of visual field, usually the left side Typically occurs after damage (e.g., stroke) to right hemisphere, particularly damage to the parietal and frontal lobes Patients are asked to draw from memory or to copy an illustration (Driver \u0026amp; Vuilleumier, 2001)\nPosterior hot zone # However, other research suggests that it is primarily regions in the “posterior hot zone” – not the prefrontal – that generate the sights, sounds, and other sensations of life as we experience it Electrical stimulation of cerebral cortex # Prior to removing a brain tumor or locus of a patient’s epileptic seizures, neurosurgeons map functions of nearby cortical tissue by directly stimulating it with electrodes Stimulating the posterior hot zone triggers a variety of distinct sensations and feelings Stimulating the frontal cortex by and large elicits no direct conscious experience Similar effects have been found after removal of cortical tissue Removal of large sections of frontal cortex (e.g., prefrontal lobotomy) does not significantly affect conscious experience, though patient may develop problems with emotional control, motor deficits, or uncontrollable repetition of specific actions or words However, removal of even small regions of the posterior cortex can lead to loss of an entire class of conscious content – patients may be unable to recognize faces or to see motion, color, or space Brain-injured patients # One possible reason for the discrepancy in research findings is that the part of the cerebral cortex that is primarily associated with consciousness depends on the type of consciousness in question In particular, some philosophers have distinguished between two types of consciousness: Access consciousness (or A-consciousness): Pertains to accessibility of information, i.e., conscious vs. nonconscious information processing Prefrontal and parietal cortical areas may play important roles in this Related to the “easy problem” of consciousness: explaining in computational or neural terms how an organism accesses and deploys information Phenomenal consciousness (or P-consciousness): Pertains to how and why we experience the world as we do Posterior hot zone may play critical role in this This is what David Chalmers has called the “hard problem” of consciousness Why and how is it that sentient organisms have qualia or phenomenal experiences? Why and how is it that some internal states are felt states (e.g., heat or pain), rather than unfelt states (e.g., seeing a thermostat or a toaster)? The Global Neuronal Workspace Theory of consciousness lends insight to access consciousness However, it does not address the problem of phenomenal consciousness Integrated Information Theory, which will be turning to shortly, does address the latter Cerebellum # One thing though that most researchers agree on is that the seat of consciousness is not located in the cerebellum, though this part of the brain contains Four times as many neurons as the cortex Half the total number of neurons in the whole brain People who lack a cerebellum (either from birth or as a result of brain injury) are still capable of conscious perception, leading a “normal” life without any loss of awareness Suggests that sheer number of neurons is not a decisive factor in the creation of conscious experience\nBut why? One reason might be that the cerebellum’s processing mostly happens locally with minimal interactions between neurons The cerebellum is almost exclusively a feed-forward circuit with no complex feedback loops that reverberate with electrical activity passing back and forth It’s functionally divided into hundreds of independent computational modules with distinct, non-overlapping inputs and output, controlling movements of different motor or cognitive systems This idea that exchange and integration of neural signals is the basis of phenomenal consciousness is one of the main ideas of integrated information theory Integrated Information Theory # In the early 2000s, Guilio Tononi pioneered a technique called zap and zip to probe whether someone is conscious or not Scalp of patient was “zapped” with an intense pulse of magnetic energy using TMS (transuranium magnetic stimulation) This induced a brief electric current in the neurons underneath, which would reverberate across the cortex, exciting and inhibiting other neurons A network of EEG sensors recorded those electrical signals, and as they unfolded over time, yielded a movie The data from the movie was compressed using an algorithm commonly used to “zip” computer files Zipping yielded an estimate of the complexity of the brain’s response Loss and recovery of integration and information in thalamocortical networks: A: Wakefulness\nB: Anesthesia\nC: Vegetative state: UWS (unresponsive wakefulness syndrome), MCS (minimally conscious state), LIS (locked-in syndrome)\nResearch findings from zap and zip: Volunteers who were awake had a “perturbational complexity index” significantly highly than when deeply asleep or anesthetized Method was subsequently able to correctly determine whether patients were conscious or in a vegetative state Measures of the brain’s responses to the TMS also seem to predict the consciousness of patients in a non-communicative and vegetative state– a finding with potentially profound clinical applications This suggests that the more information that is shared and processed between many different components of the brain in response to a single experience, the higher the level of consciousness This is the main idea of integrated information theory (IIT): Consciousness arises from neural integration and complexity Similar to what GNWT says \u0026ndash; different parts of brain that can access information are higher level If information integration theory is right, it would have implications far beyond neuroscience and medicine For instance, proof of consciousness in a creature, such as a lobster, could transform the fight for animal rights It would also answer some long-standing questions about AI Tononi argues that the basic architecture of the computers we have today \u0026ndash; made from networks of transistors \u0026ndash; precludes the necessary level of information integration that is necessary for consciousness (given our current medium we cannot represent consciousness) Even if they can be programmed to behave like a human, they would never have our rich internal life He emphasizes this is not just a question of computational power or the kind of software that is used “The physical architecture is always more or less the same, and that is always not at all conducive to consciousness”\nXenobots # Potential for consciousness in xenobots (“living robots”)? Created by scientists from skin cells and heart cells in the form of stem cells harvested from frog embryos Xenobots are able to move in a coherent fashion to explore their watery environment and can survive for days or weeks, powered by embryonic energy stores Made of organic material (thus, biodegradable) so it shouldn\u0026rsquo;t cause long-term issues Functions Groups of xenobots can move around in circles, pushing pellets into a central location Others were built with a hole through the center and were able to use that as a pouch to successfully carry an object When xenobot was cut in half, it stitched itself back up and kept going Potential applications Serving as new material for technologies that is fully biodegradable Intelligent drug delivery: carrying medicine to a specific place in body Traveling in arteries to scrape out plaque Searching out and break down harmful compounds or radioactive wastes Gathering microplastics in the oceans Criticism of IIT # Tononi’s methods (zap n zip) so far only offer a very crude “proxy” of the brain’s information integration To really prove his theory’s worth, more sophisticated tools will be required that can precisely measure processing in any kind of brain One problem is that, using previous techniques, the time taken to measure information integration across a network increases “super exponentially” with the number of nodes under consideration Even with the best technology, the computation could last longer than the lifespan of the universe Daniel Toker at UCB has recently proposed a shortcut for the mathematical calculations necessary to test the theory Controversies in CogSci: # What are some potential strengths and weaknesses of Tononi’s Integrated Information Theory?\nEvidence for unconscious processes # Some prominent psychologists today maintain that 100 years of research has provided no clear evidence for the existence of the “unconscious,” but that claim seems to be exaggerated By one estimate, our five senses take in 11,000,000 bits of information per second, of which we consciously process about 40 Some specific evidence for the existence of the unconscious\u0026hellip; Consciousness and thought suppression # White bear/red Volkswagon study Goup 1: Participants were told to try not to think about white bears Goup 2: Participants were told to try not to think about white bears but if they did, to replace the thought with the image of a red Volkswagon Which group was more successful? Group 2: It’s very difficult (if not impossible!) to suppress a maladaptive thought; it’s much easier to replace the thought with a more desirable one (Wegner, Schneider, Carter et al., 1987)\nUnconscious behaviors # Split brain: A condition in which the two hemispheres of the brain are isolated by severing the connecting fibers (mainly those of the corpus callosum) between them After operation, patients often notice that left hand seems to have a “mind of its own” Suggests that consciousness involves operations of verbal mechanism located in left cerebral hemisphere\nFreudian slips Lood gegs Bine foddy Unconscious perception # Subliminal perception and priming Rope tying study Participants are asked to tie together two strings that are hanging from the ceiling The strings are separated so that they can’t reach one of them while holding the other A table and pliers are made available At some point, the researcher walks into the room and accidentally sets one of the strings swinging Invariably, within a few minutes, the participant would figure out the solution to the problem\u0026hellip; When interviewed afterwards though, they said that the idea “just came to them” (Maier, 1931)\nSurgery patients in double-blind study wore earphones during their operations, listening to either Soothing background music and Positive suggestions about the safety and success of the procedure Results: Compared to controls, experimental group Woke up feeling significantly less pain (25% on average) Required less pain medication post-surgery (70 required no opiates at all, compared with 39 in the control group) (Nowak, Zech, Asmussen, et al., 2020 ) Unconscious communication # Study on 23-year-old woman who showed no outward signs of conscious awareness after being in a car accident (Owen, Coleman, Boly et al., 2006; wn, 2014) When researchers asked her to imagine playing tennis vs. walking around her home, fMRI scans revealed activity in regions similar to healthy person’s brain Follow-up analysis of 42 behaviorally unresponsive patients revealed 13 more who also showed meaningful though diminished brain responses to questions (Stender, Gosseries, Bruno et al., 2014) Researchers wonder if such fMRI scans might enable a “conversation” with some unresponsive patients, by instructing them, for example, to answer yes to a question by imagining playing tennis Repressed memories # Recovered memories of child abuse Dissociative identity disorder (multiple personality) Anosognosia: “unawareness of illness” Stroke patients with this disorder may deny that his arm is paralyzed Unconscious conditioning in advertising # Men shown picture of car with sexy woman standing in front judged car to be more appealing, better designed, more expensive, faster, and less safe than control group of men However, 22 out of 23 participants denied their rating had been influenced by the presence of the model “I never let myself be blinded by advertising; the car itself is what counts\u0026quot;\nUnconscious processing and sexual attraction # We are often influenced by factors of which we are entirely unaware Suspension bridge study: Males were interviewed by attractive female supposedly as part of research project, either Just after crossing a narrow, wobbly footbridge 230 feet above rapids OR 10 minutes after crossing the bridge They were given the researcher’s telephone number in case they had questions later Those in first condition were much more likely to call to ask for a date afterwards Participants had no idea their attraction was influenced by the situation (Meston \u0026amp; Frohlich, 2003) Powerful idea (beyond sexual attraction) You don\u0026rsquo;t know how green the grass is on the other side (how much better things really are) if you never make the effort (choose free will) If you are already where the grass is greener, you don\u0026rsquo;t recognize or fully appreciate the processes that lead you here Unconscious learning (Extra) # Unconscious learning: behavioral responses can be reinforced through associations without person’s awareness Double agent experiment Graduate student interviewer was told to nod his head whenever participant engaged in a particular behavior (e.g., chin rubbing) order to reinforce this behavior “Interviewer” was actually the real participant in the experiment; the participant was a confederate “Participant” was instructed to rub his chin whenever the interviewer said “yeah” Frequency of interviewer’s saying “yeah” increased substantially When interviewer was eventually told what had happened, his reaction was one of “stunned incredulity” (Rosenfeld \u0026amp; Baer, 1969) Thumb twitch study Participants were told that they were participating in a study on effects of stress on body tension and that effects of stress would be manipulated by randomly alternating periods of soothing music and static In fact, noise was not presented randomly: it was terminated whenever participants contracted a very small muscle in their left thumb that could only be detected by an electrode Participants in uninformed group were told nothing about how static could be turned off Participants in partly informed group were told that static could be turned off by specific response and to try to discover that response Results: Dramatic increase in contractions of this muscle in all participants However, interview afterwards revealed that all the participants in uninformed group still believed they had no control over the noise Only one participant in the partly informed group believed that he had discovered the effective response, which involved “subtle rowing movements with both hands, infinitesimal wriggles of both ankles, a slight displacement of the jaw to the left, breathing out, and then waiting” (Hefferline, Keenan, Harford et al., 1959) Controversies in Cognitive Science # What are some of the implications of the computational model of mind generally? And more specifically, with regard to consciousness? What are some limitations of the computational model of mind? Altered States of Consciousness # Hypnosis # Hypnosis: social interaction in which one person (the hypnotist) suggests to another (the subject) that certain perceptions, feelings, thoughts, or behaviors will spontaneously occur Hypnotic susceptibility # Hypnotic susceptibility: Correlated with measures of imagery vividness and absorption – people who are hypnotically susceptible tend to have rich fantasy lives and easily become absorbed in the imaginary events of a novel or movie Uses of hypnosis # Hypnotic recovery of memories # Hypnosis may boost recall Used to help witness in Chowchilla school bus kidnapping case to successfully recall kidnapper’s license plate Use of hypnosis in treatment of physical and psychological disorders # But can also cause people to construct false memories and to increase their confidence in these false memories Hypnotized witnesses may end up testifying confidently to events they never experienced Latter is particularly problematic because highly hypnotizable subjects are especially vulnerable to false memory suggestions Hypnotherapy: Clinical applications of hypnosis Hypnosis quite successful in treating physical disorder (e.g., warts, headaches, asthma) Not so successful in treating psychological disorders (e.g., smoking, overeating, alcoholism) Recovery rate for latter increases though when combined with other therapies like systematic desensitization Hidden observer # Research by Hilgard suggests that a dissociated part of the hypnotized person (the hidden observer) is aware of what is happening even when person is ostensibly unaware Ice water study: person kept smiling while hidden observer wrote, “This is agony, let me out!” Lemon study: person seemed to be enjoying “orange,” while hidden observer yelled out “You’ve just squirted acid in my mouth!” Use of hypnosis in pain control # Use of hypnosis in childbirth Standard hypnotherapy Hypnobirthing: combination of self-hypnosis and childbirth education A number of studies have indicated that hypnobirthing is associated with shorter hospital stays, shorter length of labor, reduction in self-reported pain, reduced epidural and analgesic use However, other studies have found inconclusive results, so overall more research is needed Mechanism: There are sensory and emotional components of pain perception Sensory component is mediated by somatosensory cortex Cognitive/emotional component is mediated by the anterior cingulate cortex and the prefrontal cortex fMRI studies using hypnotic suggestion found that a decrease in the unpleasantness of pain reduced the activation of the anterior cingulate cortex without affecting the activity of the somatosensory cortex Hallucinogens # History Used in Aztec, Mayan, Incan, West African, South Asian, and Egyptian societies since ancient times To promote physical and mental healing To induce spiritual experiences and access “altered states of consciousness” 1960s emergence of counterculture movement led to widespread usage of hallucinogens in the US Social/cultural differences in use of hallucinogens Largely part of underground lifestyle in the West Openly used for spiritual purposes in other parts of the world LSD # LSD: Lysergic Acid Diethylamide Drug action: Stimulates serotonergic and dopaminergic receptors \u0026ndash; not fully understood Positive effects: Causes perceptual distortions and hallucinations: “altered states of consciousness” Emotions can vary from euphoria to detachment or panic Sense of self may dissolve, as does boundary between oneself and external world Research has indicated that LSD may be effective for treating anxiety due to terminal illness, alcoholism, and cluster headaches Adverse effects: No documented fatalities from pharmacological action of LSD, but behavioral fatalities and suicides can occur May trigger panic attacks and extreme anxiety (“bad trips”); flashbacks May trigger psychotic break, especially in those with family history of schizophrenia Maps page MDMA, Ecstasy # Drug action: Causes release of serotonin, norepinephrine (adrenaline), and dopamine, and blocks their reabsorption Positive effects: Emotional elevation, disinhibition, feelings of connectedness with everyone Research has indicated that MDMA may be effective for treating PTSD Adverse effects: Dehydration, overheating, and increase in blood pressure can cause death, especially when combined with dancing at raves “Ecstasy” pills may be cut with dangerous chemicals Potential damage to serotonin-producing neurons, leading to increased risk of depression and sleep problems Less serotonin the next morning, can also be long-term if used too much Memory impairments Maps page Psilocybin # Drug action: Stimulates serotonin receptors Positive effects: Causes euphoria, perceptual distortions, hallucinations May induce spiritual experiences Marsh Chapel Experiment on Harvard Divinity School students in 1962 Participants reported profound religious experiences In 25-year follow-up, all of the participants described experience as having elements of “a genuine mystical nature and characterized it as one of the high points of their spiritual life” Single administration induced significant increase in personality dimension of openness to experience that persisted for over a year May be effective in treating depression and OCD Adverse effects: May cause nausea, panic attacks, confusion, and psychotic episodes, leading to accidents and suicide attempts Psychedelic therapy # Abram Hoffer study in the 1960’s Gave alcoholics a small dose of mescaline, then deliberately induced peak experiences by means of music, poetry, painting – whatever used to produce peak experiences before the person became alcoholic 50% were supposedly permanently cured Moratorium on research in this area from early 1970s to early 2000s due to war on drugs However, resurgence of interest and research in this area in recent years, in particular with regard to use of hallucinogens – especially MDMA and LSD – to treat substance abuse, PTSD, obsessive-compulsive disorder, depression, cluster headaches, and emotional suffering associated with terminal illness UCB launched the campus’s first center for psychedelic science and public education in 2020 Will conduct research using psychedelics to investigate Cognition, perception, emotion and their biological bases in the human brain Initial studies will focus on psilocybin Integration of psychedelics with psychotherapy to treat psychological disorders and brain mechanisms involved Ability of these compounds to improve cognitive flexibility, alter visual perception, engender feelings of awe and change patterns of brain activity Center also plans to collaborate with the Graduate Theological Union and eventually train guides or facilitators, in the cultural, contemplative and spiritual care dimensions of psychedelics Mysticism or psychosis (Extra!) # \u0026ldquo;From the first, the experience seemed to me to be holy. What I saw was the Power of Love – the name came to me at once – the Power that I knew somehow to have made all the universes, past, present and to come; to be utterly infinite, an infinity of infinities, to have conquered the Power of Hate, its opposite, and thus created the sun, the moon, the planets, the earth, light, life, joy and peace, never ending\u0026hellip;.In that peace I felt utterly and completely forgiven, relieved from all burden of sin. The whole infinity seemed to open up before me, and during the weeks and months that followed I passed through experiences which are virtually indescribable. The complete transformation of “reality” transported me as it were into the Kingdom of Heaven. I feel so close to God, so inspired by His Spirit, that in a sense I am God. I see the future, plan the Universe, save mankind; I am utterly and completely immortal; I am even male and female. The whole Universe, animate and inanimate, past, present and future is within me; all things are possible.”\nAnswer: Psychotic episode of John Custance\nAll at once, without warning of any kind, I found myself wrapped in a flame-colored cloud. For an instant I thought of fire, an immense conflagration somewhere close by in that great city; the next, I knew that the fire was within myself. Directly afterward there came upon me a sense of exultation, of immense joyousness accompanied or immediately followed by an intellectual illumination impossible to describe. Among other things, I did not merely come to believe, but I saw that the universe is not composed of dead matter, but is, on the contrary, a living Presence; I became conscious in myself of eternal life, but a consciousness that I possessed eternal life then; I saw that all men are immortal; that the cosmic order is such that without any peradventure all things work together for the good of each and all; that the foundation principle of the world, of all the worlds, is what we call love, and the happiness of each and all is in the long run absolutely certain.”\nAnswer: Mystical experience of R.M. Bucke, Canadian psychiatrist\n“When I walk the fields, I am oppressed, now and then, by an innate feeling that everything I see has a meaning, if only I could understand it. And this feeling of being surrounded with truth, which I cannot grasp, amounts to indescribable awe sometimes. Have you not felt that your real soul was imperceptible to your mental vision, excepting a few hollow moments?”\nAnswer: Mystical experience of Charles Kingsley, a Christian mystic\n“I am simple; I need not think. Feeling is all; feeling is love. God is love. Love is the expression of God. Feeling is only fire. Fired by God. The dancer becomes the divining motion. Dance is the divine in the world. The Dionysian religion. Love is God. I am love; I am God.”\nAnswer: Psychotic experience of Nijinsky, Russian ballet dancer\nDistinctions: The mystic, unlike the person with psychosis, develops a strong sense of self The mystic tends to reduce self-importance; psychosis tends to involve inflated self-importance The mystic tends to have ever increasing serenity, which leads him or her to be more involved in life and more loving towards all beings; those with psychosis have difficulty relating with anybody and clearly withdraw from the world The mystical experience, though ineffable, is usually coherent and what is described is clear; those with psychosis tend to be thought-disordered so their descriptions are not very lucid The mystical experience is usually brief, though it leaves so vivid an imprint that it can be remembered clearly 25 years later; in psychosis, the person may get stuck in the experience and be unable to come out of it In mysticism, there is a gradual reduction of attachment to the world; in psychosis, there is a fusion and a continuous shifting of the world The mystic tends to take responsibility, not only for themselves, but for all aspects of life around them; those with psychosis project out of themselves those things that seem especially negative Near-Death Experience # Near Death Experience: an altered state of consciousness reported after a close brush with death Reported by about 10 to 15 percent of those revived from cardiac arrest Many describe visions of tunnels, bright lights or beings of light, a replay of old memories, and out-of-body sensations Physiological explanation They suggest that damage to the bilateral occipital cortex may lead to visual features of NDEs such as seeing a tunnel or lights, and \u0026ldquo;damage to unilateral or bilateral temporal lobe structures such as the hippocampus and amygdala\u0026rdquo; may lead to emotional experiences, memory flashbacks or a life review. They concluded that future neuroscientific studies are likely to reveal the neuroanatomical basis of the NDE which will lead to the demystification of the subject without needing paranormal explanations\nTypical report: “I was left with an awareness that something more was going on in life than just the physical part of it\u0026hellip; There is more than just consuming life, more than just what we can buy. There comes a point when you have to give in to it\u0026hellip; The typical near-death survivor emerges from his experience with a heightened appreciation for life, determined to live life to the fullest. He has a purpose in living even though he cannot articulate just what the purpose is.”\nControversies in CogSci: Implications and limitations of the computational model of mind # Ultimately, the experiences that Altered States of Consciousness involve cannot really be captured by language\u0026hellip; Cognitive Neuroscience of Consciousness # As discussed earlier, various types of nonconscious processing are associated with suppression of or reduced activity in parts of the frontal and parietal cortices Hypnosis is associated with reduced activity in the dorsal anterior cingulate and reduced connections between various regions of the frontal cortex that are part of the default mode network (self-awareness) and the motor cortex Non-lucid dreaming, in comparison with lucid dreaming, is associated with reduced activity in areas of the prefrontal and temporoparietal lobes involved in self-referential processes Study by Michael C. Anderson found that in repression, the prefrontal cortex (executive control) disengages processing in the hippocampus (memory) Other important brain structures: Anterior cingulate cortex (ACC) Forms “collar” around front part of corpus callosum Functions: Integrates cognitive and affective information Awareness and processing of conflicting information Selective attention Insular cortex Lies deep within the lateral sulcus Functions: Self-awareness Consciousness Emotional regulation Extra:\nIn addition, study on repression found that autonomic arousal during free association task Predicted subsequent memory failure Was accompanied by increased activation of conflict-related brain regions (e.g., anterior cingulate cortex) and deactivation of memory-related regions (e.g., hippocampus) (Schmeing, Kehyayan, Kessler, et al., 2013) When patients with dissociative identity disorder read stories that pertained to their trauma (Simone Reinders), the alters that were unaware of the trauma, relative to alters that were aware of the trauma, showed Increased activity in cingulate gyrus Reduced amygdala and insula activity Reduced cardiovascular response "},{"id":18,"href":"/eecs-16a/6/","title":"6: Voltage Dividers \u0026 Measurement","section":"EECS 16A","content":" \\(\\) 03-01: Voltage Dividers # Voltage Divider # The voltage divider circuit consists of a voltage source (.$V_S$) and two resistors (.$R_1$ and .$R_2$) Example: We label the node connected to the voltage supply as .$u_1 (= V_S)$, since the voltage supply goes between this node and ground. Label the remaining node as .$u_\\text{mid}$ and the voltages and currents through every element in the circuit with .$V_i$ and .$I_i$ respectively Write KCL equations for all nodes with unknown voltage - in this case, this is just .$u_\\text{mid}$, since .$u_1 = V_S$. The current entering that node is .$I_{R_1}$ and the current leaving it is .$I_{R_2}$ Since these currents must be equal, .$I_{R_1} = I_{R_2}$ 4. Find expressions for element currents for all elements (except the voltage source) \u0026ndash; all steps on page 3 $$I_{R_1} = \\frac{V_S - u_\\text{mid}}{R_1}$$ $$I_{R_2} = \\frac{u_\\text{mid}}{R_2}$$ 5. Substitute the element currents into our KCL equation. We have $$I_{R_1} = I_{R_2} \\Longrightarrow \\frac{V_S - u_\\text{mid}}{R_1} = \\frac{u_\\text{mid}}{R_2}$$ 7. Solve the above equation. Rearranging, we find that $$V_S R_2 −u_\\text{mid}R_2 = u_\\text{mid}R_1$$ $$ \\Longrightarrow u_\\text{mid}(R_1 +R_2) = V_SR_2$$ $$ \\Longrightarrow u_\\text{mid} = \\frac{R_2}{R_1 + R_2} V_S = \\frac{1}{1+ \\frac{R_1}{R_2}} V_S = \\alpha V_S$$ The reason this circuit is called a \u0026ldquo;voltage divider\u0026rdquo; is that we can create any output voltage of .$u_\\text{mid} = \\alpha V_S$ for any .$\\alpha \\in [0,1]$ (assuming that all of the resistance values are non-negative) by varying the ratio of the resistor values .$R_1/R_2$. As we will see shortly, varying this ratio is exactly the mechanism we will use to convert the relative position of a user’s touch to a voltage. .$R_2$, the resistor in the numerator, is the one next to ground. .$R_1$ is connected to a non-zero voltage node (in this case .$u_1 = V_S$). Capacitor Divider # The capacitor divider is similar, differing in that the numerator is now the component closest to $V_{in}$ rather than closest to ground (as in the voltage divider with resistors) $$V_{out} = \\frac{C_1}{C_1 + C_2} V_{in}$$ Current Divider # Current $I_X$ in a resistor $R_X$ that is in parallel with a combination of other resistors of total resistance $R_T$ is $$I_X = \\frac{R_T}{R_X + R_T}I_T$$\n$I_T$ is the total current entering the combined network of $R_X$ in parallel with $R_T$ $R_T$: Total resistance of the circuit to the right of resistor $R_X$ 03-03: Power and Voltage/Current Measurement # Physics of Circuits # Read 7B 25.2 - 5\nPower refers to the rate of energy change, .$P = \\frac{dE}{dt} \\text{ [Watts]}$ .$\\dots \\Longrightarrow dE = V\\ dQ \\Longrightarrow \\frac{dE}{dt} \\equiv P = V \\frac{dQ}{dt} = VI$ .$P_\\text{el} = I_\\text{el} \\cdot V_\\text{el} = V^2_\\text{el} \\cdot R^{-1}_\\text{el} = I^2 _\\text{el} R _\\text{el}$ $P \u0026gt; 0 \\implies$ (positive) power dissipated, negative power generated. By PSC, resistors always dissipate power because current enters the .$+$ terminal Voltage sources tend to generate power, since current comes out of the .$+$ terminal (and the product .$P = IV \u0026lt; 0$) $P \u0026lt; 0 \\implies$ (positive) power generated, negative power dissipated. In an isolated (circuit) system, the sum of the power (across all components) should equal zero by conservation of energy Useful sanity-check .$R = \\rho \\frac{L}{A}$ Touchscreen # Given that the top (red) layer has a resistivity .$\\rho$ and a cross-sectional area .$A$, the resistance of the top layer from the touchpoint to the right-hand end is given by .$R_1 = \\rho \\frac{L_\\text{rest}}{A}$, the resistance of the top layer from the left-hand end to the touchpoint is given by .$R_2 = \\rho \\frac{L_\\text{touch}}{A}$ We can see that .$u_\\text{mid}$ can be found because it\u0026rsquo;s a voltage divider: $$u_\\text{mid} = \\frac{\\rho \\frac{L_\\text{touch}}{A}}{\\rho \\frac{L_\\text{rest}}{A} + \\rho \\frac{L_\\text{touch}}{A}}V_S = \\frac{L_\\text{touch}}{L_\\text{touch} + L_\\text{rest}}V_S = \\frac{L_\\text{touch}}{L}V_S$$ .$L = L_\\text{touch} + L_\\text{rest}$: The length of the touchable portion of the screen The relationship we have found between .$u_\\text{mid}$ and .$V_S$ is very convenient because .$u_\\text{mid}$ is not dependent on any material property such as .$\\rho$ and .$A$. This means that the top layer can be built with any material and the relationship between .$u_\\text{mid}$ and .$V_S$ is still valid. There are always some non-idealities in the world \u0026ndash; by making .$u_\\text{mid}$ independent of any material property, we can make the circuit model immune to such non-idealities. We also have the freedom to choose a material for the top layer that is good for display purposes (rather than needing a specific material for the touchscreen to work).\nMeasuring a Circuit # The voltmeter measures voltage across the circuit, while the ammeter needs to be put in-line with the circuit so that the current flows through the ammeter.\nThe measurement should not change the energy of the circuit It turns out that the most complete and concise way of guaranteeing these measurement tools do not influence the circuit is to state that they do not allow any power dissipated through the measurement device. Voltmeter # Because our voltmeter is made to measure voltage, we can naturally assume that the .$V \\neq 0$; this means that a voltmeter must have .$I = 0$ going into it to ensure .$P = IV = 0$.\n.$I=0$ occurs when in open-circuits, where exactly zero current is flowing.\nRecall that, for a given voltage, the higher the associated resistance, the lower the current and therefore the lower the dissipated power. That is, .$\\lim{R \\to \\infty} \\Longrightarrow I = 0$ $$V_\\text{el1} = I_\\text{meas} R$$ $$V_? - V_\\text{el1} - V_\\text{meas} = 0$$ $$\\Longrightarrow V_? = V_\\text{el1} + V_\\text{meas}$$ $$\\Longrightarrow V_? = I_\\text{meas} R + V_\\text{meas}$$ $$\\therefore V_? = V_\\text{meas} \\iff I_\\text{meas} = 0$$\n$$I_\\text{meas} = \\frac{V_\\text{meas}}{R_\\text{meas}}$$ $$\\therefore I_\\text{meas} = 0 \\iff R_\\text{meas} \\gg V_\\text{meas}$$\nVoltmeters are added in parallel to the circuit, otherwise they would stop the current from flowing. Ammeter # The ammeter has the circuit’s current flowing through it. Therefore, to ensure .$P = IV = 0$, the ammeter needs .$V = 0$.\n.$V=0$ occurs in short-circuits (ideal wires), where exactly zero potential difference exists\n$$I_? = I_R + I_\\text{meas}$$ $$\\therefore I_? = I_\\text{meas} \\iff I_R = 0$$\n$$I_R = \\frac{V_\\text{meas}}{R}$$ $$\\therefore I_R = 0 \\iff V_\\text{meas} = 0$$\nAmmeters are added in series to the circuit, otherwise they would short-circuit the measured component. "},{"id":19,"href":"/e-29/6/","title":"6-7: Joining processes","section":"Engineering 29","content":" \\(\\) Joining Processes # Welding-based # There are many ways of assembling mechanical components to create mechanisms and structures. Examples of joining processes include the use of fasteners such as bolts and rivets, adhesives, and push fits, in which interference is used to connect components together. Welding, meanwhile, involves fusing two components by melting the material near their interface, and possibly introducing additional material of the same kind to fill any gaps between the components. Metallic and polymeric materials can be welded, and the techniques used to melt the material vary with the properties of the materials being joined.\nIt is important to note that welding involves complete fusion of the components being joined: the joint is made of the same material as the components and thus can be extremely strong relative to other joining techniques because it enables us to eliminate inter-material interfaces along which cracks might propagate. Drawbacks of welding, however, include the fact that because we are heating a portion of the components above their melting point, dimensional distortion of the components may occur, and even if distortion does not occur there may be modification of the material’s mechanical properties in a heat affected zone (HAZ) around the weld. This property modification may involve recrystallization or annealing of metallic materials, softening the component after it cools.\nWelding terminology # A heat source emits heat some distance from the weld. A fraction .$f_1$ of this heat reaches the material, and a fraction .$f_2$ of the heat that arrives is retained within the weld region, and actually is used in melting material. The remainder of the heat is lost either to the surroundings, or conducted through the solid components where it may modi.$f_y$ the microstructure without melting the material. The energy required for melting a unit volume of the material may be approximated as being proportional to the square of the absolute melting temperature. If we know the rate of heat generation and the heat transfer factor .$f_1$ and melting factor .$f_2$, we can thus determine the volumetric rate at which material can be melted and thus how fast a weld with a given cross-sectional area can be created\nProportion of total heat from source contributing to melting of metal: .$f_1 f_2 H$ .$H$: Heat from source .$f_1$: Heat transfer factor \u0026ndash; amount of heat that goes into welding material .$f_2$: Melting factor \u0026ndash; amount of heat that goes into welding region Energy required for melting: $$U_m = K T^2_m$$ .$U_m$: Melting energy (J/mm.$^3$) .$K = 3.33 \\cdot 10^{-6}$ J/(mm.$^3$K.$^2$) .$T_m$: Melting temp (K) Power for melting: $$U_m A_w v = f_1 f_2 IE$$ .$IE$: Electric Power .$I$: Current (Amps) .$E$: Voltage (Volts) Rates of melting material: $$R_{H,w} = f_1 f_2 R_H = U_m R_{w, V} = U_m A_w v$$ .$R_{H,w}$: Rate of heat delivery to weld [.$W$atts] .$R_H$: Power of heat source .$R_{w,V}$: Volumetric rate of melting; .$A_w v$ .$U_m$: Melting energy (J/mm.$^3$) .$A_w$: Cross-section area of weld .$v$: Velocity of heat source Oxyacetylene welding # This versatile technique has been used for many decades to weld metals, and involves a two-stage reaction, first the combustion of acetylene to produce carbon monoxide, hydrogen and heat, and second the reaction of the CO with hydrogen and oxygen to produce carbon dioxide, water, and more heat.\nThis approach lends itself to welding in remote locations (all that is needed is a gas tank and torch) and because of the high temperature of the flame, many materials can be welded. Can also be done underwater.\nOne well known challenge with this technique is hydrogen embrittlement where the hydrogen gas generated can become incorporated into the microstructure of the component, weakening it.\n@ 1:00\nArc welding # In arc welding of any kind, a voltage is applied between the workpiece and an electrode which are separated by a gap of a few millimeters. The electric field across this gap is large enough to ionize the gas and generate a plasma in the gap. This plasma is electrically conductive, yet still highly resistive so that heat is generated as the current passes through the plasma. It is this heat that melts the material. Because the plasma is so highly localized and the heat is generated within a millimeter or so of the weld, the welded region can be precisely controlled.\nManual Metal Arc (Stick) Welding # In Manual arc welding, the electrode is a hand-held rod of the same material as the workpiece, and it melts as the weld forms, providing material to fill the gap between components. The rod has a length of several inches, so the length of weld that can be produced in one welding operation is limited. This limits the speed of the technique. The rod is usually coated with a flux, which melts during welding to produce an acidic liquid on the surface of the work, etching away naturally occurring oxides to expose the metal surface and enable a strong, continuous metallic weld to be produced.\n@ 1:37\nSubmerged arc welding # This process is suitable for producing long, straight welds in larger components. The electrode is a consumable wire that is fed into the system from a spool, and a powdered flux is fed in front of the advancing weld from a hopper. The weld region lies submerged by the flux, which helps to prevent sparks and strong ultraviolet radiation from the weld escaping from to the surroundings. The flux and electrode handling mechanism is usually mounted on a motorized carriage that moves along the component at a constant rate.\n@ 3:06\nMetal Inert Gas (MIG) Welding # Here, the electrode is consumable and fed from a continuous spool of wire. Instead of flux, an inert gas such as Argon is fed around the electrode as a shield, to prevent oxidation of the workpiece surface.\n@ 5:50\nDip vs Spray Transfer in MIG Welding # @ 6:50 + 8:30\nTungsten Inert Gas (TIG) Welding # Like MIG welding, an inert gas shield is used, but instead of a consumable electrode, a tungsten electrode with much higher melting point is used. If filler material is needed it is introduced from the side using rods of materials. The electrode can maintain a sharp shape (less than a millimeter diameter), so that highly precise and consistent welds can be produced. This also makes it much harder to learn.\n@ 11:25\nDC (Direct Current) Electrode is held negative Suitable for all metals except Al and its alloys AC (Alternating Current) Used for Al and alloys because of the robust oxide that aluminum forms which needs to be removed to enable a clean weld Frequencies 20-150 Hz used Positive-electrode part of cycle helps strip oxide from the weld pool but leads to electrode heading Negative-electrode part of cycle heats workpiece Electrical Resistance Welding # Electrodes are clamped across a stack of metal sheets to be welded. A current is passed through the stack, resistively heating the material between the electrodes. Material at the center of the stack heats up the most, because the thermal conduction path by which generated heat can escape to the surroundings is the longest there. The amount of heat energy generated can be precisely controlled, which makes this excellent for joining thin sheets that could be destroyed by a flame or an arc. The weld region is sometimes referred to as a nugget and the process is called a spot weld. Seam welding is a continuous form of spot welding whereby the sheet material passes between two conductive rollers and a series of overlapping spot welds are made in rapid succession. This process is suitable for creating sealed metal containers.\nSpot welding Sheets clamped between electrodes Current passes through Lozenge of material melts around the interface Seam welding Sheets pass between conductive rollers Current is pulsed Continuous weld is formed from overlapping spot welds @ 16:48 \u0026amp; 17:20\nFriction (Stir) Welding # Two components are rotated relative to each other at high speed under load (e.g. by mounting in a lathe). The heat generated by friction softens both components’ surfaces and they fuse.\nTwo components undergo relative rotational motion in contact Heat resulting from friction at the interface leads to melting Components are pressed together until they fuse and cool @ 20:48\nUltrasonic Welding # A horn couples ~40 kHz vibrations into the components Oscillatory compression of the components translates to shear motion at the intended joint, and dissipation of energy as heat via friction \u0026ldquo;Energy directors\u0026rdquo; are features at the welded interface that lower contact area, and concentrate the loads, promoting material melting Laser Welding # Focused laser beam provides the heat Readily automated weld paths Extremely precise welds (sub-mm-size) are possible Electron Beam Welding # Focused beam of electrons (produced, e.g., by thermionic emission) is accelerated and steered towards workpiece by electromagnetic field reaching an electron energy of ~30-100 keV Loss of kinetic energy of electrons upon collision with workpiece generates heat Carried out in vacuum: as low as ~10-4 Torr (0.01 Pa) As with laser welding, extremely high-precision and small welds are possible Similar pros and cons as for e- beam melting additive m.$f_g$ vs selective laser melting Can make especially deep, narrow welds Typical Power Densities in Welding # Power density reaching material surface Type of welding Typical power density (W/mm.$^2$) Oxyacetylene 10 Arc 50 Resistance (electrical) 1000 Laser 9000 Electron beam 10000 Modeling Heating in Welding # It is helpful to be able to predict the size of the weld pool and HAZ for given process parameters Rosenthal developed a simple equation for how temperature varies around a heat source $$T(w,y,z) - T_0 = \\frac{q}{2\\pi kR} e^{-\\left(\\frac{v(w+R)}{2a}\\right)}; R = \\sqrt{w^2 + y^2 + z^2}$$ $T_0$: Temperature of work at start $v = \\frac{dw}{dt}$: Velocity of beam $q$: Rate of heat delivery [Watts] $R$: Distance along weld $k$: Thermal conductivity $a$: Thermal diffusivity Time .$t_{8/5}$ for temperature to fall from 800 °C to 500 °C as the heat source passes is a key parameter for determining condition of HAZ: ideally about 10-25 s for carbon steels Non-Welding Joining Methods # Other joining methods that involve introducing molten material to an interface are soldering and brazing, but in both these processes the gap-filling material is an alloy with a lower melting point than the components being joined. Soldering is used, for example, to join electrical components to printed circuit boards or to connect pipes in plumbing. Brazing is simply the term used for soldering when the filler melts above 400˚C. There is some inter-mingling of atoms of the filler and component material near the interfaces, which lends the joints strength, but soldered/brazed joints are still not as strong as welded ones can be.\nBrazing # Filler is a metal or eutectic alloy with lower melting point than the work Advantages cf welding Good for thin walls Less heat required Suitable for joining dissimilar metals Avoid HAZ Use capillary forces to fill inaccessible joints Disadvantages cf welding Lower strength Appearance: differing colors Weakens at high service temperatures Applications Piping, preparing cutting tools e.g. cemented carbide Types of adhesive # Pressure-sensitive Viscoelastic solid \u0026ndash; forms under load Conforms to target solid Maximize van der Waals interactions Promotes mechanical interlocking Thermoplastic e.g. hot-melt glue Irreversible liquid-solid transition Radical-driven crosslinking: UV or thermally cured Epoxy resins (2-part; reacts when mixed to cure) Thermosetting silicones Cyanoacrylate:rapidly polymerizes in presence of water Fasteners: Riveting # Simple to process and automate Can\u0026rsquo;t be loosed by vibration (used in aircrafts) Typically cheaper (doesn\u0026rsquo;t need to be toleranced precisely) Doesn\u0026rsquo;t need the depth that threaded components do Fasteners: Screws and Bolts # Screw characterized by: Major, minor diameters Pitch (inverse of turns per inch), thread angle Length Conventional formats: [Diameter]-[Turns / unit] x [Length] #4-40 x 0.5: Imperial; #4 diameter, 40 turns per inch, 0.5 length (in) 1/4-20 x 5/8: Imperial; 1/4 \u0026hellip;, 20 \u0026hellip;, 5/8 M3-0.50 x 10: Metric; 3 mm radius, 0.50 Pitch (mm), 10 length (mm) Various head styles available Square, counterbore, countersunk, round etc Important to protect against loosening in use Sprung, split washers Glue "},{"id":20,"href":"/eecs-16a/7/","title":"7: 2D Touchscreens \u0026 Superp. + Equivalence","section":"EECS 16A","content":" 03-08: 2D Resistive Touchscreens # Bottom Plate # Trivially, we see that if we add a wire connected to .$u_\\text{mid}$ that will have the same voltage across it (wires have zero voltage drop) So why have the bottom plate at all? It lets us take the measurement for .$V_\\text{out}$ using connection points on the edge of the plate instead of having to put a probe or wire at the actual touch point every time! What if this bottom plate is non-ideal? That is, .$R \\neq 0$ ? # Both of these resistors are followed by an open circuit. From the definition of an open circuit, we know that zero current will flow through it. If .$i_\\text{mid} = i_{R4} +i_{R3}$ from KCL, and .$i_{R4} = i_{R3} = 0$ from the definition of an open circuit (Ohm’s Law says that .$0V = R \\cdot 0A$), the voltage across these new resistors will be .$0$. This means that, even with an imperfectly conductive bottom plate, the voltage .$V_\\text{out}$ will still be equal to .$u_\\text{mid}$, even with the addition of these new resistors. To measure an output voltage, we need to put some device at the open circuit labeled .$V_\\text{out}$. Interesting Circuit # These are two parallel voltage dividers, thus we can write: $$u_2 = \\frac{kR_1}{R_1 + k R_1} V_s = \\frac{k}{1+k}V_S$$ $$u_3 = \\frac{kR_2}{R_2 + k R_2} V_S = \\frac{k}{1+k}V_S$$\nWe see that regardless of the resistances .$R_1$ and .$R_2$, the potentials .$u_2$ and .$u_3$ are the same! This holds as long as .$k$ is constant: $$u_2 = u_3 = \\frac{k}{1+k}V_S$$ We can add .$R_3$ and it won\u0026rsquo;t change the circuit behavior Since .$u_2 = u_3$, there is no .$\\Delta V$, thus no current flows through the resistor. We can also find this with KCL: .$R_3 i_3 = u_2 - u_3 = 0 \\therefore i_3 = 0$ This means that .$R_3$ is at the special .$(0, 0)$ point on the .$I$-.$V$ plot, where it behaves the same way as a wire or open circuit That is, we could replace .$R_3$ with an open circuit and nothing would change 2D Resistive Touchscreen # Now, let’s introduce the physical structure of a 2D touchscreen: it consists of a top red plate and a bottom black plate. When a finger touches the screen, the top red plate is pushed into contact with the bottom black plate at the touch point. The top and bottom ends of the top red plate as well as the left and right ends of the bottom black plate are made of materials that have very low resistivities .$\\rho$, we can treat them as ideal wires (.$\\rho = 0$). The materials of the transparent screen that we touch in the middle have much higher resistivity. Top Plate # We can treat the red plate as a bunch of vertical resistor strips, where each vertical strip is connected to the strips next to it by horizontal resistors as well. When we touch the plate, we split it into a top and bottom half, or .$R_\\text{rest}$ and .$R_\\text{touch}$. Rather than considering many vertical strips, we will divide the red plate into just three equal vertical segments represented by resistors, which are connected by horizontal resistors .$R_{h1}$ and .$R_{h2}$. Adding a voltage supply .$V_S$ we can see this is the same interesting circuit as before! Since .$R_\\text{rest}$ and .$R_\\text{touch}$. are the same for each segment, we know that .$u_2 = u_3 = u_4$. As with the “interesting circuit” can replace horizontal resistors .$R_{h1}, R_{h2}$ with open circuits. $$u_3 = \\frac{R_\\text{touch}}{R_\\text{rest} + R_\\text{touch}}V_S$$ $$\\dots = \\frac{\\rho \\frac{L_\\text{touch}}{A}}{\\rho \\frac{L_\\text{touch}}{A} + \\rho \\frac{L_\\text{rest}}{A}}$$ $$\\dots = \\frac{L_\\text{touch}}{L}V_S \\text{ for } L_\\text{touch} = L_\\text{touch, vertical}$$\nThis means that .$u_3$ is mapped to the vertical position touched in the same way as the 1D touchscreen. When measuring the vertical position touched (.$L_\\text{touch, vertical}$), the bottom black plate connects to a voltmeter and measures .$u_3$, the same way it did in the 1D touchscreen Note that, although we have represented the top red plate by three segments of equal width in the circuit model we built, the value of .$u_3$ will remain the same if we choose to represent the top red plate by an infinite number of segments. Bottom Plate # We know from linear algebra that if we want to find two values (i.e. vertical and horizontal position), we will need two measurements. To find the horizontal position, we connect the supply voltage source .$V_S$ to the bottom black plate, and connect the top red plate to a voltmeter. As before, we choose to represent the bottom black plate by three segments of equal width which are connected in between by vertical resistors .$R_{v1}, R_{v2}$. $$u_3 = \\frac{R_\\text{touch}}{R_\\text{touch} + R_\\text{rest}} V_S = \\frac{L_\\text{touch}}{L}V_S$$ The important simplification used is replacing .$R_{h1}, R_{h2}$. with open circuits for the .$L_\\text{touch}$, horizontal measurement and .$R_{v1}, R_{v2}$. for the .$L_\\text{touch}$, vertical measurement. However, this kind of simplification is valid only if the resistor is at .$(0, 0)$ on the .$I$-.$V$ plot, which means the resistor has zero current flow and therefore zero voltage drop (.$IR = V$). From the .$I$-.$V$ plots, although a resistor, a wire and an open circuit can behave quite differently, their behaviors are exactly the same at .$(0, 0)$. This means that at .$(0, 0)$, these three circuit elements can be replaced by one another and the same behavior .$(I = 0, V = 0)$ is still expected. Fast Analysis # Prior, we used the fact that all of the segments had .$R_\\text{rest} = R_\\text{touch}$. In this section, we’ll consider a similar circuit where the resistances are all different\nWrite equations for the nodes with voltage sources between them. Here, the ground node and .$u_1$ have .$V_S$ between them, so .$u_1 = V_S$ Write KCL for any unknown nodes, using the .$V = IR$ relationship, and taking into account any current sources connected to the node If we consider node .$u_2$, the sum of the currents flowing out of .$u_2$ through .$R_1, R_5, R_3$ is 0. $$ \\frac{u_2 - V_S}{R_1} + \\frac{u_2}{R_3} + \\frac{u_2 - u_3}{R_5} = 0,$$ $$\\frac{u_3 - V_S}{R_2} + \\frac{u_3}{R_4} + \\frac{u_3 - u_2}{R_5} = 0$$ Now, we have just two equations and two unknowns (.$u_2$ and .$u_3$) so all the remains to be done is to solve the equations by hand. You can check that the signs of your voltage drops are correct by checking that the node in question is always positive in the numerators. So in the first equation, .$u_2$ is positive, while in the second equation, .$u_3$ is positive\nWhat if we have a current source? How do we KCL? # Remember that we treat all currents as flowing out of the node. We should also take the direction of the current source into account. In this case, the left branch of .$u_1$ has a current source .$I_S$ flowing in, so .$−I_S$ current flows out. So our equation is\n$$-I_S + \\frac{u_1 - u_2}{R_1} + \\frac{u_1 - u_3}{R_2} = 0$$\n03-10: Superposition and Equivalence # Equivalence # If we pick two terminals within a circuit, we say that another circuit is equivalent to the original circuit if it exhibits the same .$I$-.$V$ relationship at those two terminals.\nAn example of an .$I$−.$V$ is that of a resistor, i.e., .$V = IR$ or .$I= \\frac{V}{R}$ We can do this since voltage and current are governed by a linear relationship (in 16A), and a line can be uniquely determined by exactly two points, we can capture the original circuit with a simplified circuit that has exactly two components: a voltage (or current) source and a resistor Short-circuit current: # Current .$I_\\text{sc} \\equiv I_{No}$ when .$V_S = 0$. Used for norton equivalent\nIf we look at the .$I$-.$V$ plot of a voltage source .$V_S$, where .$I$ is the current going through the voltage source, then the plot would be a vertical line: $V_S = 0$ means that it allows any current to go through, however the voltage drop always remains zero. This is exactly what a wire element (sometimes called a short circuit) does.\nOpen-circuit voltage: # Voltage .$V_\\text{open} \\equiv V_{Th}$ when .$I_S = 0$. Used for thevenin equivalent\nIf we plot the .$I$-.$V$ graph of a current source .$I_S$, we get the a horizontal line: If we turn off the current source, .$I_S = 0$, which means no matter what voltage you apply, there will be no current. This is equivalent to an open circuit.\nMotivation # When we add a new component to a circuit, it interacts through only 2 parameters: current .$I$ and voltage .$V$. Equivalent circuits are used to simplify interactions between circuits.\nLet’s take the simplest case where interactions are only through one pair of nodes. In that case, we just have two possible quantities: the voltage across the nodes and the current flowing through the connections. The relationship between this current and this voltage would then fully define the interactions between the circuits. This is where the idea of equivalence comes in. If we have a circuit that exhibits the same .$I$−.$V$ relationship from the standpoint of a pair of nodes, the other circuit (the one you are interacting with) can’t tell the difference.\nThe idea of equivalence is to be able to replace one (or both) of the interacting circuits with a simpler circuit that will give us the same overall behavior.\nLimitations # The .$I$-.$V$ characteristic is the only feature preserved by an equivalent circuit Equivalence tells us nothing about the power in a circuit. From the standpoint of any other nodes in the circuit (i.e. any pairs of nodes), the circuit may or may not be equivalent That is, looking at the same circuit but examining a different pair of terminals may not produce equivalent .$I$−.$V$ relationship. Series \u0026amp; Parallel # These are proven starting page 10 of notes We cannot use these to simplify our circuit if there are dependent sources. Remember that only independent sources are zeroed out, and there are no resistor formulas for dependent sources. In addition, some resistor configurations cannot be decomposed into combinations of parallel and series resistances. Series: The voltage is the sum of the voltage drops of the individual components: $$V = V_1 + \\dots + V_n = I (R_\\text{total})$$ From KCL we know elements will have the exact same current through them: $$I = I_1 = \\dots = I_n$$ Resistance is the sum of their individual resistances: $$R_\\text{total} = R_1 + \\dots R_n$$ Parallel: From KVL we know elements will have the exact same voltage across them: $$V = V_1 = \\dots = V_n$$ The current in each individual element is found by Ohm\u0026rsquo;s law: $$I = I_1 + \\dots + I_n = V R_\\text{total}$$ Total resistance will always be less than the value of the smallest resistance $$\\frac{1}{R_\\text{total}} = \\sum_{i=1}^n \\frac{1}{R_i}$$ Notation: We use .$\\parallel$ to denote parallel components\nComparison of effective resistance (left), inductance (middle) and capacitance (right) of two resistors, inductors and capacitors in series and parallel\nThevenin # Any linear two-terminal circuit can be replaced at terminals .$A$–.$B$ by an equivalent combination of a voltage source .$V_{Th}$ in a series connection with a resistance .$R_{Th}$ \u0026ndash; wiki\nFind .$V_{Th}$: We recognize that it equals the difference in node potentials at .$A$ and .$B$, so performing NVA on the original circuit will yield .$V_{Th}$ Connect an open circuit across the two output terminals and measure the voltage across them. This measured .$V_{oc}$ equals .$V_{Th}$. Find .$R_{Th}$: Zero out any independent sources. Remember, this means voltage sources turn into a wire and current sources turn into an open circuit. Then apply either a test current into the terminal and measure the resultant voltage, or apply a test voltage and measure the resultant current. .$R_{Th}= \\frac{V_{test}}{I_{test}}$ Norton # Any linear two-terminal circuit can be replaced by a current source .$I_{No}$ and a single resistor .$R_{No}$ in parallel \u0026ndash; wiki\nFind .$I_{No}$: Connect a short circuit across the two output terminals and measure the current through it. This measured .$I_{Sc}$ equals .$I_{No}$. Find .$R_{No}$: Zero out any independent sources. Remember, this means voltage sources turn into a short circuit and current sources turn into an open circuit. Then apply either a test current into the terminal and measure the resultant voltage, or apply a test voltage and measure the resultant current. .$R_{Th}= \\frac{V_{test}}{I_{test}}$ Note that the second step doesn’t change because .$R_{No}$ is equal to .$R_{Th}$! Relation / Conversion # A Norton equivalent circuit is related to the Thévenin equivalent by\n$$R = R_{Th} = R_{No}$$ $$V_{Th} = I_{No}R$$ $$I_{No} = \\frac{V_{Th}}{R} $$\nNote that .$R_{No}$ is equal to .$R_{Th}$, since the slope of the .$I$-.$V$ curve is the same. Now, instead of looking at the .$V$-axis intercept, we find the intersection with the .$I$-axis: At the intersection with the .$I$-axis, the voltage drop between .$A$ and .$B$ is zero, which is equivalent to placing a wire between .$A$ and .$B$ (i.e. shorting .$A$ and .$B$). We denote the current through the wire be .$I_{No}$.\nWhy this works:\nSince the .$I$-.$V$ relationship is linear, we can calculate the slope (which is the reciprocal of resistance) from any two points. .$V_{Th}$ and .$I_{No}$ are the points where the .$I$-.$V$ curve crosses the.$V$ and .$I$ axes, respectively (see the left-hand figure). However, this method does not work if .$V_{Th}$ and .$I_{No}$ do not provide two unique points on the .$I$-.$V$ curve (see the right-hand figure). Specifically, this method only works if there is at least one independent source in the circuit. When there are no independent sources, .$V_{Th} = I_{No} = 0$ which does not provide enough information to calculate Req. Superposition # Recall two weeks back: To solve for the currents and node potentials in a circuit, we set up a matrix problem of the form .$A\\vec x =\\vec b$ where .$\\vec x$ contained the unknown currents and node potentials, .$\\vec b$ contained the independent current and voltage sources, and .$A$ described the relationship between them. Since this matrix equation describes a real system, we know that there is a unique solution. Therefore, .$A$ is invertible: $$\\vec x = A^{−1} \\vec b$$ This means that we can describe any current or node potential (ie. any element of .$\\vec x$) as a linear combination of the independent current and voltage sources (the elements of .$\\vec b$).\nFor example, consider a circuit with .$n$ independent sources voltage sources .$V_{s1} \\dots V_{sn}$, and .$m$ independent current sources .$I_{s1} \\dots I_{sm}$. An arbitrary node potential .$u_i$ (or equivalently, an arbitrary current .$i_i$) can be written as $$u_i = \\alpha_1 V_{s1} + \\dots +\\alpha_n V_{sn} + \\beta_1 I_{s1} + \\dots \\beta_m I_{sm}$$ where the .$\\alpha$’s and .$\\beta$’s are coefficients from inverting .$A$. Since this equation is linear, we can calculate each term of this equation separately and then add them together at the end.\nFor example, if we want to calculate the first term, .$\\alpha_1 V_{s1}$ we can set all of the other voltage and current sources to zero, then solve for .$u_i$. Repeating this for every source then adding the results is equivalent to calculating .$u_i$ with all of the sources present. However, splitting up the calculations can help us see simplifications and patterns that might be less obvious with all of the sources present.\nTL;DR:\nFor each independent source .$k$ (either voltage source or current source) Set all other independent sources to .$0$ Voltage source: replace with a wire \u0026ndash; no voltage drop Current source: replace with an open circuit \u0026ndash; no current flows Compute the circuit voltages and currents due to this source .$k$ Compute .$V_\\text{out}$ by summing the .$V_{\\text{out; }k }$s for all .$k$. We can apply the idea of replacing elements with equivalent elements (e.g. replacing a .$V_S = 0$ voltage source with a wire) to resistors as well. When do resistors have an equivalent representation? Recall that by Ohm’s law, the .$I$-.$V$ graph across a resistor. From this we can see:\nZero voltage source and zero resistance are equivalent to wires (i.e. short circuits); Zero current source and infinite resistance are equivalent to open circuits "},{"id":21,"href":"/cogsci-c100/mindfulness/","title":"7: Mindfulness","section":"CogSci C100","content":" Mindfulness, awareness and acceptance # Why Study Mindfulness? # Western research has indicated that meditation can be effectively used to: Treat psychological disorders, including anxiety, depression, substance abuse, eating disorders, sleep disorders, ADHD, and conduct disorder Recent reviews indicate that overall effect sizes are modest, but mindfulness is still considered an evidence-based treatment for most psychological disorders The small effects are comparable with what would be expected from the use of psychotropic medications (e.g., antidepressant) in a primary care population but without the associated toxicities Treat physical conditions, including chronic pain and cardiovascular disease, and enhance immune system function Lengthen life span quite dramatically \u0026ndash; 30% reduction in mortality due to heart disease and 49% reduction in mortality due to cancer, according to one 20-year study (Schneider, Alexander, Staggers, et al., 2005) If this is replicable then you\u0026rsquo;d make lots of money \u0026ndash; part of the reason for the research interest Slow the cognitive decline associated with “normal aging” (Lazar, Kerr, \u0026amp; Wasserman, 2005; Luders, Toga, Lepore et al., 2009) Increase activity in areas of the brain associated with optimism, empathy, attention, and emotional regulation Highly trained “super-meditators” show activity in these regions that is off the charts Calm-abiding practice # All mindfulness therapies share a common aim of cultivating an attitude of awareness of the present moment with acceptance This is an attitude that extends toward whatever is arising at the moment, including thoughts, feelings, and experiences of contact with the outer world The aim is in line with the psychoanalytic view that it is primarily experiential avoidance that causes distress Research # Neuropsychological evidence that awareness/labeling of negative emotions actually reduces their intensity \u0026ndash; fMRI study by Lieberman, Eisenberger, Crockett et al. (2007) Participants were shown frightening faces/faces expressing strong emotions and asked to choose a word that described the emotion on display Controls were asked merely to identify the gender of the people in the photos Labeling the fear-inducing object Reduced activity in the amygdala, the seat of fear and other negative emotions Increased activity in a parts of the prefrontal cortex (right ventrolateral and medial PFC) involved in vigilance and discrimination, relative to controls Naming the emotion transformed the images from objects of fear to objects of scrutiny, potentially resulting in a more effective response Study on spider phobia (Kircanski, Lieberman, \u0026amp; Craske, 2012) Researchers recruited participants who had a spider phobia and exposed them to spiders Four experimental conditions that differed in their instructions for what to do with the anxiety Label the anxiety felt about the spider Reappraisal: think differently of the spider so that it feels less threatening Distract from the anxiety elicited by the spider No specific instruction (control) Later (on Day 2 and Day 9), participants returned to the lab and were again exposed to spiders to test the long-term effects of their emotion manipulation Those who had been assigned to labeling their emotions had lower physiological reactivity to the spiders, as measured by skin conductance responses Within the affect labeling condition, participants who verbalized a larger number of fear and anxiety words had even fewer skin conductance responses Limitations # Unfortunately though, being accepting and nonjudgmental is easier said than done because of our natural human tendency to avoid painful experiences It is very difficult, if not impossible, to force ourselves to fully experience situations and emotions that we deem to be painful Traumatic experiences are a classic example of this However, even in ordinary life, we are constantly dissociating from painful situations and emotions \u0026ndash; or ones that we think may potentially cause pain This may range from actual pain-inducing experiences to insights that might cause us to feel bad about ourselves There is a limit to the extent to which we can force ourselves just to “be with” our anger or sadness or fear Meditation # Meditation is a technique that can help resolve this problem of experiential avoidance Step One in mindfulness practice is to engage in focused meditation because\u0026hellip; It is very difficult to be aware of ourselves and our surroundings if we are afflicted by \u0026ldquo; monkey mind\u0026rdquo; Focused meditation allows us to calm monkey mind Focused meditation ultimately enables us to connect with the nonmoving mind and the “suchness” of experience, which produces a sense of general “Okness” Focused meditation is just calmly abiding, with the object of concentration, Dhāraṇā \u0026ndash; not obsessive glomming There should be a relaxed connection with the object The aim is to “be present with the object,” not to lock onto it and grip it like a dog, or to concentrate on it the way you might concentrate on memorizing the details of an image As the meditation develops, allow your perception of the object to change as you are present with the object on more and more subtle levels… The focus becomes just stillness itself, rather than the physical object You find that there is something in you that is imperturbable and open, fundamentally calm Developing one-pointedness ( Ekaggata) of concentration enables you to find one-pointedness in yourself, the non-moving mind, which resolves all things Stilling the eyes is a way of stilling the mind\nInstructions # Find a quiet place and sit in a comfortable position with your spine straight. You can sit cross-legged or in a chair. Tilt the chin very slightly down but do not allow the head to loll forward. Place your hands comfortably on your lap. Take a few deep breaths. Collect your attention, and begin to move it slowly down your body. Notice the sensations in each part of your body. Relax any tensions, particularly in the face, neck and hands. Direct your gaze softly downwards or close your eyes if you wish. Focus on an object (e.g., a bit of the wall in front of you, a statue, a sound like “Om”) or on the breath. Be fully present with the object of your attention. If you are focusing on the breath, don’t gloss over the inhalation and say, “OK, well, this is the inhalation part” \u0026ndash; that’s just labeling it. “Be with” each in-breath for its full duration and with each out-breath for its full duration, as if you were riding the waves of your own breathing. Seek nothing. No “next”! Integrate your whole body in the experience. Meditation is state of full body awareness. If your attention wanders (and it will!), simply release the thought and focus back on the object of your attention. After a while, you can try noticing the distractions and acknowledging it with a simple word, e.g., thinking, wandering, hearing. Meditation Q \u0026amp; A # If one has a physical condition that makes sitting uncomfortable, is it OK to meditate while lying down or standing up? Yes, and in fact, depending on the physical condition, that might be highly advisable There are four standard meditation positions: sitting, lying down flat on one’s back, standing, and walking Can focused meditation produce negative effects? In general, focused meditation is considered a safe practice, but there are large individual differences in how people respond to any practice, so use good judgment and stop and seek help when necessary In particular, for those with trauma or abuse histories (or “fear types”), even basic meditation practices can can lead to intense, and possibly uncomfortable emotions In those cases, when doing focused meditation, it is recommended that one direct attention to an external object or the feet, rather than the breath, as the latter may trigger too many memories and emotions Lurid example of the agitated mind and how it causes us to be unable to sit alone with our thoughts and emotions Participants were asked to sit in a chair, without a device or a book and without falling asleep, for 6 to 15 minutes They were given the option to self-administer mild electric shocks rather than just sit alone with their thoughts 67% of men and 25% of women did just that! (Wilson, Reinhard, Westgate, et al., 2014) Relationship between attention and happiness # Study in which 1000 people were texted at random times throughout the day As soon as participants received text, they had to answer 3 questions What are you doing right now? Where is your mind right now? Is it focused on what you’re doing or is it focused elsewhere? How happy or unhappy are you right at this very moment? (Killingsworth \u0026amp; Gilbert, 2010) Results: Average American adult spends 47% of waking life not paying attention to what they’re doing When they were not paying attention, they were significantly less happy Meditation and the Default Mode Network # Meditators from various traditions show reduced activity in their Default Mode Network (DMN) when meditating, as well as when they are not meditating The DMN or “task negative network” Includes posterior cingulate cortex (PCC), medial prefrontal cortex (mPFC), and angular gyrus Is active when we are not focused on a particular task \u0026ndash; rumination Is involved in self-referencing, recognition of emotions in others, remembering the past, and imagining the future Is associated with ruminating about the past, worrying about the future, thinking about what other people are thinking about you “A Wandering Mind Is an Unhappy Mind” (Killingsworth)\nMindfulness as a Two-Part Process # However, use of calm-abiding practice (focused awareness) to stabilize the mind is only the first step in mindfulness practice This first step is what make the second step \u0026ndash; mindfulness or awareness of the present moment with acceptance (open monitoring) \u0026ndash; possible because\u0026hellip; Calm-abiding practice connects you with the suchness of things, with the ground of reality, which imbues all things with a sense of “Okness” This deep state causes the sense of judgment to naturally drop away, and there is a cognition that everything is exactly as it should be Whatever is felt is meant to be felt, and whatever is thought is meant to be thought “There is no where you can be that you were not meant to be” ♬\nMindfulness practice # When a thought arises, note How it manifests in your body How it manifests in your energy (i.e., emotionally) Whether it is a positive, negative, or neutral thought These are all implicit and shouldn\u0026rsquo;t require any thought You can also try noting more specifics about the thought, e.g., whether it is about wanting, grasping, anger, fear, etc., as well as the kind of self the thought is associated with Integration of body and mind # A number of modern-day teachers have emphasized that inclusion of some sort of practice that integrates the physical body with the mind (such as various types of yoga, running, etc) is critical for mindfulness to be effective for modern laypeople who do not have luxury of spending 12 hours a day meditating To a large extent, becoming more mindful or aware of one’s thoughts and emotions means becoming more aware of one\u0026rsquo;s body Meditation can help us “physicalize the mind,” so that when we have a negative thought, we can actually feel it almost as something physically pulling us off center This can make it vastly easier to control negative thoughts and emotions in daily life When you have a negative thought, it\u0026rsquo;s as if someone is pulling you off axis \u0026ndash; mindfulness is being aware of this connection A meta-analysis of 78 fMRI studies of meditation found that one of the main commonalities among various different styles of meditation was changes in activity of the insula (Fox, Dixon, Nijeboer et al., 2016) The insula specializes in body awareness Some yogis have demonstrated quite remarkable control over body functions, such as breathing, heart rate, body temperature and other vital function e.x. Tummo practice Instructions # When your attention is relatively stable on the object of attention, try shifting your awareness to the process of thinking itself. Watch thoughts come into and leave the field of your attention. Try to perceive them as “events” in your mind. Note their content and their charge while, if possible, not being drawn into thinking about them, or thinking the next thought, but just maintaining the “frame” through which you are observing the process of thought. Note that an individual thought does not last long. It is impermanent. If it comes, it will go. Be aware of this. Note how some thoughts keep coming back. Note what feelings are associated with different thought contents. If you get lost in all this, just go back to focusing on the object. Important: this exercise requires great concentration and should only be done for short periods of time, like two to three minutes per sitting in the early stages. Insight # Working with Negative Emotions # If you are in an overall relatively positive frame of mind and the emotion is not very strong: Try to experience the emotion fully in your body-energy-mind, that is, focus on the energy and physical sensations of the emotion, but do not let yourself get caught up in the content, in the story associated with the emotion Initially, you should only try to do this very briefly This practice is not about analyzing thoughts or emotions If you are still thinking about the emotion or the situation that caused the emotion 30 seconds later, STOP (this is important \u0026ndash; you can harm yourself!) It\u0026rsquo;s easy to recognize this, and even tell yourself this \u0026ndash; it\u0026rsquo;s another to truly embody this belief You may not feel happy due to some event/circumstance that clouds your vision. Recognize these emotions\u0026ndash; what is this stimuli evoking that you wish to avoid? Try to focus on this emotion and realize that it is just that \u0026ndash; an emotion, sensations in your body. They are not the end-of-the-world, just sensations, and as such they are malleable and can be changed if you believe so.\nOnce you have substantial meditation experience, when you experience a negative emotion, you can\u0026hellip; Look straight at the emotion and let it dissolve or “liberate” as it arises Instantaneously see the various factors that caused the emotion to arise That is, mindfulness gives us insight into the narrative we have about ourselves Rather than holding beliefs (e.g., “I am worthless”) to be a true description of who we are, we see the narrative as a constellation of thoughts This can foster more breathing room and lead to increased well being It’s not so much about changing the narrative, but rather about changing our relationship to it Your ordinary mind cannot do this: remember that meditation is not done with the ordinary thinking mind When we think, we just stay caught in our own conceptual systems, our own habitual ways of looking at things Meditation accesses a deeper mind with much greater awareness that is able to solve problems more effectively Instructions (Dealing with Negative Emotions) # Start with calm-abiding meditation. Now if there\u0026rsquo;s something difficult that\u0026rsquo;s happening for you \u0026ndash; a difficult emotion, or a physical sensation that\u0026rsquo;s hard, let your attention go to that. It may be an aching in your shoulder or back, or a headache, or it could be a sense of sadness, or anxiety, or anger. Where do you feel that sensation in your body? Where do you feel that emotion in your body? Notice it, just notice it for one moment. Tap into it, feel it. Make sure to breathe. And now return your attention back down to the object of concentration and the centered stillness. And just let yourself stay there for a moment. Feeling it, sensing it, relaxing, maintaining the mindfulness, yet giving yourself a break from what could be potentially overwhelming to feel. And now once again return your attention to that part of the body that feels unpleasant \u0026ndash; the body ache or pain, or the emotion, the sensations of the emotion in your body, the vibrations in your chest, or the clenching in your belly, or the tightness in your jaw. Just notice, and breathe, and let it be there. Let whatever is there, be there. And then bring your attention again back down to the feeling of centeredness. Relaxing, staying present and alert, feeling the safety, the connection in that place. Now let yourself stay connected to this place, but see if you can cast what we might call a sidelong glance at the difficult area in your body. Is it possible to still feel connected to you body in the area that feels good, and yet know there\u0026rsquo;s something going on that feels unpleasant, and just let it be there? Keeping maybe 75% of your attention on the part that feels peaceful and at ease, still breathing, casting the sidelong glance at this difficult area, noticing what happens to it. Is it growing or shrinking? Is it changing, shifting into something else? Become aware of whatever it is it\u0026rsquo;s doing, relaxing, breathing. Using Mindfulness to Break Habits # Develop greater awareness of how you are feeling immediately before you engage in the problematic behavior Develop greater awareness of how you feel during the activity Devise an alternative behavior Note how you feel when you utilize the replacement activity instead (kind of self-conditioning) Be honest if you are unable to implement the replacement activity\u0026ndash; simply note how you feel as you continue to engage in the old habit and in particular, how you feel when you finally stop The end sucks, and your body will dwell on this emotion. Thus, that will reduce the odds or your engaging in the behavior again in the future The promise of happiness from cravings often misleads, so one strategy for overcoming addictions is to mindfully focus attention on the actual experience when indulging a craving or temptation, so as to compare it with the expectation of reward that preceded it. – Kelly McGonigal\nEx: Binging on hot cheetos and YouTube videos Become more aware of how you feel physically and emotionally as you sit there munching, watching video after video Perhaps you are feeling tired or hopeless Once you recognize that, you can see if you can devise some other activity that will more effectively alleviate the tiredness or sense of hopelessness, e.g, going to bed or talking to a friend Again, note that this is simply data collection It’s not about beating yourself up for being “bad” or “lacking in discipline.” Just be fully aware of what you are experiencing Also, become more aware of exactly how you feel when you finally do turn off the computer and consciously note those feelings This will increase your chances of being able to prevent a repetition of the behavior in the long run Mindfulness might also allow you to notice things that help stop the behavior, e.g., only taking small portions so that you would need to get up to obtain more hot cheetos Guided Meditations # There are many websites and apps now with guided meditations Some of these have practices that are empirically validated and others don’t Here are a couple of websites with evidence-based practices: UCLA Mindful Awareness Research Center (MARC) Ronald Siegel, assistant clinical professor of psychology at Harvard Medical School Acceptance and transformation # Making peace with where we are Feeling and accepting our negative emotions allows us to transform those emotions On the other hand, ignoring, repressing, denying, or even trying to change negative emotions ties up our energetic resources and actually makes real change much more difficult “What you resist, persists”\nThis is the underlying premise of mindfulness practice and, as discussed earlier, it is also supported by neuropsychological research (Lieberman et al., 2007; Kircanski et al., 2012) But if I accept myself fully, won’t that undermine my motivation to try hard and improve? Actually, it’s just the opposite: beating up on yourself results in a less effectual response: If an experience is painful, we tend to avoid looking at it, so we don’t learn from it The attempt to suppress the negative thoughts and emotions saps our energetic resources Feeling bad about ourselves causes us to engage in more unhealthy habits Self-compassion study (Breines \u0026amp; Chen, 2012) # Participants were told that the purpose of the study was to understand the relationship between test performance and personality and asked to identify their biggest weakness Then they took a difficult test (a 10-item version of the GRE antonyms test) Afterwards, they were given an opportunity to study a list of words and definitions that would be on a subsequent 10-item antonyms test for as long as they wanted Self-compassion group: saw an additional statement embedded in the instructions that read, “If you had difficulty with the test you just took, you’re not alone. It’s common for students to have difficulty with tests like this. If you feel bad about how you did, try not to be too hard on yourself.” Self-esteem control condition saw an additional statement that read, “If you had difficulty with the test you just took, try not to feel bad about yourself—you must be intelligent if you got into Berkeley.” Results: No one guessed the hypothesis that viewing test failure in a certain way might affect study time or effort Self-compassion increased study time, which in turn predicted higher test scores, though it did not directly lead to improved performance Open, Nonjudgmental Awareness # Mindfulness practice is about developing greater awareness Taken to higher levels, this can produce states of great bliss If we fully connect with the “suchness” of things in our body-energy-mind, we find a ground which holds everything and gives rise to a sensation of great bliss in the center of the body This takes the truism that true happiness must come from within to a whole new level! The experience of bliss in turn makes it possible to connect even more fully with the suchness of things, to be even more inclusive\u0026hellip; The above is accompanied by a transformation of perception The big secret is that enlightenment is right here right now You see that everything is perfect just as it is You don’t need to change anything bad into anything good or to “fix” yourself in any way \u0026ndash; wabi-sabi Ironically, this actually makes it easier to effect changes because your energy is no longer all tied up in beating yourself up Obstacles (Optional) # Mental elaborations stand in stark contrast to the kind of open, nonjudgmental, direct awareness described above As Rick Hanson points out, most of the negative emotions we feel do not come from actual aversive events, but from our reactions to them Suppose you’re walking through a dark room at night and stub your toe on a chair Right after the first stab of pain comes “Who moved that darn chair?!” This then continues to elaborate, maybe moving to the person whom you think moved the chair and all the evil things they have done to you In addition, attachment to/obsession with particular outcomes prevent us from experiencing the kind of open, nonjudgmental awareness associated with deep meditative states Applying Mindfulness to Daily Life # Mindfulness practice is not just about sitting on a meditation cushion; it’s about a way of being that extends to how we engage in all of our daily activities It’s knowing that you are exactly where you should be this moment and appreciating this moment, engaging in activities with full focus of attention Ex: Walking meditation is about really enjoying the walking \u0026ndash; walking not in order to arrive, but just to enjoy each step We shake off all worries and anxieties, not thinking of the future, not thinking of the past, but just enjoying the present moment We are aware of the contact between our feet and the Earth “Walking as if we are kissing the Earth with our feet” \u0026ndash; Thich Nhat Hanh\nSimilarly, if we try to rush through washing the dishes in order to get to dessert, it will become an unpleasant task. And we will be equally incapable of enjoying our dessert\u0026hellip; Focusing on the future becomes a habit \u0026ndash; you cannot just selectively focus on pleasant tasks “With the fork in my hand, I will be thinking about what to do next, and the texture and the flavor of the dessert, together with the pleasure of eating it, will be lost. I will always be dragged into the future, never able to live in the present moment.” \u0026ndash; Thich Nhat Hanh\nIf you ask a Zen teacher how to find enlightenment, he might answer, “Have you eaten? Then wash your bowls.” If you cannot find the meaning of life in an act as simple as that of doing the dishes, you will not find it anywhere Instructions: Mindful Eating # Sit comfortably in a chair. Place a raisin in your hand. Examine the raisin as if you had never seen it before. Imagine it as its \u0026ldquo;plump self\u0026rdquo; growing on the vine surrounded by nature. As you look at the raisin, become conscious of what you see: the shape, texture, color, size. Is it hard or soft? Notice any thoughts that arise. Bring the raisin to your nose and smell it. Are you anticipating eating the raisin? Is it difficult not to just pop it in your mouth? How does the raisin feel? How small it is in your hand? Place the raisin in your mouth. Become aware of what your tongue is doing. Bite ever so lightly into the raisin. Feel its squishiness. Chew three times and then stop. Describe the flavor of the raisin. What is the texture? As you complete chewing, swallow the raisin. Sit quietly, breathing, aware of what you are sensing. Then repeat with other raisins. Clarifications # Mindfulness doesn’t just mean “noticing things” It’s about being present with experience in a way that’s much more vivid, immediate, and real Even “negative emotions” are perceived as juicy experiences that are “OK” The world will seem to light up like a lightbulb Awe # Asked what Zen training leads to, a Western student in Japan answered “No paranormal experiences that I can detect. But you wake up in the morning and the world seems so beautiful you can hardly stand it.”\nResearch has indicated that the emotion that confers the greatest health benefits may be awe (Stellar, John-Henderson, Anderson et al, 2015) Participants who scored high on awe had the lowest levels of interleukin-6, which is tied to inflammation (and thus mortality) Instructions: Mountain Meditation # Find a time when you can sit for half an hour without interruption. Assume a comfortable erect position with back erect, chin slightly tucked in towards the chest, both feet flat on the floor (if sitting in chair), and hands quietly resting on thighs. Tense and release the muscles from your toes to your head. Observe your breath and remain in tune with feeling the air pass in and out of your nostrils with each inhalation and exhalation as you continue the meditation. Mindfulness-Based Stress Reduction (MBSR) programs incorporate a number of mindfulnessrelated practices, such as Mountain Meditation and Lovingkindness Meditation Visualize a mountain and then become that mountain Imagine yourself as being that majestic mountain with your summit in the clouds. Imagine how solid and strong and how connected to the earth you are, for you, the mountain, have stood for thousands of years. Breathing in, I see myself as a mountain Breathing out, I feel solid and strong.\nThe weather has always been in a state of flux around you. The views change from blue sky views with gentle breezes and showers to mighty banks of storm clouds, dispensing heavy downpours, to sleet and snow. Yet, you have stood firm and immovable, and the winds of change have whirled for centuries around you without any noticeable effects. Breathing in makes me calm. Breathing out helps me settle.\nJust as changes in weather whirling about outside of you, the mountain, provoke no angst, the emotions, activities and situations that whirl around you in your everyday life shall not disturb you when your meditation ends. You shall remain tall and strong and connected. Breathing in, I feel secure. Breathing out, I feel grounded.\nYou shall remain upright, firmly grounded and connected to the earth, regardless of the weather whirling around you. So now just sit and continue to follow your breath as you sink deeply into your majestic mountain base without collapsing your spine. Become one with the feelings of solidity, strength and connection. Breathing in, I feel still and connected. Breathing out, I reflect things as they are.\nEnd your meditation when you feel it is time to do so. Lovingkindness Practice # Instructions # Preliminary practice # Start by giving loving-kindness to yourself because without loving yourself, it is almost impossible to love others. If you are an empty cup, you have nothing to give. May I be happy May I abide in well-being May I be secure May I dwell in safety Practice this regularly for some days to establish a strong sense of loving-kindness for yourself. Main practice # Visualize a love that someone gave you that really moved you, perhaps in your childhood. Remember a particular instance when they really showed you love, and you felt their love vividly. Now let that feeling arise again in your heart and infuse you with gratitude. As you do so, your love will go out naturally to that person who evoked it. You will remember then that even though you may not always feel that you have been loved enough, you were loved genuinely once. Knowing that now will make you feel again that you are, as that person made you feel then, worthy of love and really lovable. Let your heart open now, and let love flow from it; then extend this love to all beings. Begin with those who are closest to you, then extend your love to friends and to acquaintances, then to neighbors, to strangers, then even to those whom you don’t like or have difficulties with, even those whom you might consider as your “enemies,” and finally to the whole universe. Let this love become more and more boundless. (From The Tibetan Book of Living and Dying) Research on Compassion Practice # Study on compassion meditation in long-term Tibetan meditation practitioners who had had logged in 10,000-50,000 hours of practice (Lutz, Greischar, \u0026amp; Rawlings, 2004) Meditators were asked to engage in compassion meditation during EEG study Compassion meditation does not focus on particular objects, memories, or images; rather the emphasis is on generating feelings of benevolence and compassion, causing them to “pervade the mind as a way of being” Controls were undergraduates who had been given a crash course in compassion meditation and had practiced for an hour Results: Long-term practitioners produced showed high levels of activity in gamma-band frequencies (25-42+ Hz) and increased neural synchrony Larger the waves, the more in sync your brain is (this naturally decreases with age) This involves large regions of the brain pulsing in synchrony 30-80 times a second As they went deeper into meditation (jhana states), there appeared to be both a spreading and a strengthening of gamma wave activity When controls engaged in compassion meditation, they also showed an increase in gamma activity, but the increase was slight The color scale indicates the percentage of subjects in each group that had an increase of gamma activity during the mental training: (Left) Controls; (Right) Practitioners Correlation between the length of the long-term practitioners\u0026rsquo; meditation training and the ratio of relative gamma activity averaged across electrodes in the initial baseline condition Gamma waves and neural synchrony # Gamma waves Type of very high-frequency brain wave Size of the gamma wave is related to the number of neurons firing in sync Research has linked neural synchrony of high-frequency brain waves to enhanced attention, working memory, learning and conscious perception Greater synchrony between various sections of the brain indicates greater integration of cognitive and affective functions and less dissociation Compartmentalization of brain functions is associated with aging and cognitive decline What the meditation practitioners themselves reported experiencing during this state: A change in the quality of moment-to-moment awareness, bringing with it a vast panorama of perceptual clarity “It is as if a mental fog lifts, one that you did not realize had been impeding your perception” (Davidson)\nMonks who had spent the most years meditating generated the highest levels of gamma waves Increased gamma activity and neural synchrony were evident in the long-term practitioners even when they were not meditating Gamma Wave \u0026amp; Cognitive Functioning # MIT neuroscientists found that exposing mice to strobe lights and clicking sounds at frequencies that stimulate gamma waves reduced levels of beta-amyloid associated with Alzheimer’s and improved cognitive function (Martorell, Paulson, Suk et al., 2019) Study 1: Mice were engineered to exhibit Alzheimer’s-like qualities Exposed to clicking sounds at 40 Hz for an hour a day for a week Results: Induced synchronized gamma-wave oscillations in the brain Gamma waves are involved in concentration, sleep, perception, and movement, and are disrupted in patients with Alzheimer’s Reduced levels of beta-amyloid and tau-proteins in the auditory cortex and nearby hippocampus Increased activation of microglia, which is important in clearing harmful debris, as well as improved functioning of blood vessels Mice performed better on memory tasks, including recognizing objects and navigating a water maze to find a hidden platform Results: Increased gamma brain waves in the visual cortex, hippocampus, and prefrontal cortex Reduced neuronal and synaptic loss in these brain regions Reduced inflammation Improved performance on memory tasks Findings point to an overall neuroprotective effect, even in the later stages of neurodegeneration New clinical trials starting using human participants Study 2: Mice were exposed to a combination of light and sound stimulation (Martorell, Paulson, Suk et al., 2019) Results: Expanded effects to prefrontal cortex Resulted in clustering of microglia around amyloid deposits and reduced amyloid pathology Effects were short-lived, however, diminishing a week after stimulation Longer-term follow-up study on mice with more advanced Alzheimer’s disease Mice given 6 weeks of gamma entrainment using strobe lights (Adaikkon, Middleton, Marco et al., 2019) Optimism and prefrontal dominance # EEG studies by Richard Davidson found that meditation practice is associated with increased left prefrontal activity (Davidson, 2012)\nAs mentioned earlier, left prefrontal cortex brain activity is known to be associated with positive outlook and feelings of happiness and well-being (Davidson, 2012) Meditation, Prefrontal Dominance \u0026amp; Optimism Early on, Davidson had noticed that an elderly Tibetan monk in one of his studies showed much greater predominance of activity in the left prefrontal than any of the other people previously tested Research on other long-term meditators provided further confirmatory evidence For instance, one meditation adept, Matthieu Ricard, showed increased left pre-frontal cortical activity that was 4.5 standard deviations outside the standard bell curve Left Right Brain Dancer # An early study found that less extensive meditation practice (40 minutes a day for 8-10 weeks) was also associated with a significant shift in hemispheric dominance In addition, degree of shift in activity from right to left prefrontal was found to correlate with enhancement in immune system (resistance to flu virus) (Davidson, Kabat-Zinn, Schiumacher et al., 2003) Empathy and ability to identify microexpressions # Neurological Effects of Mindfulness # MRI study on Western lay practitioners who incorporated meditation practice into their daily lives (Lazar, Kerr, \u0026amp; Wasserman, 2005) Meditators averaged 6.2 hours of practice a week for 9.1 years Compared to control participants, showed thickening in parts of prefrontal cortex and the right anterior insula These regions of the brain are involved in attention, sensory processing, and empathy Cortical growth was not due to the growth of new neurons, but resulted from Wider blood vessels More supporting structures such as glia and astrocytes Increased branching and connections Between-group differences in prefrontal cortical thickness were most pronounced in older participants, suggesting that meditation might be particularly important in preserving cognitive functions as people age VBM study on long-term meditators found similar effects Lay practitioners who had practiced meditation for 10-90 min daily for an average of 24 years Meditation was associated with increased gray matter volume in areas important in emotional regulation and memory, including Orbitofrontal cortex Hippocampus Meditation increases density of gray matter in frontal and temporal in much the same way that physical exercise increases the size of muscles (Luders, Toga, Lepore et al., 2009) Other Cognitive \u0026amp; Affective Benefits # Paul Ekman found enhanced ability to identify microexpressions in meditators A series of faces displaying various expressions was shown in very quick succession The target expression remained onscreen for one thirtieth of a second Participants were asked to identify that expression The two experienced Western meditators whom Ekman tested achieved results that were far better than those of 5000 participants previously tested The ability to recognize such fleeting facial expressions has been associated with a capacity for empathy and insight, as well as openness to new experiences, intellectual curiosity, and general reliability and efficiency “They do better than police men, lawyers, psychiatrists, customs officials, judges - even secret service agents” \u0026ndash; the group that had hitherto proven to be the most accurate, according to Ekman\nPsychological effects of mindfulness # Treatment of Psychological Disorders # Research has indicated that mindfulness practices are useful in the treatment of a wide array of psychological disorders, including: Anxiety and depression (Hofmann, Sawyer, Witt, \u0026amp; Oh, 2010) Substance abuse (Melemis, 2008) Eating disorders (Kristeller \u0026amp; Hallett, 1999) Stress (Grossman, Niemann, Schmidt \u0026amp; Walach, 2004) Effects of meditation on these psychological, as well as physical, conditions are probably mediated at least in part by reduction in cortisol levels Research by Herbert Benson in the 1970s (primarily on transcendental meditation) found that meditation is associated with “a wakeful, hypometabolic state of parasympathetic activity” (fancy way of saying relaxed) Research # Research on use of mindfulness in treating psychological disorders in children and adolescents: 14-18 year olds who took an Mindfulness-based stress reduction (MBSR) class reported a decrease in anxiety, depression, and somatic complaints, as well as an increase in sleep quality and self-esteem, compared with controls (Biegal, Brown, Shapiro, \u0026amp; Schubert, 2009) Meta-analysis of 15 studies on children and adolescents found that mindfulness was effective in treating anxiety disorders, ADHD, substance abuse, sleep disorders, and conduct disorder (Burke, 2010) More recent reviews have found that meditation enhances ability to regulate emotions and attention in children (Meeiklejohn, Phillips, Freedman et al., 2012; Cairncross \u0026amp; Miller, 2016) Mindfulness and Subjective Well-being # Research on mindfulness practice and subjective well-being (self-reported happiness) Brown (2009) found that a large discrepancy between financial desires and financial reality was correlated with low subjective well-being but that the accumulation of wealth did not tend to close the gap Mindfulness practice however was associated with a lower financial-desire discrepancy and thus higher subjective well-being Mindfulness may promote the perception of “having enough” Optional # 2014 meta-analysis published in JAMA Internal Medicine called into question the effectiveness of mindfulness training programs in improving mental health and reducing stress-related behavior The meta-analysis examined 47 randomized controlled trials of mindfulness meditation programs, which included a total of 3,515 participants Studies were primarily 8-week-long mindfulness training programs that used psychological and behavioral assessments, rather than neuroimaging Along with mindfulness, meta-analysis included meditations that emphasized use of a mantra Mantra: repetition of a word or phrase in such a way that it helps one transcend to an effortless state where focused attention is absent (Goyal, Singh, Sibinga et al., 2014) Results: Meditation programs resulted in Only moderate reductions in anxiety, depression, pain, and stress/distress These small effects were comparable with what would be expected from the use of antidepressants but without the associated toxicities Problems identified in review: Use of outcome measures that can be easily biased by participants’ beliefs in the benefits of meditation Control participants that received less time and attention from the teacher or the group than those in meditation program Very few mantra meditation programs met inclusion criteria Reviewers pointed out that effectiveness of programs may depend in part on Type of meditation practice Amount of training Use and qualifications of instructor Degree of emphasis on religion or spirituality Whether program integrated dietary regimens and/or movement exercises (e.g., yoga) Most forms present meditation as a skill that requires expert instruction and time dedicated to practice Research Issues # The modest psychological effects found in studies of short-term mindfulness contrasts with the much larger effects of neuroimaging and reaction time studies on long-term meditation practitioners; This suggests that Results of meditation studies depend in part on amount of practice and amount of training/teaching received by practitioner Changes in brain function and structure may precede noticeable psychological and behavioral changes Even novice meditators showed increase in gamma waves during compassion meditation but effects endured past period of meditation only in long-term practitioners (Lutz, Greischar, \u0026amp; Rawlings, 2004) One problem though that pertains even to the well-controlled studies with long-term meditators (e.g., showing increased left prefrontal activity in meditators and enhanced ability to detect microexpressions) is that it’s not clear precisely which meditation practice(s) may be contributing to those effects Some recent studies try to address this problem by asking participants to engage in specific types of meditation practice while in the scanner The problem of multiple practices is compounded by the fact that many of the long-term meditators practice within traditions that require extensive preliminary training involving Working with the mind, e.g., developing greater awareness of the feeling tone of thoughts Ethical precepts: just focusing on doing what you know at a deep level to be right is another doorway to great meditation experience See the Five precepts. “Right is right were wisdom in the scorn of consequence” \u0026ndash; Tennyson\nPhysical Effects of Mindfulness # Mortality Rates # Experimental study in which patients with mild hypertension were trained in meditation and followed for 19 years Meditation group showed a 23% decrease in overall mortality, a 30% decrease in rate of cardiovascular mortality and a 49% decrease in the rate of mortality due to cancer compared with controls (Schneider, Alexander, Staggers, et al., 2005) 73 residents of homes for elderly were assigned either to daily transcendental meditation (TM or mindfulness), a relaxation group, or a no-treatment group After three years, survival rate was 100% for TM, 87% for mindfulness, 65% for relaxation, and 77% for no-treatment group Only differences between meditation and nonmeditation groups were significant In general, there are more similarities than differences between psychological effects of different types of meditation (Alexander, Langer, Newman et al., 1989) Gene Expression # Just one day (8 hours) of intensive practice of mindfulness meditation resulted in significant modulation of expression of proinflammatory genes (Kaliman, Alvarez-Lopez, Cosin-Tomas et al., 2014) Mindfulness and Physical Pain # A large component of “physical pain” is actually mental The mind reacts to pain with fear, rejection, despondency, or a feeling of powerlessness, dramatically compounding the pain These are the mental elaborations mentioned earlier “This pain means that my body is suffering injury and I’m going to DIE!!” Pain is actually just sensations \u0026ndash; it’s the aversion response that causes most of the suffering Sort of like little kid screaming about getting shots, when the punches they get roughhousing on the playground are actually many times more painful As you focus on your bodily sensations, you begin to realize that what you thought was pain is just a cluster of sensations The mind learns to recognize those sensations simply as sensations (“Oh, that’s my feet tingling or my knees burning”) Rather than thinking about the pain, how to get it to stop, what to do about it, etc., over time, one is able to just be with the sensations and not try to fix it Research # Research on neurophysiological response to pain in meditators vs. non-meditators (Grant, Courtemanche, Rainville et al., 2011; Lutz, McFarlin, Perlman et al., 2013) Used hot laser to create pain in the foot/arm Results: In comparison with the non-meditators, the Zen practitioners Showed significantly greater activity in the somatosensory cortex, as well as in the insula, the part of the brain involved in proprioception (noticing body sensations) Reported that the pain sensations were very, very vivid Showed significantly less activity in parts of the prefrontal which are involved in evaluating the pain Rated the pain very low, as a 1, 2, or 3, as opposed to the non-meditators who rated their pain as a 8, 9, or 10 Meditators with the most experience showed the largest reductions in prefrontal and amygdala activation In addition, the lower pain sensitivity in meditators was strongly predicted by reductions in functional connectivity between executive (prefrontal) and pain-related cortices Results suggest a functional decoupling of cognitive-evaluative and sensory-discriminative dimensions of pain, allowing practitioners to view painful stimuli more neutrally Consciousness, Mindfulness \u0026amp; the Mind-Body Connection # In our culture, we normally think that: Our mind is identical with our body OR Our mind is in our body However, the meditation traditions from which mindfulness arises hold that Our body is actually in our mind … and that we are a lot more than we think The small sense of self with which we normally identify is an arbitrary construct based on sensory feedback mechanisms Ramachandran demo Elisabeth Haich’s experience: Asked to sit under a palm tree and meditate on it week after week Three stages of concentration: intellectual, emotional, spiritual First stage: you think about what this object actually is Second stage: “with every nerve and every drop of your blood,” you feel the object of concentration and what it is like Third stage: you become identical with the object of concentration … all of a sudden I have the odd feeling that I am no longer looking at the tree from the outside, but from the inside. To be sure I still perceive its outward form with my eyes, but I begin, to an everincreasing extent, to see and experience the inner being, the animating creative principle of the palm … to see it, to experience it, TO BE IT!\nAnd finally there comes a moment when I am suddenly conscious of the fact that the palm is no longer outside myself \u0026ndash; no! \u0026ndash; it never was outside \u0026ndash; it was only a false conception on my part \u0026ndash; the palm tree is in me and I in it \u0026ndash; I myself am the palm tree!\n"},{"id":22,"href":"/e-29/7/","title":"7-9: Visualization","section":"Engineering 29","content":" Orthographic projections # Formal (working) drawings: purpose # Need a formal way of documenting designs Legal documents i.e patents; contracts may rely on them Must stand on their own \u0026ndash; readable to any human No subsequent explanation No verbal assists No ambiguity Solution: multi-view orthographic projection World-wide engineering standard Can easily include tolerances What is a projection? # Projection of a 3D object’s edge onto a 2D plane by rays perpendicular to that plane such that they are parallel to one another (unlike real-world) Dashed lines represent hidden detials Projections are independent of projection distance Projection depends on part orientation # Use judgement to select most useful/informative orientation Often, a projection is clearest when a significant flat surface of the object is parallel to the projection plane Left is a better pictorial view (it\u0026rsquo;s more 3D) Right is better because it\u0026rsquo;s face is parallel Multi-view orthographic projection # Usually can’t convey all information about an object using a single projection Use multiple projections from different viewpoints What is an orthographic projection? # Orthos: Greek for \u0026ldquo;right\u0026rdquo;, \u0026ldquo;true\u0026rdquo;, or \u0026ldquo;correct\u0026rdquo; Each projection is formed by rays perpendicular (at right angles) to its projection plane The different views of a multi-view drawing are taken from viewpoints at right angles to each other Multi-view orthographic projection is a standardized, accepted form of representing objects Graphos: drawing \u0026ldquo;Glass-box\u0026rdquo; interpretation # Need an agreed way to organize different projections on the page Imagine projecting object onto sides of a box Unfold the box onto the page So-called \u0026ldquo;third-angle\u0026rdquo; projection Will not necessarily show all six projections Projections are aligned Example of multi-view orthographic projection # Lines connecting a given point in adjacent views are always perpendicular to the \u0026ldquo;unfolding\u0026rdquo; line (of the \u0026ldquo;glass box\u0026rdquo;) View interpretation # Need to consider how many views needed to remove ambiguity Multiview characteristics # Inclined face Face in 2 views Line in 3rd view Oblique face Face in 3 views Which is the oblique face? # How many views? # Example: cut from sheet material Unlikely to be multiple levels of relief Projections of edges therefore unnecessary Grooves or etched patterns would be labeled as such In this case, two views not enough to describe geometry completely \u0026ndash; we need 3\nThick enough material that there could conceivably be multiple levels of relief: side view needed In this particular example, all features pass through the full thickness of the material Would any two views be enough to describe the geometry completely?\nExample of hidden lines # First- vs third-angle orthographic projection # 3rd angle projection used in U.S. glass box convention 1st angle projection (Europe, Japan, India) top/bottom and left/right arrangement reversed Example of why specifying 1st or 3rd angle matters ANSI standards (Y14.5) # Adopted by drafters and engineers to expedite the transfer of information Maximum information with the minimum drawing Will only cover highlights here Views At least two views (except flat sheet) Add views as required so the dimensions of the object can be defined entirely in true length measurements Add views as necessary for presentation clarity Solid lines Assumed to be intersections of planes or optical limits of cylinders Tangent edges are usually not shown, or shown using phantom lines Hidden lines Use to add information, clarity (good practice not to over-use) Use views requiring the fewest hidden lines Center lines Use to mark the centers of holes, or cylindrical surfaces ≥180º Circles Assumed to be intersections of cylinders and orthogonal planes Section views Used for clarification of internal geometries Explained in a later lecture Small cuts on curved surfaces\nSmall radii, intersections of blended planar surfaces shown as a line\nRepresenting threads use schematic representations\nParts with odd rotational symmetry Simplify to a symmetrical view even though that is not a strictly accurate projection\nTangent and non-tangent surfaces\nA line drawn where a curved surface meets a planar surface indicates no tangency: i.e. there is an abrupt change in the angle of the surface No drawn line indicates tangency: i.e.surface angle is continuous/smooth Pictorial views # Review of isometric, oblique, and perspective # Color; shading # Section views # Advanced projections # Auxiliary views # Additional notation # Dimensioning # Showing welds # "},{"id":23,"href":"/eecs-16a/8/","title":"8: Capacitors \u0026 Capacitive Touchscreen","section":"EECS 16A","content":" \\(\\) 03/15 Capacitors # Structure and Physics # See also, 7B 24 Capacitors is a circuit element that stores charge Positive charges build up on the (top) surface of the plate connected to the positive terminal and negative charges build up on the (bottom) surface of the plate connected to the negative terminal. Capacitors have an associated quantity, $C$apacitance (farads); the ratio of the amount of electric charge $Q$ (coulombs) stored on a conductor to a difference in electric potential $V$ Units: Farads, $F = \\frac{C}{V}$. Depends on the physical geometry of a capacitor $$C = \\varepsilon_0 \\frac{A}{d}$$ $A$: Area of plates $d$: Distance between plates $\\varepsilon_0$: Permittivity of free space, $8.854\\cdot 10^{-12} \\frac{\\text{F}}{\\text{m}}$ A physical diagram of a capacitor, with 2 metal plates and a dielectric (commonly air) between them\nFor most purposes, capacitors do not have polarity\u0026ndash; their orientation doesn’t impact their behavior. The plate that corresponds to the \u0026ldquo;+\u0026rdquo; terminal, stores $+Q = +CV$ and the plate that corresponds to the \u0026ldquo;-\u0026rdquo; terminal stores $−Q = −CV$ The amount of charge $Q$ on a capacitor is related to its geometrical structure (capacitance $C$) and the voltage applied to it $V$: $$Q=CV$$ When we apply a voltage across the conductive plates, we create a potential difference, and so charges will build up on the plates They will not build up indefinitely because this potential difference is not infinitely strong At some point, an additional positive charge will be ambivalent about joining the plate; while there is a potential difference $V_+ − V_−$ between the plates attracting the charge, there are also repulsive forces from the charges that already exist on the plate After this amount of critical charge forms on the plate, no additional charge will enter Geometry The more the $A$rea, the more the total charge that can fit on the plate because the individual charges can spread out more, decreasing the repulsive forces Smaller the $d$istance between the plates, the more strongly the $+$ and $−$ charges attract each other That is, for the same voltage, decreasing the distance of plate separation increases the charge that will build up on the plate Energy Storage # When we apply a potential difference, charges build up on the plates; these charges are the reason that a capacitor stores energy. The repulsion between charges on a given plate means that moving a charge onto the plate takes energy (supplied by the voltage source). The more charge already on a plate, the more energy it takes to push another charge on.\n$$V = \\frac{dE}{dQ}$$ $$dE = V dQ$$ $$dE = V \\cdot d (CV)$$ $$\\int dE = C \\int_0^V V\\ dV$$ $$ \\Longrightarrow E = \\frac{1}{2}CV^2$$ This is the energy of a capacitor when it is fully charged, holding the complete $Q$ determined by $C$ and $V$. $I$-$V$ Relationship and Behavior # Current # $$Q = CV$$ $$\\frac{dQ}{dt} = \\frac{dCV}{dt}$$ $$ \\Longrightarrow I = C \\frac{dV}{dt}$$\nCurrent is only flowing through the capacitor if the voltage across the capacitor is changing with time If the voltage is no longer changing, then the current through the capacitor will equal 0. That is, in steady state (after the current has been running for a very long time), direct current (DC) capacitors act as open circuits If the voltage across it is constant, then the plates are already full of charge for that voltage That is, any additional charge will feel the repulsive forces of the existing charges and will not want to enter the plate. Note that the negative charges build up on the bottom plate; this happens because the positive charges on the top plate push the positive charges away from the bottom plate. This means that in reality, the charges entering the top plate are not the same exact physical charges that exit the bottom plate, but it doesn’t matter! From the perspective of the other circuit elements (and for our purposes), current flows just the same\nVoltage # $$I = C \\frac{dV}{dt}$$ $$I\\ dt = C\\ dV$$ $$\\int I\\ dt = \\int_0^V C\\ dV$$ $$It = C(V(t) - V(0))$$ $$V(t) = \\frac{I}{C}t + V(0)$$ To integrate the left-hand side, we assume that the current $I$ is constant wrt time. Network Simplifications # Full derivations start page 5 Note We can only simplify (that is, find $C_{eq}$) iff we know the starting states of each capacitor Parallel # Parallel capacitors share terminal nodes, thus the voltage across capacitors is equal at steady state. Therefore, their capacitances add up: $$C_{eq} = \\sum_i C_i = C_1 + \\dots + C_n$$ Intuitively, charge is apportioned among them by size. Series # Series capacitors have the same current through them by KCL The entire series acts as a capacitor smaller than any of its components: $$\\frac{1}{C_{eq}} = \\sum_i \\frac{1}{C_i} = \\frac{1}{C_1} + \\dots + \\frac{1}{C_n}$$ Intuitively, the separation distance, rather than plate area, adds up The inner middle two plates carry equal and opposite amounts of charge, so their net contribution to charge storage is $0$ If all initially uncharged, the charge across all are equal to each other at steady state Note: The parallel operator $\\parallel$ is just a mathematical tool; it happened to describe the equivalence for resistors in parallel but the operator actually applies to capacitors in series. As with resistors in parallel, capacitors in series are best simplified pairwise; that is, for a system with capacitors in series can be simplified as $C_{eq} = C_1 \\parallel C_2 = \\frac{C_1 C_2}{C_1 + C_2}$\n03/18 Capacitive Touchscreen # When there is touch, we form a capacitor It is worth noting that there is capacitance everywhere because everything is a conductor (to some extent). There is capacitance between your fingers and your laptop keys, your fingers and our phone, and (to a much lesser extent) you and Pluto!\nMeasuring $V$ or $I$ # When we have no touch, we have an open circuit so we only have $C_0$ When we do touch, we close the circuit so $C_1$ and $C_2$ both begin charging: $$C_0 + C_1 \\parallel C_2$$ $$C_0 + \\frac{C_1 C_2}{C_1 + C_2}$$ $$C_0 + \\Delta C$$ $$C_0 + C_{eq}$$ DC # Assuming $V_{out}(0) = 0$ $$I_S = C_{eq} \\frac{d V_{out}(t)}{dt}$$ $$V_{out} = \\int_0^t \\frac{I_S}{C_{eq}}dt$$ $$\\dots = \\frac{I_S t }{C_{eq}}$$ $$ \\Longrightarrow C_{eq} = \\frac{I_S t}{V_{out}} $$ Downside:\n$V_C$ will grow to infinity Can\u0026rsquo;t to build a current source easily AC # If we replace our regular direct current (DC) source with an alternating current (AC) source then we don\u0026rsquo;t have to worry about $V_C \\to \\infty$ $$V_C(t) = \\begin{cases} \\frac{I}{C}t \u0026amp; t \\in \\gamma [0, \\frac{T}{2}) \\\\ -\\frac{I}{C}(t- \\frac{T}{2}) + \\frac{I}{C} \\frac{T}{2} \u0026amp; t \\in \\gamma [\\frac{T}{2}, T) \\\\ \\end{cases}; \\gamma \\in \\mathbb{Z^+}$$ Measuring $\\Delta C$ # We can’t measure capacitance directly, but if we can transform capacitance into a voltage, we can measure that.\nAttempt 1 # Doesn\u0026rsquo;t work: $V_{out} = V_s$ regardless of if there is a touch or not Attempt 2 # Phase 1 ($\\phi_1$): Close $s_1$, open $s_2$ $$Q_{eq} = C_{eq} V_S $$ Phase 2 ($\\phi_2$): Close $s_2$, open $s_1$\nResults in Charge sharing causing unknown/ambiguous initial conditions Attempt 3 # Phase 1 ($\\phi_1$): Close $s_1, s_3$, open $s_2$\n$C_{eq}$ charges: $Q_{eq} = C_{eq} V_S$ $C_{ref}$ discharges: $Q_{ref} = C_{ref} V_{out} = 0$ $$Q_{\\phi 1} = C_{eq} \\cdot V_S$$ By conservation of charge $$Q_{\\phi 1} = Q_{\\phi 2}$$ $$C_{eq} \\cdot V_S = C_{eq} \\cdot V_{out} + C_{ref} \\cdot V_{out}$$ $$ \\therefore V_{out} = \\frac{C_{eq}}{C_{eq} + C_{ref}} V_S$$ therefore, when touching we change voltage!\nPhase 2 ($\\phi_2$): Close $s_2$, open $s_1, s_3$\n$V_{C_{eq}} = V_{out}$ $V_{C_{ref}} = V_{out}$ $$Q_{\\phi 2} = C_{eq} \\cdot V_{out} + C_{ref} \\cdot V_{out}$$\n"},{"id":24,"href":"/cogsci-c100/mem-process/","title":"8: Memory Process","section":"CogSci C100","content":" Three-Stage Model of Memory # Three-stage modal model of memory; Atkinson-Shiffrin model: Sensory memory Working memory or short-term memory Long-term memory Old model that\u0026rsquo;s not used today Sensory memory is seen as a type of perception Short term is similar to attention Interesting to study to see how our model evolved and how memories are processed Sensory memory # Holds sensory information very briefly (1/2 to 4 secs) If you stare at an image then close your eyes you can still see the image Large capacity store Sensory input is held very briefly in sensory memory to allow selection and processing of information Now often considered to be a part of perception (a) was flashed, then (b), and people reported seeing (c) [Eriksen \u0026amp; Collins, 1967]\nTwo types of sensory memory:\nIconic memory (photographic memory): visual sensory memory \u0026ndash; doesn\u0026rsquo;t go away after four seconds but functions like sensory Echoic memory: auditory sensory memory Eidetic imagery (photographic memory): # Eidetic memory: Characterized by relatively long-lasting and detailed images of visual scenes that can sometimes be scanned and \u0026ldquo;looked at\u0026rdquo; as if they had real existence\nExamples of people with a photographic-like memory are rare. Eidetic imagery is the ability to remember an image in so much detail, clarity, and accuracy that it is as though the image were still being perceived. It is not perfect, as it is subject to distortions and additions (like episodic memory), and vocalization interferes with the memory\nEidetic imagery is relatively rare \u0026ndash; only 5% or so of tested schoolchildren have it and proportion is much smaller in adults It is not an especially useful form of mental activity. Contrary to popular lore, memory experts don’t generally have eidetic imagery; their skill is in organizing material in memory, rather than in storing it in picture form. Group of schoolchildren was shown a picture for 30 seconds. Picture was taken away, and children asked whether they could still see anything and, if so, to describe what they saw. Some children showed evidence of this kind of memory.\n10-year-old boy, looking at blank easel from which a picture of Alice in Wonderland had just been removed, is asked whether he sees something there (on the blank easel) Participant (P): I see the tree, gray tree with three limbs. I see the cat with stripes around its tail. Experimenter (E): Can you count those stripes? Participant (P): I see the tree, gray tree with three limbs. I see the cat with stripes around its tail. Experimenter (E): Can you count those stripes? P: Yes (pause). There’s about 16. E: You’re counting what? Black, white or both? P: Both. E: Tell me what else you see. P: And I can see the flowers on the bottom. There’s about three stems but you can see two pairs of flowers. One on the right has green leaves, red flower on bottom with yellow on top. And I can see the girl with a green dress. She’s got blond hair and a red hair band and there are some leaves in the upper left-hand corner where the tree is. (Leask, Haber, \u0026amp; Haber, 1969) Short-term memory (STM) / Working Memory # Holds items that are actively being thought about That is, intentionally rehearsing information Has limited capacity 7 +/- 2 items, e.g., letters, words, dots, though this number can vary by task Number of words you can speak in 1.5 seconds (those who speak faster can recall more) Also limited in time, but longer than sensory memory Lasts 5 to 30 seconds Information decays rapidly unless maintained in consciousness through rehearsal Working memory (Baddeley) is now the preferred term because it emphasizes that this is an active, rather than a passive, process Includes a phonological loop (associated with the left hemisphere) that briefly stores sounds, and a visuospatial sketchpad (associated with the right hemisphere) that stores visual and spatial information Just recall left is associated with language and right is generally \u0026lsquo;wholistic\u0026rsquo; Two visuospatial tasks will interfere with each other if performed simultaneously, as will two items in the phonological loop However, people can perform a verbal task and a spatial task simultaneously Aside: Feynman Episodic buffer: another component of working memory that can hold and combine information from phonological loop, visuospatial sketchpad and long-term memory to form a story \u0026ndash; important in time sequencing E.x. if you\u0026rsquo;re trying to recall how someone reacted to a joke you made last night; you think about what you said / how you said it / how they reacted / etc. Central executive: integrates information from phonological loop, visuospatial sketchpad, and episodic buffer Similar to attention/sensory memory in Atkinson Shriffin model Long-term memory (LTM) # Repository of all one’s knowledge Unlimited in the amount it can store No time limit Serial position effect # Evidence for short-term memory and long- term memory: serial position effect Also called primacy-recency effect Six lists test (first five are fruits, last is animals) There is a tendency for people to remember best the items learned first (because of LTM) and the items learned last (because of STM) Proactive interference (PI): words from previous lists interfere with your ability to learn new words of a similar nature (doing progressively worse on Lists 2-5) Release from PI: items are different so no interference memory for these items should be good (doing well on list 6) Retroactive interference: words from later list interfere with recall of a prior list (difficulty recalling items from list 1 at the end) Experiment also shows that semantic similarity of items affects recall VEGT is harder to recall than ANQR Medical applications of serial position effect Our memories of how painful an experience was tends to depend on the peak intensity and how much pain was felt at the end Participants in experiment were asked to immerse one hand in painfully cold ice water for 60 secs, then other hand in same ice water for 60 secs followed by a slightly less painful 30 secs more (Finn, 2011) When asked which trial they would prefer to repeat, most participants preferred the longer trial, with more net pain \u0026ndash; but less pain at the end Strategy of tapering off pain (though this means increasing net pain experienced) has been implemented in painful medical procedures like colonoscopies Serial position can color our memory of pleasures too In one experiment, participants, on receiving a fifth and last piece of chocolate, were told it was their \u0026ldquo;next\u0026rdquo; one Others were told it was their \u0026ldquo;last\u0026rdquo; piece The latter liked the chocolate better and also rated the whole experiment as being more enjoyable (O’Brien \u0026amp; Elsworth, 2012) Possible applications to romantic relationships? Types of long-term memory # Explicit memory # Explicit/declarative memory (with conscious recall): recall or recognition of information; can be verbally transmitted Episodic memory: recall of personal facts (autobiographical) Semantic memory: recall of general facts Implicit/non-declarative memory (without conscious recall): memory that influences one’s behavior or thought but does not itself enter consciousness; cannot be verbally transmitted e.x. you can\u0026rsquo;t teach someone how to ride a bike by explaining to them Procedural memory: Recall of how to do things Also includes conditioning effects \u0026ndash; see Édouard Claparède\u0026rsquo;s experiment Implicit Association Test # implicit.harvard.edu 80% of those who have ever taken the test end up having pro- white associations, including about 50% of African Americans Does the IAT correlate with specific behaviors? White college students who had more implicit negative stereotypes of Black people showed significantly more nonverbal signs of discomfort (e.g., standing further away, making less eye contact, stumbling over words more, smiling less) when interviewed by Black interviewers People with a stronger pro-White bias on the IAT were also more likely to perceive anger and apparent threat in Black faces (Greenwald, Banaji, \u0026amp; Nosek, 2015; Hugenberg \u0026amp; Bodenhausen, 2003) How can implicit stereotyping be reduced? In case of black-white implicit stereotyping, thinking about admired out-group member (Tiger Woods) and disliked in-group member (Jeffrey Dahmer) can reduce implicit IAT bias for up to 24 hours Imagining counter-stereotypic women (e.g., strong women) similarly reduces implicit stereotyping Students enrolled in a prejudice and conflict seminar taught by a black professor show a reduction in implicit biases after participation in course, as compared with controls (Devine, P.G., 2001) (Rudman, Ashmore, \u0026amp; Gary, 2001) Long-term impact The power of implicit attitudes: Newlyweds’ implicit and explicit attitudes toward their new spouses were measured Explicit measure: spouses asked to report extent to which they would describe their marriage using 15 pairs of opposing adjectives (e.g., \u0026ldquo;good\u0026rdquo; vs. \u0026ldquo;bad,\u0026rdquo; \u0026ldquo;satisfied vs. \u0026ldquo;dissatisfied\u0026rdquo;) Implicit measure: version of IAT that required spouse to indicate as quickly as possible the valence of positively and negative valenced words after being exposed to 300-ms primes of photos of their partner and various control individuals Marital satisfaction was tracked over 4 year period Implicit attitudes were a better predictor of future marital happiness than their explicit attitudes (McNulty, Olson, Meltzer et al., 2013) Neurobiology of memory processing # Where in the brain is long-term memory stored? # Karl Lashley (1950) Rats learn maze Lesion cortex (of different location each time) Test memory Still performed better than brand new rat Thus, Memories do not reside in single, specific spots Long-term potentiation (LTP) # Long-term potentiation (LTP): mechanism through which learning occurs in brain A long-term increase in the excitability of a neuron to a particular synaptic input caused by repeated high-frequency activity of that input Stimulating a particular neural circuit will increase the sensitivity of neurons in that circuit, increasing the probability that they will fire again Process involves binding of glutamate to NMDA receptor More glutamate can be bound More synapses can be formed Make NMDA receptor more sensitive to binding What LTP means psychologically: Both positive and negative thoughts tend to be self-reinforcing The more it snows (Tiddely pom),\nThe more it goes (Tiddely pom),\nThe more it goes (Tiddely pom),\nOn snowing. And nobody knows (Tiddely pom),\nHow cold my toes (Tiddely pom),\nHow cold my toes (Tiddely pom),\nAre growing.\n– The House at Pooh Corner\n\u0026ldquo;Passing mental states become lasting neural traits\u0026rdquo; \u0026ndash; Rick Hanson\nThe Hippocampus and Frontal Lobes # Two structures that play particularly important roles in the processing and storing of new explicit memories are Frontal lobes: recalling information and holding it in working memory Hippocampus: \u0026ldquo;save\u0026rdquo; button for explicit memories Used to believe that this is were memories were stored We now understand the hippocampus acts as a \u0026rsquo;loading dock\u0026rsquo; where the brain temporarily holds to-be-remembered information Items then migrate for storage elsewhere in process called memory consolidation Example: Removing a rat’s hippocampus 3 hours after it learns the location of some tasty new food prevents long-term memory formation Removal 48 hours later does not (Tse, Langston, Kakeyama et al., 2007) After a training experience, the greater one’s heart rate efficiency and hippocampus activity during sleep, the better the next day’s memory will be (Peigneux, Laureys, Fuchs et al., 2004; Whitehurst, Cellini, McDevitt et al., 2016) Memory disorders: Evidence for Separate Memory Systems # Dissociation: when brain damage affects two behaviors very differently, this suggests that the two behaviors are produced by different processes Clive Wearing, an English musician, suffered damage to his hippocampus as result of encephalitis If you walk out of the room, then come back 10 minutes late, he won’t remember you But, he can still conduct a choir and play the piano and harpsichord beautifully Due to damage to the hippocampus Ala Memento Explicit Memories: Hippocampus # The hippocampus is central to the formation of explicit memories People with full temporal lobe amnesia (damage to the hippocampus and surrounding areas) cannot form new explicit memories though they can form new implicit memories They often have normal IQs and can carry on a normal conversation but cannot remember anything that happened more than a few minutes previously The posterior hippocampus contains place cells, neurons involved in spatial navigation Volume of posterior hippocampus is positively correlated with amount of time spent as a taxi driver Follow-up study London cabbies have to pass centuries old test, \u0026ldquo; The Knowledge,\u0026rdquo; to get their license They spend 3-4 years memorizing 25,000 streets Voxel-based morphometry (VBM) was used to assess hippocampal volume before and after learning Cabbies who passed the test showed significant growth in their posterior hippocampus, whereas controls did not (Maguire, Gadian, Johnsrude et al., 2000) (Woollett and Maguire, 2011) Implicit memories # Two structures that play important roles in the formation of implicit memories are\u0026hellip; Cerebellum: involved in learning of procedural memories for skills\nMakes sense, implicit is mostly procedural memories Basal ganglia: deep brain structure important in motor sequencing\nParkinson’s disease involves degeneration of parts of the basal ganglia e.x. you want to get out of a chair but don\u0026rsquo;t know how Cases of organic amnesia have provided evidence for the distinction between different memory systems (e.g., implicit and explicit) Summary # ACT-R: Hybrid Cognitive Architecture # Controversies in Cognitive Science # To what extent is the mind modular, that is, organized in special information-processing modules?\nIf memory is modular, how do the modules interact? How might this be represented in a computational model of mind? ACT-R: One of the most famous cogsci models: about how memory/information is stored and organized \u0026ldquo;Adaptive control of thought \u0026ndash; rational\u0026rdquo; Cognitive architecture with modular organization that was developed by John R. Anderson in 1976 It is hybrid in sense that it incorporates both symbolic (e.x. static rules) and subsymbolic (i.e. neural networks) information processing ACT-R buffer Workspace: what you\u0026rsquo;re paying attention to Serves to integrate outer and inner, between modules on different layers Acts like control system: Theres a feedback loop where you send a command/signal and then get information back which you use to adjust subsequent commands Perceptual-motor layer Perceptual module in turn consists of a visual module, audition module, etc. Motor module consists of speech module, manual module, etc. E.x you can decide to move in a certain direction (motor movements, perceptual \u0026ndash; you choose what to look at) Cognitive layer Declarative memory is organized in \u0026ldquo;chunks\u0026rdquo; Procedural memory is encoded as production rules: actions for the system to perform, e.g., retrieve a chunk from memory, send a command to the motor module to perform an action Production rules can be nested within each other, so that output of a given production rule will trigger firing of another production rule All of the above modules are encoded in the form of physical symbols What makes ACT-R a hybrid architecture is that the symbolic, modular architecture is run on a subsymbolic base\nACT-R is designed to operate serially, so that at any given moment, only one production rule can be active, but how does it select that one? The pattern-matching module controls which production rule gains access to the buffer by working out which production rule has the highest utility at the moment of selection, as determined by How likely the system is to achieve its current goal if the production rule is activated The cost of activating the production rule These calculations are subsymbolic and use an artificial neural networks approach Subsymbolic equations are also used to model how accessible information is in declarative memory\nThe basic units of declarative memory are chunks, but each chunk is associated with a particular activation level, which is determined by How useful the chunk has been in the past How relevant the chunk is to the current situation and context The general information processing that takes place in the buffers is symbolic In contrast, the calculations that determine whether or not a particular item of knowledge ends up in a buffer are subsymbolic \u0026ndash; Performance Mechanisms \u0026ndash; Learning Mechanisms \u0026ndash; \u0026mdash; Symbolic Sub-Symbolic Symbolic Sub-Symbolic Declarative Chunks Knowledge (usually facts) that can be directly verbalized Relative activation of the declaraitve chunks affects retrieval Adding new declarative chunks to the set Changing activations of declarative chunks and changing strength of links between chunks Production Rules Knoledge for taking particular actions in particular situations Relative utility of production rules affecting choice Adding new production rules to the set Changing utility of production rules Types of Memory Disorders # Anterograde amnesia: inability to form lasting memories for new experiences Retrograde amnesia: inability to remember events that occurred before the onset of condition Organic amnesia: physical cause # Brain injury through accident or stroke Korsakoff’s amnesia (Wernicke-Korsakoff syndrome): amnesia caused by brain damage resulting from thiamine deficiency, usually as a result of chronic alcoholism Not the alcohol, but lack of thiamine (nutritionally deficient) Patient staggers, develops abnormal eye movements, becomes confused, and experiences severe loss of memory for recent and relatively long-term events Patients tend to make things up (confabulate) rather than admit they can’t remember N.B.: In most cases of trauma-induced organic amnesia, there is spontaneous recovery after matter of minutes or hours\nTends to affect women (despite most alcoholics being men) since they have less of an enzyme to metabolize the alcohol Psychogenic (\u0026ldquo;hysterical\u0026rdquo;) amnesia: psychological cause # Dissociative identity disorder (multiple personality disorder): disorder in which a person exhibits two or more distinct and alternating personalities Dissociative amnesia: inability to recall important personal information, usually precipitated by a traumatic experience Hypnosis may be used to help recover memories Dissociative fugue: disorder in which person has sudden, unexpected episode of travel from home during which he can’t remember some or all of his past life Study found that incidence of this rises sharply whenever popular film depicting this subject is released Alzheimer’s disease # Organic Disease occurring in latter part of life that is characterized by deterioration of memory, reasoning, and language abilities Common form of neurocognitive disorder (deterioration of intellectual abilities; another common cause is vascular dementia) Occurs in 7% of population above age of 65 and up to 40% of people older than 80 years Associated with loss of neurons in cortical and sub-cortical regions: ventricles may be enlarged \u0026ndash; patients might end up losing as much of 50% of their brain mass Produces severe degeneration of large parts of the brain: can eventually destroy most of the hippocampus and cortical gray matter Sometimes gets confused with dementia, parkinson\u0026rsquo;s, or other disorders Deficiencies of acetylcholine: failure to show eyeblink conditioning Brains of patients contain many amyloid plaques, which contain a core of misfolded b-amyloid protein surrounded by degenerating axons and dendrites, and neurofibrillary tangles, dying neurons that contain twisted filaments of tau protein Causes # Genetic component \u0026ndash; in only ~10% of cases People with down syndrome tend to develop it Higher risk of disease in those who have previously suffered a stroke or head trauma Wear a helmet! Conditions associated with cardiovascular disease, e.g., obesity, diabetes, high blood pressure, high cholesterol, smoking, and lack of exercise, increase risk Associated with low levels of vitamin D and certain B vitamins Eat veggies for vitamins (prevents strokes too) Exposure to lead and toxic substances, such as air pollutants, may increase risk Article published in Lancet in 2017 noted that hearing loss is now known to be the largest modifiable risk factor for developing dementia, exceeding that of smoking, high blood pressure, lack of exercise, and social isolation Research indicates that use of anticholinergics (in many drugs) is associated with reduced brain volume and lower levels of glucose metabolism, particularly in the hippocampus Anticholinergics include Tylenol PM, Benadryl, Claritin, Dimetapp, Paxil, and Xanax Researchers concluded that these drugs could trigger or worsen Alzheimer’s (Gray, Anderson, Dublin et al., 2015) Those on anticholinergics also showed poorer performance on cognitive tests Also, studies have found that older people with tooth and gum disease score lower on memory and cognition tests Experts speculate that inflammation in diseased mouths migrates to the brain Preventing # Those who stay physically active and are non-obese have lower risk of developing disease Heart health =\u0026gt; brain health Studies have found that people who keep their minds active tend to show less loss of intellectual functioning in general as they age Study of nuns in convent (Snowdon, 1997) found that education and intellectual activity seem to protect against Alzheimer’s Study also found that degree of sentence complexity and amount of positive affect expressed in writing samples when subjects were in their 20’s were negatively associated with incidence of disease and positively associated with longevity There have been cases where autopsies showed presence of disease but no symptoms had been present Use it so you don’t lose it! Do Plaques and Tangles Really Cause Alzheimer’s? # Claudia Kawas’ 90+ study on the “oldest old” Team of medical experts were asked to examine years of medical records to determine if each of participants had Alzheimer’s 40% of participants that they concluded had Alzheimer’s did not have any amyloid plaques or neurofibrillary tangles at death On the other hand, about half of participants who died without symptoms of dementia had plaques and tangles Conclusion? Maybe plaques and tangles have nothing to do Alzheimer’s (unlikely) There may be multiple factors (e.g., lack of physical exercise, lack of cognitive stimulation, history of head trauma, etc.) that contribute to the development of Alzheimer’s\u0026hellip; Whether people develop the disease may depend on the number of \u0026ldquo;hits\u0026rdquo; they take from these various factors Q\u0026amp;A # Q: Which regions of the brain are thicker in successful agers than in regular agers? Those regulating cognition or emotion? A: Those regulating emotions, e.g., the midcingulate cortex and anterior insula. Q: What type of activities will increase your chances of remaining sharp into old age? A: Those that require hard work and cause you to feel somewhat tired, stymied, frustrated, e.g., grappling with a math problem or pushing yourself to your physical limits. (From research by Dr. Lisa Feldman Barrett) Q: What factor most lowered people’s risk of getting dementia? A: Tiny strokes called microinfarcts Some participants had hundreds or thousands of them Q: What caused symptoms of dementia in those who died without plaques and tangles? A: Not vitamins, not alcohol, not caffeine, not exercise\u0026hellip;. but high blood pressure! High blood pressure =\u0026gt; less likely Note though that this study focused exclusively on people over 90 years of age \u0026ndash; results may be different for younger people Hyperthymesia # Hyperthymesia of Superior Autobiographical Memory: Rare condition in which one has virtually perfect memory from about age of 10 Larry Cahill and James McGaugh constructed extensive memory tests to assess extent of each memory of each of six subjects What was the date of the fall of the Berlin wall? How many times did it rain in 1986? Associated with enlarged hippocampus and caudate nucleus: standard deviation of 7 or 8 \u0026ndash; equivalent to a 10-foot tall person, as opposed to 5’9\u0026quot; Often associated with OCD type behavior; only 1 of 6 are married "},{"id":25,"href":"/cogsci-c100/mem-strategies/","title":"9: Memory Strategies","section":"CogSci C100","content":" Memory strategies # Deep processing/meaning # Meaning/deep processing: processing information deeply (by its meaning) produces better recall at a later time than does shallow processing (attending to its appearance or sound) Mere repetition is not very effective in enhancing long-term memory, but understanding aids memory To enhance recall, use elaboration: relate new information to other information in memory Organization # Organization: effective organization of material greatly enhances recall Chunking: organizing items into familiar, manageable units Chunking lists of numbers, letters, or names can enhance memory E.x. 17761812186119141941 Hierarchical organization: clustering chunks as a hierarchy Material organized in a hierarchical structure is recalled significantly better than disorganized material Organization of material during studying aids later recall First-letter technique: using the first letter of each word in a series to make up a phrase (acronyms) Roy G. Biv HOMES Please Excuse My Dear Aunt Sally King Henry Died Monday Drinking Chocolate Milk SOH-CAH-TOA SACRED MANOR Mother Very Earnestly Made Jelly Sandwiches Using No Peanuts King Peter Came Over From Germany Seeking Fortune Fat Cats Get Drunk After Eating Beans The knuckle trick for the number of days in each month Imagery # Recall is best when information is encoded both visually and semantically Key word method Method of loci technique or mnemonic (memory technique) Involves creating imaginary locations (houses, palaces, roads, and cities) to which the same procedure is applied. It is accepted that there is a greater cost involved in the initial setup, but thereafter the performance is in line with the standard loci method. The purported advantage is to create towns and cities that each represent a topic or an area of study, thus offering an efficient filing of the information and an easy path for the regular review necessary for long term memory storage.\nRhymes and songs # Homer’s Odyssey was transmitted by storytellers who relied solely on their memories The use of rhyme, rhythm, and repetition helped the storytellers remember Even the simple addition of a familiar rhythm or melody can help students remember information Many children learn the letters of the alphabet to the tune of \u0026ldquo;Twinkle, Twinkle, Little Star\u0026rdquo; I before E except after C And when saying \u0026ldquo;A\u0026rdquo; as in Neighbor or Weigh. (And weird is weird.) Spring ahead; Fall back Those at the top of the cave must \u0026ldquo;hold on ‘tite’ lest they fall down\u0026rdquo; to distinguish between stalagmites and stalactites Imagery and prospective memory # Use of imagery in enhancing prospective memory Important in healthcare because old people have bad memory but have to take there medicine consistently Study on 60- to 81-year-old adults who needed to remember to test their blood sugar Imagination condition: pictured themselves taking blood sugar during activity they routinely engaged in, e.g., right after drinking morning orange juice Rehearsal condition: repeatedly reciting instructions out loud Deliberation condition: writing down pros and cons for taking blood sugar Results: participants remembered 76% of time in imagination condition vs. 46% in other conditions (Liu \u0026amp; Park, 2004) Use of physical retrieval cues can also be effective for enhancing prospective memory, e.g., leaving pen in middle of desk or tying yarn around finger (Rogers \u0026amp; Milkman, 2016) Review # Review: putting information into your own words and testing yourself to determine what you do not know is much more effective than passively reading over material Even more effective if you wait at least ten minutes after reading a selection before quizzing yourself Talking to people about what you know and testing yourself ( generation effect) is one of the most effective memory strategies! Taking lecture notes by hand, which requires summarizing material in your own words, leads to better retention than does verbatim laptop note taking (Mueller \u0026amp; Oppenheimer, 2014) \u0026ldquo;The pen is mightier than the keyboard\u0026rdquo;\nSpacing effect # Spacing effect: information is retained better when study sessions are distributed over time rather than massed Why cramming is bad Study compared students who learned 50 Spanish words in lessons spread one day apart in lessons separated by 30 days When tested eight years later, the students who had distributed their practice recalled almost twice as much (Bahrick \u0026amp; Phelphs, 1987) Practice effect # Practice effect: the more time that is spent studying material, the more likely one is to remember it Overlearning aids recall Ericsson’s studies of expert performers in diverse fields (music, math, science, chess, and sports) found that in every case, outstanding performers shared certain characteristics 10,000-hour rule: virtually all experts had spent over 10,000 hours in deliberate practice (usually 10+ years for at least 4 hours every day) They were taught by outstanding teachers Mozart started practicing the violin at the age of three, his father was an outstanding violin teacher, and he practiced on average 10 hours a day A large number of studies have found a direct correlation between number of hours of practice and level of expertise in elite musicians, chess players, etc. E.x. Olympians Context effect # Context effect (encoding specificity effect or state dependent memory): recall is better where there is high degree of similarity between test environment and encoding environment People who learned lists of words underwater showed better recall when tested under water; those who learned on land showed better recall when tested on beach (Godden \u0026amp; Baddeley, 1975) Students who learned words in room that smelled of chocolate, cinnamon-apple, or mothballs performed better on recall test if same smell was again present (Schab, 1990) Participants who were drunk when learning a word list performed better on recall test when they were again drunk (Weingartner, Adefris, Eich et al., 1976) Self-reference effect # Me! Me! Me!\nSelf-reference effect: memory is enhanced by relating stimuli to one’s own personal experience In advertising, people are more likely to recall the brand name of a product if they had been asked to imagine themselves using the product than if they had been given information about the product (Klein \u0026amp; Kihlstrom, 1986) Best if you imagine yourself as a participant in experiments Asians exhibit greater self-referencing of Asian models, and this results in more favorable attitudes toward the ad and greater purchase intention (Martin, Lee, \u0026amp; Yang, 2004) Emotional arousal # Emotional arousal: arousal enhances learning and retention of memories\nArousal sears events into the brain \u0026ndash; why people recall their first kiss, political assignations, etc. If you want to learn something, get excited about it People given a drug that blocks the effects of stress hormones will later have more trouble remembering the details of an upsetting story Why you can recall old childhood movies well e.x. why most people (who lived through 9/11) can recall that day better than yesterday However excessive stress may corrode neural connections (e.g., posttraumatic stress disorder) Sleep # Sleep, especially REM sleep, is important for memory consolidation Information transfers from hippocampus to other regions Less interference when you\u0026rsquo;re sleeping Timing factor # Timing factor: material presented an hour before sleep tends to be remembered best Even 10 minutes of waking rest enhances memory of what we have read (Dewar, Alber, Butler et al., 2012) Multimodal approach # Multimodal approach: paying attention to your mental and physical condition, and choosing what memory strategy(s) that will work best for you Belief # Belief: anything that you believe will boost your memory will boost your memory! Researchers were able to use subliminal messages representing either senile behavior (e.g., \u0026ldquo;absent-minded,\u0026rdquo; \u0026ldquo;senile\u0026rdquo;) or \u0026ldquo;wise\u0026rdquo; behaviors (e.g., \u0026ldquo;sees all sides of issues,\u0026rdquo; \u0026ldquo;smart\u0026rdquo;) to manipulate performance of seniors on memory tests (Levy \u0026amp; Langer, 1994) Issue of whether all memories are potentially retrievable is impossible to prove, but it’s probably advantageous to believe that they are Other Factors Affecting Memory # Schemas # Schemas: generalized mental representation or concept of a given class of objects, scenes, or events that can aid (or distort!) recall Mood-congruence effect # Mood-congruence effect: tendency to recall experiences that are consistent with one’s current good or bad mood Depressed people are likely to recall more negative experiences than positive experiences Currently depressed people recall their parents as rejecting, punitive, and guilt-promoting, whereas formerly depressed people describe their parents much as do those who have never suffered depression In the long run, people tend to recall pleasant events better than unpleasant ones (positivity bias) E.x. you recall your good gradeschool teachers, but not the bad ones In the short run, we tend to experience a negativity bias E.x. you have a good day but recall the one bad experience as you\u0026rsquo;re falling asleep Mood effect # Mood effect: people score more poorly on memory tests when they are depressed Introversion/extraversion factor # Introversion/extraversion factor: introverts are more easily distracted by outside noise/music during encoding than extraverts Explains why extroverts may enjoy studying at Cafes / with music Factors Affecting Neurogenesis # Learning An enriched environment E.x. rats in social environment have more dense brains Exposure to estrogen Physical exercise # Physical exercise Studies involving mice, as well as people, show strong relationships between being physically active or fit and having greater brain volume and stronger cognitive abilities \u0026ndash; even when participants are young and healthy Recent research on mice indicates that the neurocognitive benefits of exercise derive in large from irisin Irisin: hormone produced during endurance exercise that improves cognitive function, dampens neuroinflammation, ensures the healthy development of new neurons, and reduces risk of development of Alzheimer’s Muscle fiber contains a scaffold made of special proteins that hold these acetylcholine receptors in place Rats that ran on a treadmill show increased levels of brain-derived neurotrophic factor (BDNF), which is known to support the health of existing neurons and coax the creation of new brain cells (Cassihas, Lee, Fernandes et al, 2012) As little as 3 hours a week of aerobic exercise (brisk walking) triggers biochemical changes that increase production of new neurons; stretchers-and-toners do not show similar brain changes (Colcombe, Erickson, Paige et al, 2006) Children who walked briskly or jogged on a treadmill for 20 min showed marked improvement in math and reading comprehension scores; those who read quietly for 20 min did not (Pontifex, Saliba, Laine et al, 2013) Older women who walked or did weight training for 6 months performed significantly better on tests of spatial and verbal memory (Nagamatsu, Chan, Davis et al, 2013) Sedentary behavior is associated with brain shrinkage: reduced thickness of medial temporal lobe, which contains hippocampus (Siddarth, Burggren, Eyre et al, 2018) Rats in experimental and control groups were then placed in brightly lit maze with single, darkened chamber Rats gravitate toward dark places, so animals were expected to learn the location of the chamber and aim for it Bags of weighted pellets were gently taped to the rats’ rear ends They were rewarded with a Froot Loop when they reached the top of a 3-foot ladder (Kelty, Schachtman, Mao et al., 2019) Rats in experimental group were injected with substance known to induce inflammation and cognitive impairments, then given weight training Research on rats indicate that exercise can even compensate for neurocognitive impairments Results: In first few tests, control animals were fastest and most accurate With a little practice though, the weight-trained animals, despite their induced cognitive impairments, caught up to and in some cases surpassed the speed and accuracy of the controls You can perform better even if you have dementia if you exercise! Moreover, researchers found that the memory centers of the weight trainers now teemed with enzymes and genetic markers known to help kick-start the creation and survival of new neurons Any amount of exercise is beneficial Waking up and working out You have the most motivation at the start of the day BDNF (clear your mind and help you make positive decisions) and Endorphins (minimize discomfort, block pain, feelings of pleasure) are released when you work out Addictive, but good for you Factors that Inhibit Neurogenesis # Stress and glucocorticoids \u0026ndash; as little as a few hours of either in a rat! Potentially food additives, and in general unhealthy heating Nootropics: Drugs that may enhance memory # Caffeine: anything that increases attention will tend to boost learning and memory Gingko biloba: may enhance memory in Alzheimer’s patients Increases blood flow Experimental drugs Drugs that boost production of CREB (e.g., phosphodiesterase inhibitors), which turns on genes that code for production of proteins important in formation of new synapses Drawback: can produce really nasty side effects, such as severe nausea Drugs that increase levels of glutamate, a neurotransmitter that enhances LTP Drawback: may increase risk of seizures and strokes At this point, it’s probably much more effective just to get more sleep! "},{"id":26,"href":"/eecs-16a/9/","title":"9: Op-Amps, Comparators \u0026 Charge Sharing","section":"EECS 16A","content":" \\(\\) 03/29: Operational Amplifier and Comparator # Dependent Sources # As briefly mentioned before, the voltage and current sources covered so far have been independent sources, meaning they have a constant, fixed value. However, dependent sources also generate currents and voltages but their output depends on some other \u0026ldquo;controlling\u0026rdquo; current .$i_x$ or voltage .$v_x$ in the circuit.\nVCVS: Voltage-controlled voltage source .$V = f_a (v_x)$ Unitless.$^1$ voltage gain .$a$ E.x. Voltage amplifier Input, output impedance: .$\\infty, 0$ CCCS: Current-controlled current source .$I = f_c (i_x)$ Unitless.$^1$ voltage gain .$c$ E.x. Current amplifier Input, output impedance: .$0, \\infty$ VCCS: Voltage-controlled current source .$I = f_b (v_x)$ Units: Siemens, .$S$; .$\\Omega^{-1}, \\mho$ E.x. Transconductance amplifier with conductance .$b$ Input, output impedance: .$\\infty, \\infty$ CCVS: Current-controlled voltage source .$V = f_d (i_x)$ Unit: Ohms E.x. Transresistor with resistance .$d$ Input, output impedance: .$0, 0$ $^1$The proportionality constant between dependent and independent variables is unit/dimension-less if they are both currents (or both voltages)\nOp-Amp # An operational amplifier (op-amp) is a device that transforms a small voltage difference into a very large voltage difference. Made up of\u0026hellip; Two input terminals with potentials: $U_+$ (non-inverting) and $U_-$ (inverting) Signal voltage = $\\Delta U = U_+ - U_-$ Two power supply terminals $V_{DD}$ (positive power supply) and $V_{SS}$ (negative power supply) One output terminal with potential: $U_{out} \\in [V_{SS}, V_{DD}]$ We can model the op-amp behavior with the two VCVS circuit to the right Notice that no current is drawn from either power supply terminals so the behavior of the circuit shouldn\u0026rsquo;t change $A$ is the internal signal gain Unit-less quantity The larger $A$, the sharper/quicker the transition between $V_{SS}$ and $V_{DD}$ With KVL we can find $U_{out}$: Except we can\u0026rsquo;t have $U_{out} \u0026gt; V_{DD}$ or less than $V_{SS}$ so we end up clipping $U_{out}$ \u0026ndash; this is called railing $$U_{out} = V_{SS} + \\frac{V_{DD} - V_{SS}}{2} + A \\Delta U$$ $$\\dots = \\frac{V_{DD} + V_{SS}}{2} + A \\Delta U$$ $$\\dots = \\begin{dcases} V_{SS} \u0026amp; \\frac{V_{DD} + V_{SS}}{2} + A \\Delta U \u0026lt; V_{SS} \\\\ V_{DD} \u0026amp; \\frac{V_{DD} + V_{SS}}{2} + A \\Delta U \u0026gt; V_{DD} \\\\ \\frac{V_{DD} + V_{SS}}{2} + A \\Delta U \u0026amp; \\text{otherwise} \\\\ \\end{dcases}$$ Rail-determined offset: $\\dfrac{V_{DD} + V_{SS}}{2}$ $y$-intercept in $V_{out}, \\Delta U$ graphs Zero in NFB, that is $$U_+ = U_-$$ Signal gain: $A \\Delta U = A (U_+ - U_-)$ $A$ is the slope of the zone between rails $A \\approx \\infty$ is a vertical line Op-Amp as a Comparator # The sign of the output will indicate which input voltage is larger. We often want to know whether $V_?$ is smaller or larger than $V_{ref}$ so we\u0026rsquo;ll set $ V_- = V_{ref}$ and $V_+ = V_?$ The blue region is the (super narrow, and in the ideal case, perfectly vertical) linear region where the input actually has \u0026ldquo;space\u0026rdquo; to be amplified without clipping For the vast majority of realistic inputs, the outputs sits at one of the rails (here, $V_{SS} = −2V$ and $V_{DD} = 3V$) In this way, the sign of the output directly gives the relationship between $V_{?}$ and $V_{ref}$ Note that with zero input difference, the output voltage ($y$-intercept) sits at the rail-offset: $\\left(0, \\dfrac{V_{DD} + V_{SS}}{2} \\right) = (0, 0.5)$ Comparator # Comparator: Enables us to take a wonky digital signal (i.e voltage) and convert it to a binary output signal Optimized for binary output and speed Used in devices that measure and digitize analog signals i.e ADCs $$U_{out} = \\begin{cases} V_{DD} \u0026amp; U_+ \u0026gt; U_- \\\\ V_{SS} \u0026amp; U_+ \u0026lt; U_- \\\\ \\end{cases}$$ Capacitive Touchscreen with Comparator # AC # See last weeks notes: 8: AC Source\nBecause they are on the same node, we can observe $U_+ = V_C$ and $U_− = V_{ref}$ The power sources are $V_{DD} = 3.3V$ and $V_{SS} = 0$, thus we can write the following: $$V_{out} = \\begin{cases} 3.3V \u0026amp; V_C \u0026gt; V_{ref} \\\\ 0V \u0026amp; V_C \u0026lt; V_{ref} \\\\ \\end{cases}$$ So how do we choose $V_{ref}$? We want our comparator to be high (on, $V_{out} = V_{DD}$) depending on touch (or lack thereof) Thus we should set $V_{ref}$ to be between the peak of $V_C$ with a finger and the peak of $V_C$ without a finger To be fairly robust to noise/error, $V_{ref}$ should be exactly between the two peaks: $$V_{ref} = \\frac{V_{C, touch} + V_{C, no\\ touch}}{2} = \\dots = \\frac{1}{2} \\frac{C_{eq} + \\Delta C}{C_{eq} + C_{ref} + \\Delta C}\\frac{C_{eq}}{C_{eq}+C_{ref}}$$ When there is a finger present, $V_C$ is always less than $V_{ref}$ so the output voltage is 0\nNow every period of the current source cycle, $T$, we can measure if there is a finger touching this pixel\nDC # We can also use a DC voltage source too and end up with the same equation\n03/31: Charge Sharing and Negative Feedback # Charge Sharing # Floating node: Node out of or into which no charge can flow \u0026ndash; virtual ground We apply charge sharing to these nodes If the sum of charges is 0 on a floating node, the sum of charges on that floating node at steady state will stay 0. Examples: Nodes connected only to capacitor plates Nodes connected only to op-amp inputs or comparator inputs Notation: In each phase, the circuit has a different configuration by opening/closing different switches. Generally, a switch labeled $\\phi_i (i = 1,2,3, \\dots)$ will be closed in phase $i$ and open in all other phases. Switches labeled $\\phi_1$ will be closed in phase 1 and open in phase 2, Switches labeled $\\phi_2$ will be open in phase 1 and closed in phase 2. Algorithm # Step 1 # Label the voltages across all the capacitors. Choose whichever direction (polarity) you want for each capacitor - this means you can mark any one of the plates with the “+” sign, and then you can mark the other plate with the “−” sign. Make sure you stay consistent with this polarity across phases.\nStep 2 # Draw the equivalent circuit during both phases. For this problem, phase 1: $\\phi_1$ closed, $\\phi_2$ open, and phase 2: $\\phi_1$ open, $\\phi_2$ closed. Also, label all node voltages on the circuit for both phases. No need to try and maintain the same names, since certain nodes of the phase 1 circuit might be merged or split in phase 2. $\\phi_1$ $\\phi_2$ Step 3 # Identify all “floating” nodes in your circuit during phase 2. You can identify those nodes as the nodes connected only to capacitor plates, op-amp inputs or comparator inputs. These will be the nodes where we apply charge sharing.\nIn this case the only node that is floating during phase 2 is node $u_3$. (Node $u_2$ is connected to the voltage source, i.e. $u_2 = V_S$, and the 3rd node is the ground node)\nStep 4 # For steps 4-6 we will examine each phase 2 floating node individually. Pick a floating node from the ones you found in step 3 and identify all capacitor plates connected to that node during phase 2. Then, calculate the charge on each of these plates in the steady state of phase 1. To do so, identify all nodes in your circuit during phase 1. Label all node voltages, and write the voltages across each capacitor as functions of node voltages (step 2 should help you with that). Do this according to the polarities you have selected. Then the charge is found as $Q = CV_C$ (where $V_C$ is the voltage across a capacitor). Careful: The plate marked with the “−” sign will have $Q = -CV_C$ and the plate marked with the “+” sign will have $Q = CV_C$ stored onto them. Careful: We assume here that you know all node voltages during phase 1. If you don’t, before starting this procedure calculate the node voltages you need using one of the previously introduced circuit analysis techniques (most likely KVL). Looking at our single floating node we can see that the “+” plates of $C_1$ and $C_2$ are connected to it during phase 2. Let’s calculate the charge on these plates connected to the floating node $u_3$ during phase $\\phi_1$, denoted $Q_{u3}^{\\phi 1}$ $$Q_{u3}^{\\phi 1} = V_1^{\\phi 1} C_1 + V_2^{\\phi 1} C_2 $$ $$\\dots = (V_S - 0)C_1 + 0$$ $$\\dots = V_S C_1$$ Step 5 # Find the total charge on each of the floating nodes in the steady state of phase 2 as a function of node voltages using the same process as Step 4. Make sure you kept the polarity same and pay attention to the sign of each plate. $$Q_{u3}^{\\phi 2} = V_1^{\\phi 2} C_1 + V_2^{\\phi 2} C_2 $$ $$\\dots = (u_3 - u_2)C_1 + (u_3 - 0)C_2$$ $$\\dots = (u_3 - V_S)C_1 + u_3 C_2$$ Step 6 # Apply conservation of charge. The charge in the steady state of phase 1 we calculated in Step 4, $Q_{u3}^{\\phi 1}$, is the initial charge of phase 1. Charge must be conserved at a floating node, so the steady state charge of phase 2, $Q_{u3}^{\\phi 2}$, should equal the initial charge $Q_{u3}^{\\phi 1}$. We can then write in terms of the unknown(s). $$Q_{u3}^{\\phi 1} = Q_{u3}^{\\phi 2} $$ $$V_S C_1 = (u_3 - V_S)C_1 + u_3 C_2 $$ $$u_3 = \\frac{2C_1}{C_1 + C_2}V_S$$ Step 7 # Repeat steps 4-6 for every floating node. This will give you one equation per floating node (i.e. if you have $m$ floating nodes you will have $m$ equations). You can then solve the system of equations to find the node voltages during phase 2 (unknowns).\nIn this problem we did not go through step 7 since we only had one floating node during phase 2. This means we have only one unknown node voltage ($u_3$) for which we solved using our single equation from Step 6. Example 2 on page 4 has multiple floating nodes.\nImportant Notes # A node that is floating during phase 2 is not necessarily floating during phase 1. In fact, it could have been two separate nodes during phase 1 depending on how your switches are configured. We only care about the floating nodes during phase 2 since those are the nodes with unknown voltages. What about floating nodes during phase 1? You should be able to calculate the voltage of these nodes based on the initial condition of the circuit and the circuit techniques that you have learned so far (most likely KVL). When handling charge sharing problems you should avoid using any parallel or series capacitance formulae that you have learned. Any simplification of the circuit might lead to mistakes since the circuit in different phases is configured differently and some capacitors placed in series during one phase may be in parallel or even not connected at all in another phase! "},{"id":27,"href":"/cogsci-c100/forgetting/","title":"10: Forgetting \u0026 Reconstruction","section":"CogSci C100","content":" Are there limits to memory capacity? # Memory experts # Rajan Mahadevan memorized .$\\pi$ to over 30,000 digit in 3 hours, 45 minutes; father had memorized all of Shakespeare’s sonnets Subconsciously done and primarily for numbers Russian journalist Shereshevskii \u0026ldquo;recalled everything he had ever seen or heard\u0026rdquo; Could report back 70 digits or words after only having heard them once Could recall list and circumstances under which he had learned it 15 years later Potentially first (recorded) case of Hyperthymesia Drawbacks of having a \u0026ldquo;perfect\u0026rdquo; memory Rajan: \u0026ldquo;As a child, I used to be so lost in my own thoughts, I would talk to myself. It was hard to fit in. Other kids didn’t know what to make of me.\u0026rdquo; Shereshevskii: ultimately, unable to distinguish between conversations he’d heard 5 minutes or 5 years before, he ended up in an asylum Memory whizzes are often poor at abstract thinking \u0026ndash; generalizing, organizing, evaluating Savant Syndrome and Memory # Savant syndrome: people who are born with severe intellectual disability but show superior ability in one intellectual domain, such as music, art, or mental arithmetic\nAbout 10% of children with autism have savant talents Analysis of case history of 13 musical savants All had severe deficits in ability to understand and use language 5 were blind or partially so All showed an extraordinarily intense interest in performing music beginning at a very young age, usually before age 4 (Treffert, 2008) Savant syndrome is largely attributable to a seemingly limitless memory Artistic savants can reproduce exact copies of animals or people or scenes from memory Musical savants can play back, note for note, long passages of music heard just once Human calculators can tell you the day of the week that corresponds with any given day of any given month and year, past or future Suggests that memory capacities are potentially virtually limitless\u0026hellip; Possible to create savant-like memorization skills and artistic abilities in people without autistic traits by disrupting left anterior frontal lobe with TMS OG Rainman, Kim Peek, didn\u0026rsquo;t have two separate hemispheres (no interference) \u0026ndash; could read two pages at once Neurocognitive Model of Savant Syndrome # Savant syndrome is associated with Disruption of global connectivity in neural networks, which results in impairment in certain types of cognitive processing, such as executive function and social cognition Less global and more local focus Enhanced connectivity in local brain regions (in part through disruption of connections to prefrontal cortex, which exerts inhibitory control on other cortical regions), resulting in specialization and facilitation of low-level cognitive processing Feats of memory in ordinary people # Participants were shown 2500 slides of faces and places for 10 sec each Afterwards, they were able to pick out slides they had seen with 90% accuracy (Standing, Conezio, Haber, 1970) People can remember a substantial amount of material learned decades earlier, including foreign language vocabulary, mathematical knowledge, and information from a psychology course May not be able to produce information without stimuli (hint) Three stages of memory processing and forgetting # Memory as an information processing system: Forgetting can derive from problems in encoding, storage, or retrieval Encoding failure # Encoding failure: information never entered long-term memory (lack of attention)\nMemory of penny: You must have seen the Apple logo thousands of times; Can you draw it? Only 1 in 85 UCLA students, including 52 Apple users, could accurately draw the Apple logo (Blake, Nazarian, \u0026amp; Castel, 2014) Storage decay # Storage decay: information stored in long-term memory gradually fades\nIn general, storage decay is not as severe as most people tend to think… Study found that people remembered nearly 40% of foreign language vocabulary, idioms, and grammar after 50 years (and 75% when recognition tests are used) People who had taken psychology class remembered about 70% of broad general facts and research methods 10 years later (Bahrick, Bahrick, \u0026amp; Wittinger, 1975) Role of interference # Is storage decay simply due to the passage of time or to interference from new memories formed during this interval? Research suggests that storage decay is primarily due to interference\nParticipants learned lists of nonsense syllables, then either slept or engaged in normal daily activities Recall was significantly better when participants slept during the retention interval (Jenkins \u0026amp; Dallenbach, 1924) The degree to which memories interfere with each other depends on their similarity Study subjects intermittently It’s harder to remember a list of letters if all the letters rhyme (V, G, P, D) Retrieval failure # Retrieval failure: failure to access information that is stored in long-term memory \u0026ndash; \u0026ldquo;forgotten\u0026rdquo; material is not completely erased but merely inaccessible\nResearch on cuing # Evidence for retrieval failure Willem Wagenaar (1986), a Dutch psychologist kept a diary in which he recorded one or two of his experiences every day for six years, resulting in a total of over 2400 incidents He tested his memory by having a colleague supply some information from each diary entry, then seeing if he could recall the remainder With a sufficient number of cues, he was able to recall virtually every incident that he had recorded over those six years Savings during relearning # Research using digit-word pairs has found that even when information appears to have been entirely forgotten (can neither be recalled or recognized), the information can be relearned much more rapidly the second time around Even in the case of material that has apparently been \u0026ldquo;forgotten\u0026rdquo; , some memory trace is preserved In addition, even if you can’t actively recall material, it may still be present in your brain in some form and may affect you in various ways Education is what survives when what has been learned has been forgotten\u0026quot; \u0026ndash; B.F. Skinner\nTOT phenomenon # Tip-of-the-tongue (TOT) phenomenon: sensation we have when we are confident that we know the word for which we are searching yet cannot recall it\n\u0026ldquo;The signs of it were unmistakable; he would appear to be in mild torment, something like the brink of a sneeze, and if he found the word his relief was considerable\u0026rdquo; This subjective feeling of knowing strongly suggests that the forgotten material is really still there, but is this feeling trustworthy? Research suggests that it is Even when people cannot remember the word for which they are searching, they often can identify important attributes such as the first letter, the number of syllables, and (in Romance languages) the grammatical gender of a noun Providing first letter or number of syllables of target word may prompt recall Causes of retrieval failure # Lack of appropriate retrieval cues (due to encoding specificity principle) Ex: Inability to recognize student from your biology class at a dorm party Context effect: you walk into kitchen to get something, but forget; returning back into the original room, you recall again! Repression of painful or anxiety-provoking information There have been documented cases of individuals who had been treated in hospital emergency rooms for childhood sexual abuse, yet these individuals failed to recall the episode when interviewed as adults (Williams, L.M., 1994; Schooler, Bendiksen, \u0026amp; Ambadar, 1997) Are Encoded Memories Ever Forgotten? # Difficult, if not impossible to prove, but the bulk of research evidence suggest that memories are never completely forgotten \u0026ndash; they are still present in some form though they may be inaccessible Penfield study \u0026ndash; stimulation of the temporal lobes could lead to vivid recall of (once thought forgotten) memories Oliver Sacks’ \u0026ldquo;Murder\u0026rdquo; story Memory and Forgetting in Children # Babies only 3 months old can learn that kicking moves a mobile – and retain that learning for a month However, adults generally can’t remember events that occurred before 2 or 3 years of age \u0026ndash; infantile amnesia College students recalled details of birth of younger sibling much more accurately if they were three-years-old at the time of the sibling’s birth than if they were two-years-old (Usher \u0026amp; Neisser, 1993) Recall is poorer if a dramatic change in childhood environment occurred (moving to a different country) or if language spoken changed Two contributing factors: We index much of our explicit memory with a command of language that young children do not possess The hippocampus is one of the last brain structures to mature, and as it does, more gets retained (Akers, Martinez-Canabal, Restivo et al., 2014) Study found that 10-year-olds could consciously recognize (amid other photos) only 1 in 5 of their former preschool classmates However, their physiological responding (skin perspiration) was greater when shown their former classmates \u0026ndash; even when not consciously recognized (Newcombe, Drummey, Fox et al., 2000) You have been mine before,\nHow long ago I may not know:\nBut just when at that swallow\u0026rsquo;s soar\nYour neck turned so,\n\u0026ndash; Dante Gabriel Rossetti\nMemory Reconstruction # Memory as reconstruction: what we think we remember often never really occurred \u0026ndash; we filter information and fill in missing pieces\nReconsolidation # Reconsolidation: Whenever we retrieve a memory, the brain rewrites it a bit \u0026ndash; it is slightly altered chemically by a new protein synthesis that links it to our present concerns and understanding\nEvery thought we think rewires our brain to some extent, changing its structure and/or function Because the processes involved in memory reconstruction are unconscious, we can be convinced that our memories are accurate even when they are partially or even wholly wrong (Offer, Kaiz, Howard et al., 2000) 73 ninth-grade boys were interviewed, then reinterviewed 35 years later When asked to recall how they had reported their attitudes, activities, and experiences, most men performed at a rate no better than chance 1 in 3 now remembered having received physical punishment though, as ninthgraders, 82% said they had Researchers are experimenting with manipulating reconsolidation to treat people with traumatic memories People are asked to recall the traumatic or negative experience At the same time, they are given Propranolol, a memory-blocking drug or A brief painless electric shock This disrupts reconsolidation of the memory, \u0026ldquo;erasing\u0026rdquo; it in part Treatment still in experimental stages at this point Causes of Memory Distortions # Consistency bias # Consistency Bias (or hindsight bias): we tend to reconstruct the past to be more consistent with our current feelings and beliefs\nPeople asked how they felt 10 years ago about marijuana or gender issues recalled attitudes closer to their current views than to the views they had actually reported a decade earlier (Markus, 1986; Mazzoni \u0026amp; Vannucci, 2007) Couples who were re-interviewed after eight months remembered their past feelings about their partners as matching their current feelings more than had actually been the case (McFarland \u0026amp; Ross, 1987) Schemas # Schemas: generalized information about a situation or event (e.g., things people do at birthday parties)\nIn general, people show enhanced recall for schema-consistent material but there are exceptions Repisodic memory # Repisodic memory: recall of a supposed event that is really the blending of details over repeated and related episodes\nIf asked to recall the details of last Monday’s lunch, you might produce a repisodic memory based on what you usually do for lunch Misinformation effect # Misinformation effect: incorporating misleading information presented after an event into one’s memory of the event\nStudy in which people were shown a film depicting a traffic accident, then either asked how fast the cars were going when they hit each other or how fast they were going when they smashed into each other People gave faster speed estimates when word smashed was used When asked a week later whether there was any broken glass in the accident, people who heard \u0026lsquo;smashed\u0026rsquo; were much more like to answer yes (Loftus \u0026amp; Palmer, 1974) Source amnesia # Source amnesia: attributing to the wrong source an event that we have experienced, heard about, read about, or imagined\nCase of Donald Thomson, a psychologist studying memory who was accused of rape after being interviewed on live television Photo lineup studies Participants watched a staged crime, then examined mug shots to identify the perpetrator A few days later, they were asked to identify perpetrator in a lineup Participants revealed a strong tendency to select people whose faces had been seen only in the mug shots Implantation of false memories # Studies show that it’s not all that hard to plant a false memory in a person’s mind Participants in one study were falsely led to \u0026ldquo;recall\u0026rdquo; that they had knocked over a punch bowl at a wedding when they were six-years-old (Hyman, Husband, \u0026amp; Billings, 1995) In a more recent study, 70% of students reported a detailed false memory of having committed a crime, such as assaulting someone with a weapon (Shaw \u0026amp; Porter, 2015) Memories are particularly easy to implant if person is asked to visualize event Overconfidence in flashbulb memories # Overconfidence in flashbulb memories (memory for the situation in which you first learned a very surprising and emotionally arousing event) People were interviewed one day after the explosion of the space shuttle Challenger and again, three years later Participants reported having vivid memories of what they had been doing when they heard the news and were very confident that their memories were accurate However, these recollections were often wrong! Average accuracy score was only 3 on a 7-point scale, and 25% of participants were wrong on every single detail (Neisser \u0026amp; Harsh, 1992) Flashbulb memories are typically no more accurate than memories for other events \u0026ndash; people’s confidence in their testimony is not strongly correlated with accuracy Eyewitness Testimony # How accurate are our memories?\nOn the one hand, our memories can be remarkably accurate On the other hand, our memories are far from perfect Eyewitness testimony is often mistaken # It is estimated that 2000 to 10,000 people are wrongfully convicted each year in the U.S. on the basis of eyewitness testimony Case of man who spent 11 years in prison for rape before they found out on the basis of DNA testing that he couldn’t have been the assailant One study examined 62 cases in which innocent people were later exonerated on the basis of DNA evidence (including 8 in which person had been sentenced to death) In 52 of these 62 cases, the crucial evidence leading to conviction had come from eyewitnesses (Scheck, Neufield, \u0026amp; Dwyer, 2000) Dilema: Eyewitness testimony is also the #1 factor that leads to conviction too Factors affecting accuracy # Errors are especially likely if: Witness’ attention was stressed and/or distracted (e.g., by presence of a gun) Plausible misinformation was given during questioning Witness is pressured to give a specific response Witness is given positive feedback (could be even a simple \u0026ldquo;OK\u0026rdquo;) Confidence of a witness is a very poor predictor of whether a memory is accurate; nevertheless juries are strongly influenced by confidence (Busey, Tunnicliff, Loftus et al., 2000) Face recognition # Experiment: Picture, in your mind, a stranger whom you’ve seen recently (e.g., waiter or waitress who served you recently or the person who sat next to you on the bus) Do you think you could pick that person out of a police lineup? What if I first asked you to write down in as much detail as you can about what that person looks like? Do you think this will improve or impair your ability to pick that face out of a lineup? Answer: Impair Attempt to verbalize memories/insights may in fact impair your ability to recall what was actually there (Dodson, Johnson, \u0026amp; Schooler, 1997) Experiment: Participants are shown series of faces and asked to make either Holistic judgments (Does she look like an accountant?) OR Judgments about specific features (Does he have bushy eyebrows?) Which condition resulted in better performance on recognition test? Answer: The holistic judgments condition (Baddeley, 1979) Expert police sketch artist focus on asking witness about emotional characteristics of suspect in sketching portrait (Jonathan Schooler) Own-ethnicity bias # Own-race bias or other race effect: People are more accurate in identifying members of their own race\nAnalyses of people’s descriptions of faces for police artist show that people tend to describe members of their own race more in terms of emotions or personality characteristics Effect seems to be primarily due to experience People of European descent more accurately identify individual African faces if they have watched a great deal of basketball on television, exposing them to many African-heritage faces (Li, Dunning, \u0026amp; Malpass, 1998) Improving Testimony # Warn witness that perpetrator might not be present Witnesses are often convinced that culprit must be present in a lineup and so simply pick the person who most closely resembles their memory of the perpetrator Researchers found that telling witnesses that perpetrator might not be present reduced number of innocent people who were identified incorrectly by 42% Cognitive interview technique # Cognitive interview technique: Ask witness to describe what happened before beginning questioning Reinstate physical and emotional context of the crime as fully as possible, e.g., by returning to the scene of the crime for the interview Ask witnesses to visualize scene and describe every detail before detective begins questioning Technique has generally been found to result in recall of 30% to 35% more information without any increase in erroneous recall Ask witness open-ended questions Recovered Memories # Much of the recovered memory literature has focused on cases of child abuse\nReal memories # Research has demonstrated that some people may indeed forget about painful childhood memories and recall it years later There have been documented cases of individuals who had been treated in hospital emergency rooms for childhood sexual abuse, yet these individuals failed to recall the episode when interviewed as adults (Williams, L.M., 1994; Schooler, Bendiksen, \u0026amp; Ambadar, 1997) Case of a college professor, Ross Cheit, who woke up one morning and suddenly remember having been molested by camp counselor \u0026ndash; counselor confessed when confronted with the crime (Schacter, 1996) False memories # Many reported incidences of abuse probably never happened Research has shown that it is not all that hard to implant false memories Study on implanting false memories in children (Ceci, 1995) An adult repeatedly asked child to think about several real and fictitious events \u0026ldquo;Think hard, and tell me if this ever happened to you. Can you remember going to the hospital with a mousetrap on your finger?\u0026rdquo;\nAfter 10 weeks, a new adult asked the same question 58% of preschoolers produced false (and often vivid) stories regarding one or more events they had never experienced When reminded that his parents had told him several times that the mousetrap incident never happened \u0026ndash; that he had imagined it, child protested, \u0026ldquo;But it really did happen. I remember it!\u0026rdquo; (Weiten, W., 1998) Children have particular difficulty with source monitoring Ex: they sometimes recall that they had performed a task that someone else had actually performed According to Freud, children’s attention tend to be focused on erogenous zones 3-year-olds were asked to show on anatomically correct dolls where pediatrician had touched them 55% who had not received genital examinations pointed to either genital or anal areas (Ceci \u0026amp; Bruck, 1993, 1995) Children’s Eyewitness Testimony # Under ideal circumstances, children’s reports can be trustworthy Reports may be unreliable when Children are young They have been supplied with suggestive questions Interviewers ask questions in a highly emotional tone or use complex language (Pipe, Lamb, Orbach et al., 2004) Nine-year-old girl said she had seen suspect with blood spattered on his shirt and hands Two weeks before scheduled execution, girl admitted that she wasn’t certain whether the red stain was blood or salsa (suspect had worked in a salsa factory) She said that she had originally testified against man because her mother had told her that he was a bad man, and people encouraged her to be more certain of her testimony than she was (Ceci \u0026amp; Bruck, 1995) Déjà Vu # Who experiences déjà vu? About 60% of population Negatively correlated with age Positively correlated with socioeconomic level and education Positively correlated with stress and fatigue More common in people who travel Scientific explanations (Alan S. Brown, 2003): Dual processing explanation Incoming sensory data follow several different pathways A slight alteration in transmission speed in one pathway could cause the brain to interpret the data as two separate experiences Attentional explanation A fully processed perceptual experience that matches a minimally processed impression received moments earlier produces a strong feeling of familiarity The original impression may not have been fully processed due to a physical distraction or a mental distraction, such as preoccupation with other thoughts Memory explanation: Implicit familiarity without explicit recollection Ex: Seeing a lamp in your friend’s apartment that is similar or identical to one that used to be in your aunt’s house "},{"id":28,"href":"/e-29/10/","title":"10: Metrology","section":"Engineering 29","content":" \\(\\) Terminology # Metrology: The scientific study of measurement When we manufacture many nominally “identical” components and then measure them, our measurements will inevitably show variation. Some of this variation will come from actual differences between the components’ dimensions — stemming, for example, from variation of the properties of the stock material used, cutting tool wear over time, changes in machine temperature during processing, or small, random, operator errors. Additional variation can, however, come from the measurement process itself, as there may be operator error in using a measuring instrument, or the instrument’s response may drift over time.\nIdeally, we would have some way to distinguish unambiguously between measurement errors and true dimensional deviations which might take a component outside of the specified tolerance zone. In reality, however, measurement and true dimensional errors are superimposed in the reading that we obtain. The best we can do is minimize measurement errors through the use of proper technique and appropriate instrument calibration. Reducing measurement error often costs money, however, either by requiring more expensive equipment or by needing a slower, more meticulous measurement process. The measurement process used needs to be appropriate for the task at hand. As long as the measurement error is a small fraction of the tolerance zone size, it will usually suffice.\nResolution, discrimination: The smallest difference between two measurements that can be detected E.x. in an instrument with a digital readout, the resolution corresponds to the change in reading that occurs when the least significant digit of the readout is incremented or decremented. In an “analog” instrument, such as a micrometer with a Vernier scale, the resolution is determined by the spacing between tick marks on the finest scale on the instrument. Accuracy: The amount of deviation of a measurement from the true value (\u0026ldquo;inaccuracy\u0026rdquo; really) Description of only systematic errors, a measure of statistical bias of a given measure of central tendency; low accuracy causes a difference between a result and a true value; ISO calls this trueness Repeatability/precision: The ability to avoid scatter/variation in successive measurements of the same physical dimension Description of random errors, a measure of statistical variability Accuracy is the proximity of measurement results to the true value; precision is the degree to which repeated (or reproducible) measurements under unchanged conditions show the same results.\nGiven a statistical sample or set of data points from repeated measurements of the same quantity, the sample or set can be said to be accurate if their average is close to the true value of the quantity being measured, while the set can be said to be precise if their standard deviation is relatively small. Resolution, accuracy and repeatability all play a role in determining measurement error. A measurement technique could exhibit large errors in individual measurements, but when those measurements were averaged might yield a mean value that was accurate. Such a technique would have poor repeatability, and inaccurate individual measurements, but would effectively be accurate on average.\nConversely, a measurement technique might show little measurement-to-measurement variability (highly repeatable) but might always give a result that deviated greatly from the true value (highly inaccurate). Such a technique would be described as biased and would indicate poor instrument calibration.\nBasic Measurement Tools # How to read a (Micrometer\u0026rsquo;s) Vernier scale # Internal Hole Micrometer # Dial Gage # Sources of Measurement Error # Parallax: where the scale is inadvertently held some distance from the edge being measured, and the angle of viewing affects the reading that is taken Geometrical errors: The instrument is not aligned properly to the dimension that is to be measured. Cosine error: When failing to use the flat portions of a caliper’s jaws to ensure the axis of a cylindrical component lies perpendicular to the diameter being measured (Measurement \u0026gt; diameter) Instrument/component Distortions: when the instrument is loaded excessively (e.g. pressing too hard on a caliper thumbwheel) and either the instrument or the component distorts elastically. The distortion may be invisible to the eye, but may be larger than the resolution of the measurement tool. Hole gage is likely to be more accurate than calipers, because the hole gage will tend to center itself within the hole whereas the jaws of the caliper may come to rest across a chord of the hole, rather than a diameter, and thus underestimate the dimension. We thus expect caliper measurements of internal diameters to be biased towards underestimation. Calibration errors/drift Could be due to wear of tools Sampling Principles # $N$: Number of measurements of the same dimension $\\sigma$: Standard deviation of underlying measurement process $\\hat \\sigma$: Best estimate of standard deviation of the process, based on $N$ measurements of it: $$\\hat \\sigma = \\sqrt{\\frac{\\sum_{i=1}^N (x_i - \\hat x)^2}{N-1}}$$ The mean of our $N$ values will not exactly equal the true value. But if the measurement process is centered on the true value, the mean of $N$ measurements follows a normal distribution with standard deviation $\\sigma / \\sqrt N$ \u0026ndash; central limit theorem $\\sigma / \\sqrt N$: Standard error of the mean We can express a confidence interval for the true dimension e.g. just a 0.3% chance that true value lies outside $\\hat x \\pm \\dfrac{3 \\hat \\sigma}{\\sqrt N}$ See 68–95–99.7 rule Measurement Processes: Important Messages # Measurement processes have inherent variability Multiple measurements of a single dimension help build confidence in our estimate of the dimension Part-to-part dimensional variation and measurement variation both need to be considered Advanced Metrology # Electromechanical Systems: Coordinate Measuring Machines (CMMs) # Industrial dimensional measurement has increasingly moved towards automated coordinate measurement machines (CMMs). A CMM has a solid probe mounted in a rigid frame and its position is actuated with motors and leadscrews having highly accurate positional feedback. In sophisticated machines, the probe may have five or more degrees of freedom to enable access to complex geometries. The probe is touched against multiple locations on a component under test, contact is detected by force sensors in the probe assembly, and software interprets the measurements to check dimensions against a solid model.\nThe probe is usually spherical and made of a hard material with a low friction coefficient such as synthetic ruby, to enable many measurements to be made without the tip significantly eroding or visibly scratching the component under test.\nLower-cost CMM machines are available, which are not as rigid but are more portable. These usually have the probe assembly on an articulated arm, with encoders in each joint that determine angular positions of the joints and thus the position of the tip at any given moment. We will see a demonstration of such an arm, a Romer system from Hexagon Metrology, in Lab 7on dimensional measurements. The accuracy of an arm-mounted CMM may be more than ten times poorer than a conventional, more rigid frame-mounted system.\nStereo Vision Systems # Stereo vision systems use the same principle as human sight to extract depth information from pairs of images. By illuminating a component with structured light to define unambiguous features on a surface, and then sweeping a pair of cameras around the component, software can determine complex 3D shapes which would require hours of laborious mechanical scanning to measure with a CMM.\nThey require the surface being measured to scatter light uniformly, so polished surfaces have to be laboriously coated with a thin layer of fine powder – which must later be removed. This requirement removes some of the advantages over traditional CMMs. There is a basic tradeoff between dimensional resolution and field of view, and there is debate about how well the accuracy of these systems has been characterized.\nBasic Principles # Same depth-measuring principle as human vision Differences between the images captured by two cameras give surface position information Requires a light-scattering finish on component (not mirror-finish) Scanning White Light Interferometry # Object and reference beams interfere at camera to give \u0026ldquo;fringes\u0026rdquo; which show surface position of object with ~ nm resolution Consider two cases: Monochromatic (single wavelength) light: can lead to ambiguous results if $z \u0026gt; \\lambda$ White light (unambiguous determination of $z$) Path length difference (object vs reference): $2z$ For a single wavelength: Constructive interference when $z = k \\lambda /2$ Destructive interference when $z = (2k+1)\\lambda/4$ $k \\in \\mathbb{Z}^+$ E-field at camera = $E_0 (1 + e^{4\\pi z / \\lambda})$ White light: many wavelengths mix to give an intensity \u0026ldquo;envelope\u0026rdquo; at the sensor Object can be scanned over long distances in $z$ direction and topography measured unambiguously $z$ Optical Interferometry vs Contact Metrology # A different type of optical metrology exploits the wave properties of light to make dimensional measurements with a resolution smaller than a single wavelength. This technique has seen wide deployment, for example, in measuring sub-micrometer structures on integrated circuits, and characterizing finely polished mirrors, lenses and prisms.\nIn optical interferometry as shown in the class, a beam of light is split by a semi-mirrored surface (beamsplitter). One of the resulting beams (the reference beam) reflects off a flat mirror and back towards an image sensor. The other beam (object beam) is directed down to the sample surface, and then back up to the image sensor.\nLet us first consider monochromatic (single-wavelength) light. When the distance travelled by the object beam is an odd number of half wavelengths greater or less than that travelled by the reference beam, the beams destructively interfere at the image sensor and yield a dark signal at a particular pixel. When the path length difference is an even number of half wavelengths, there is constructive interference and a bright signal. For intermediate path length differences, there is an intermediate signal. Each pixel in the image sensor picks up light from a different location on the sample object, and at any given instant will receive different interference intensities if the object is not perfectly smooth. The object being measured is slowly scanned vertically away from the beamsplitter, and as the path lengths change, the signals at the image sensor oscillate. With monochromatic light, however, these oscillations are periodic and the relative positions of adjacent portions of the surface cannot be unambiguously determined, especially if the surface roughness is more than about a quarter of a wavelength.\nThus, white light is more commonly used. Here, the effects of the many wavelengths superimpose on a grayscale image sensor, and the interference intensity is at its peak only when the path length difference between object and reference beams is exactly zero. As the path length differences change, the intensity oscillations decay in magnitude, and software can fit an envelope to these intensity variations to establish relative surface positions.\nThis technique is suitable for highly accurate and precise measurements of sub-micrometer dimensional variations, but is typically limited to fields of view of a few millimeters. Moreover, because it relies on reflections, surfaces must be reasonably flat; otherwise the reflected light would not bounce back towards the image sensor.\nSummary of Metrology Techniques # Kurtz and Nesbitt, Optical Engineering 50(7), 073605 (2011) on bCourses under \u0026ldquo;Supplementary materials\u0026rdquo; "},{"id":29,"href":"/eecs-16a/10/","title":"10: NFB, GRs \u0026 Buffer + Loading","section":"EECS 16A","content":" \\(\\) 04/05: # Negative Feedback (NFB) # The main idea is \u0026lsquo;how do we get an op-amp to output a voltage that isn\u0026rsquo;t railing?\u0026rsquo; Negative feedback refers to the idea that there is some output amount that is the ideal or intended amount If the actual amount becomes larger than this reference, then the system must detect this deviation and bring it down to the reference. Similarly, if the actual amount drops too low, the system must bring it back up to the reference. Negative-feedback amplifiers helps maintain the output voltage at a constant level despite the fact that the op-amp wants to rail the output. $$V_{out} = A(U_+ − U_−)$$ $$V_{out} = A(V_{in} − V_{out})$$ $$V_{out} + A \\cdot V_{out} = A \\cdot V_{in}$$ $$V_{out}(1 + A) = A \\cdot V_{in}$$ $$V_{out} = V_{in} \\frac{A}{A+1} $$ $$V_{out} \\approx V_{in} \\text{ for } A \\approx \\infty $$ To incorporate negative feedback into our op-amp design, we must create a connection that goes from the output into the negative input\nSo when $V_{in} \\in [V_{SS}, V_{DD}]$ we get a non-clipped output Therefore, it\u0026rsquo;s common to omit the supply terminals when the op-amp is in negative feedback When the op-amp is acting as a comparator (i.e. not in negative feedback) the output voltage is basically always either $V_{DD}$ or $V_{SS}$ However, when an op-amp is in negative feedback, the output voltage is generally independent of the supply We are implicitly stating that the supply voltages are large enough that we never clip That is, when negative feedback is applied around an amplifier with high open-loop gain it reduces the overall gain of the complete circuit to a desired value due to the loop gain Summary of terms Checking for NFB # One convenient method is to check what happens if the output voltage happens to fluctuate by a little bit above the desired output; when this change occurs and propagates back to the input of the op-amp, does it cause the output to come back down to the desired level? If so, the system is in negative feedback.\nZero out all independent sources as in Thevenin-Norton Equivalencies Voltage sources become wires and current sources become open circuits \u0026ldquo;Dink\u0026rdquo; the output If in NFB and $V_{out} \\uparrow$, then $U_- \\uparrow$ and $U_+ - U_- \\downarrow$ (the difference between the higher and lower voltages decreases if the lower voltage increases.) Then, $A(U_+ − U_−) \\downarrow$ also. From this, we can directly say that $V_{out} \\downarrow$ That is, an increase in the output voltage ultimately led to a decrease in the output voltage. Example on page 3 Golden Rules # 1: $i_- = i_+ = 0$ # No current flows in/out of the input terminals $U_-$ and $U_+$\nFor ideal op-amps, regardless of NFB Thus, the behavior of the input terminal\u0026rsquo;s circuits shouldn\u0026rsquo;t change 2: $U_- = U_+ \\iff$ NFB # That is, the voltage potential at the positive input terminal (relative to ground) and the voltage at the negative input terminal (relative to the same ground) are the same Intuitively, the \u0026ldquo;error signal\u0026rdquo; going into the op amp must be zero Implied by $A \\to \\infty$, full proof on page 7 $$v_{in} - f v_{out} = v_c$$ $$v_{out} = A v_c$$ $$ \\Longrightarrow v_{out} = \\frac{A}{1+Af}v_{in}$$\nThus, $v_{in} = 0V \\implies v_c = 0V$ and $v_{out}$ can be non-zero even when $v_c = 0$ given $A = \\infty$\nDAC # Digital-to-Analog Converter: Component that takes in digital signals (bits) and translates them into output analog voltages.\n04/07: Buffering, Loading, \u0026amp; Design # Reference: Op-Amp Example Circuits # Signals vs. Supply Voltages # In real systems, voltages are typically fluctuating signals e.x. changes in light intensity or electrical brain activity read from an EEG Signals are voltages, they are different from the supply voltages of an op-amp because they are typically much smaller and they change over time To distinguish, we will use the following symbol to represent time-varying voltage signals: Note that while we are alright having signals as our inputs to op-amps, voltage signals generally should not be used for power since they\u0026rsquo;d create time-varying behavior Loading # Loading: an electrical component or portion of a circuit that consumes (active) electric power\nThe loading effect makes it hard to design circuits because they mean that components will behave differently depending on what they\u0026rsquo;re connected to Specifically, if the circuit afterwards draws current through some element\u0026rsquo;s $R_{el}$, then the output voltage drops. Furthermore, the amount it drops also depends on the rest of the circuit\u0026rsquo;s resistance $R_{th}$ \u0026ndash; but we don’t want to have to redesign our circuit every time we change $R_{el}$ Example # In the real world, the battery (voltage source) has some parasitic, internal resistance along with the wires \u0026ndash; $r$ Thus, the voltage drop across the bulb depends on the value of $r$, which is determined by the battery’s internal structure Current flows in this circuit, so some of the battery’s voltage is lost to the internal resistance of the battery Most of the time this just means the battery/wires heat up $$V_{lightbulb} = \\frac{R_l}{R_l + r} V_{battery}$$ We want some way to ensure that no matter what $r$ is, the voltage dropped across the bulb was a constant, isolating it from the source Buffering # Buffer (amplifier): provides electrical impedance transformation from one circuit to another, with the aim of preventing the signal source from being affected by whatever currents (or voltages, for a current buffer) that the load may be produced with\nBuffers are a powerful tool because they allow us to split circuits into blocks that we can analyze separately and then combine later When circuit blocks behave the same way regardless of what they’re are connected to, we don’t need to worry about what’s inside, making it much easier to design complex circuits. Example # We can throw a unity gain buffer in our situation above to solve our issue In a unity gain buffer, $v_{in} = v_{out}$ The voltage at the non-inverting input is $V_{bat}$ since no current can flow into the op-amp ( GR#1) and the same voltage appears at the output because of our unity-gain buffer This is the case if, by Ohm’s law, current must flow out of the op-amp\u0026ndash; which is perfectly fine since it’s only the inputs that current cannot flow into or out of. $$V_{lightbulb} = U_{out} = V_{battery}$$ Now the full $V_{bat}$ will be dropped across the bulb, and we don’t need to worry about any loading effects No matter what the $r$ or $R$ values are, the circuit’s behavior is consistent Design Procedure # 1: Specification # Concretely restate the goals for the design.\nFrequently, a design prompt will include a lot of text, so we’d like to restate all of the most important features of our design. We’ll refer to these specifications later to determine if our design is complete. 2: Strategy # Describe your strategy (often in the form of a block diagram) to achieve your goal.\nTo do this, start by thinking about what you can measure vs. what you want to know For example in our capacitive touchscreen, we want to know if there is a touch and we can measure voltage. Since we know that a touch can change the capacitance, we break this down into the following block diagram: 3: Implementation # Implement the components described in your strategy.\nThis is where pattern matching is useful: remind yourself of blocks you know, (ex. voltage divider, inverting amplifier) and check if any of these can be used to implement steps of your strategy. 4: Verification # Check that the design from 3 does what we specified in 1\nCheck block-to-block connections, as these are the most common point for problems. Does one block load another block causing it to behave differently than expected? Are there any contradictions (ex. a voltage source with both ends connected by a wire, or a current source directed into an open circuit)? "},{"id":30,"href":"/math-53/10/","title":"10: Parametric Equations and Polar Coordinates","section":"Math 53","content":" 10.1 Curves Defined by Parametric Equations # Parametric equations are written as .$(f(t), g(t))$ They define curves (not functions!) Time .$t \\in [t_i, t_f]$ Initial point: .$\\big(f(t_i), g(t_i)\\big)$ Terminal point: .$\\big(f(t_f), g(t_f)\\big)$ For parametric equation in form .$\\big(h + r \\cdot \\sin(t), k + r \\cdot \\cos(t)\\big)$ on .$t \\in [0, 2\\pi]$ , the resulting curve is a circle clockwise centered around the point .$(h,k)$ Likewise, .$(r \\cdot \\cos(t), r \\cdot \\sin(t))$ is counter clockwise Useful Desmos calculator If we need to graph an equation in form .$x = g(y)$ , we can use parametric equations .$x = g(t)$ and .$y = t$ 10.2 Calculus with Parametric Curves # Tangents # Given .$x = f(t)$ and .$y = g(t)$ where both .$f$ and .$g$ are differentiable, we can get the tangent line to the curve by finding .$\\frac{dy}{dx}$ We can find .$\\frac{dy}{dt}$ using the chain rule: $$\\frac{dy}{dx} \\cdot \\frac{dx}{dt}$$ If .$\\frac{dx}{dt} \\neq 0$ , then we can solve .$\\frac{dy}{dt}$ : $$\\frac{dy}{dx} = \\frac{\\frac{dy}{dt}}{\\frac{dx}{dt}}$$ Horizontal tangents exist when .$\\frac{dy}{dt}= 0$ (assuming .$\\frac{dx}{dt} \\neq 0$ ) Vertical tangents exist when .$\\frac{dx}{dt}= 0$ (assuming .$\\frac{dy}{dt} \\neq 0$ ) Double Derivative # $$\\frac{d^2y}{dx^2} = \\frac{d}{dx}\\Big(\\frac{dy}{dx}\\Big) = \\frac{d}{dt}\\cdot\\frac{dt}{dx}\\Big(\\frac{dy}{dx}\\Big) = \\frac{\\frac{d}{dt}\\Big(\\frac{dy}{dx}\\Big)}{\\frac{dx}{dt}}$$\nAreas # Given .$x = f(t)$ and .$y = g(t)$ over .$t \\in (\\alpha, \\beta)$ , we can calculate the area between .$C$ and the .$x$ -axis with the following equation $$A = \\int_\\alpha^\\beta f\u0026rsquo;(t) \\cdot g(t)\\ dt $$ Likewise, between .$C$ and the .$y$-axis we derive the parametric equation for .$y$ : $$A = \\int_\\alpha^\\beta f(t) \\cdot g\u0026rsquo;(t)\\ dt $$\nArc Length # To find length .$L$ of curve .$C$ given in form .$x = f(t)$ and .$y = g(t)$ on .$t \\in [\\alpha, \\beta]$ where .$\\frac{dx}{dt} = f\u0026rsquo;(t) \u0026gt; 0$ (meaning that .$C$ is traversed once from left to right as .$t$ increases from .$\\alpha$ to .$\\beta$ ) and where .$f\u0026rsquo;$ and .$g\u0026rsquo;$ are continuous on .$[\\alpha, \\beta]$ : $$L = \\int_\\alpha^\\beta \\sqrt{\\Big(\\frac{dx}{dt}\\Big)^2 + \\Big(\\frac{dy}{dt}\\Big)^2}\\ dt$$ Note that trig functions (.$\\sin, \\cos, \\text{etc.})$ ) loop every .$\\frac{\\pi}{2}$ because of the formula finds the absolute value.\nSurface Area # Suppose the curve .$C$ given the equations .$x = f(t)$ and .$y = g(t)$ on .$\\ t \\in [\\alpha, \\beta]$ where .$f\u0026rsquo;$ and .$g\u0026rsquo;$ are continuous, and .$g(t) \u0026gt; 0$ , is rotated about the .$x$ -axis. If .$C$ is traversed exactly once, then the area of the resulting surface is given by $$S = 2\\pi \\int_\\alpha^\\beta y(t) \\sqrt{\\Big(\\frac{dx}{dt}\\Big)^2 + \\Big(\\frac{dy}{dt}\\Big)^2}\\ dt$$ Similarly, if instead .$C$ is rotated about the .$y$ -axis instead we use the following: $$S = 2\\pi \\int_\\alpha^\\beta x(t) \\sqrt{\\Big(\\frac{dx}{dt}\\Big)^2 + \\Big(\\frac{dy}{dt}\\Big)^2}\\ dt$$\n10.3 Polar Coordinates # Pole = origin; labeled .$O$ Polar axis = line through .$O$ Polar coordinates = .$r, \\theta$ .$r$ is distance from point .$P$ and .$O$ .$\\theta$ is the angle between the polar axis and the line .$OP$ .$(r, \\theta) = (-r, \\theta + \\pi)$ Converting between Cartesian # .$x = r\\cdot \\cos\\theta$ .$y = r\\cdot \\sin\\theta$ .$r^2 = x^2 + y^2$ .$\\tan\\theta = \\frac{y}{x}$ Symmetry # If polar equation is the same when .$\\theta = -\\theta$ , the curve is symmetric about the polar axis If polar equation is the same when .$r = -r$ or when .$\\theta = \\theta + \\pi$ , the curve is symmetric about the pole If polar equation is the same when .$\\theta = \\pi-\\theta$ , then the curve is symmetric about the vertical line .$\\theta = \\pi/2$ Polar Tangents # Knowing that $$x = r \\cdot \\cos\\theta = f(\\theta)\\cdot\\cos\\theta$$ $$y = r \\cdot \\sin\\theta = g(\\theta)\\cdot\\sin\\theta$$ We can use the product rule to find the tangent equation $$\\frac{dy}{dx} = \\frac{\\frac{dy}{d\\theta}}{\\frac{dx}{d\\theta}} = \\frac{\\frac{dr}{d\\theta} \\sin\\theta + r\\cos\\theta}{\\frac{dr}{d\\theta}\\cos\\theta - r \\sin\\theta} $$\nHorizontal Tangent: .$\\frac{dy}{d\\theta} = 0$ (when .$\\frac{dx}{d\\theta} \\neq 0$ ) Vertical Tangent: .$\\frac{dx}{d\\theta} = 0$ (when .$\\frac{dy}{d\\theta} \\neq 0$ ) Notice that for .$r = 0$ , then .$\\frac{dy}{dx} = \\tan\\theta$ if .$\\frac{dr}{d\\theta} \\neq 0$ We can then write the full formula: $$y-y_0 = \\frac{dy}{dx}\\Big(\\theta\\Big)(x-x_0)$$ 10.4 Area and Lengths in Polar Coordinates # Area of Polar Region # Knowing that the area of a \u0026ldquo;slice\u0026rdquo; can be written as .$A = \\frac{1}{2}r^2\\theta$ , we can expand this to the case where the curve is a function .$r = f(\\theta)$ : $$A = \\int_a^b \\frac{1}{2} r^2\\ d\\theta$$\nArc Length of Polar Curve # Given a polar curve .$r = f(\\theta), a \\leq \\theta \\leq b$ , we can re-write our base equations as $$x = r\\cos\\theta = f(\\theta)\\cos\\theta$$ $$y = r\\sin\\theta = f(\\theta)\\sin\\theta$$ Thus we can use the product rule when differentiating to get $$\\frac{dx}{d\\theta} = \\frac{dr}{d\\theta}\\cos\\theta - r\\sin\\theta$$ $$\\frac{dy}{d\\theta} = \\frac{dr}{d\\theta}\\sin\\theta + r\\cos\\theta$$ and using .$\\cos^2\\theta + \\sin^2\\theta = 1$ , we have $$\\Big(\\frac{dx}{d\\theta}\\Big)^2 + \\Big(\\frac{dy}{d\\theta}\\Big)^2 = [\\text{ugly stuff, see page 711}] = \\Big(\\frac{dr}{d\\theta}\\Big)^2 + r^2$$ so when .$f\u0026rsquo;$ is continuous, we can write the arc length equation as $$L = \\int_a^b \\sqrt{\\Big(\\frac{dx}{d\\theta}\\Big)^2 + \\Big(\\frac{dy}{d\\theta}\\Big)^2} d\\theta = \\int_a^b \\sqrt{r^2 + \\Big(\\frac{dr}{d\\theta}\\Big)^2}d\\theta$$\nCommon shapes # Rose # $$a\\cdot[\\sin \\text{or} \\cos](k\\cdot\\theta)$$\n.$r$ = .$a$ .$\\sin$ = Symmetry over .$\\theta = 0$ .$\\cos$ = Symmetry over .$\\theta = \\pi/2$ If .$k$ is even, rose will have .$2k$ petals. If .$k$ is odd, rose will have .$k$ petals. .$T = 2\\pi/k$ is range of a petal\u0026rsquo;s cycle .$2\\pi/T = k$ cycles displayed in graph .$A_{\\text{petal}} = \\frac{\\pi a^2}{4k}$ Cardioid # $$r(\\theta) = a(1-\\cos\\theta)$$ $$A = \\frac{3}{2}\\pi a^2$$ $$L = 8a$$\nLimaçon # $$r = b+a\\cos\\theta$$ Other equations I\u0026rsquo;m too lazy to type out\n"},{"id":31,"href":"/e-29/11/","title":"11-12: GD\u0026T","section":"Engineering 29","content":" \\(\\) Why use GD\u0026amp;T? # Limitations of conventional tolerancing # Example: four pins need to make a clearance fit with four holes\nTolerance limits on pin and hole spacing need to be specified (Left) Tolerance zones for centers of holes (Right) What if a pin is positioned at the extreme edges of both tolerance zones? Case 1 is extreme along $x$ Case 2 is extreme along both $x$ and $y$ In conventional tolerancing, tolerance zones are inherently one-dimensional In two- or three-dimensional designs, we risk accepting non-functional parts if each dimension is considered in isolation Options: tighten tolerances, enlarge clearance, or use\u0026hellip; GD\u0026amp;T: standard graphical language, ASME Y14.5, that allows tolerances to be unambiguously defined on drawings Allows different features to be related to each other (3D) Specifies how measurements are set up and made with datums Datums # Theoretical datums # Theoretical datums are defined on drawings using features of the component to be manufactured Datums can be Planes (e.g. flat surfaces of the component) i.e after facing operations Axes (e.g. axis of a hole or cylindrical feature) i.e when turning Every datum is given a letter on the drawing which uniquely identifies it Example: three perpendicular datum planes: Datums in measurement # Real manufactured objects deviate (even if slightly) from the ideal geometry Surfaces not perfect planes Axes not perfectly straight, etc So, we need a procedure to interpret measurements from a component to establish a \u0026ldquo;best fit\u0026rdquo; for the datum plane or axis Use a physical \u0026ldquo;datum simulator\u0026rdquo; like a rigid, flat surface (e.g. granite table) to rest against the component Measure multiple points $(x,y,z)$ with CMM or optical metrology and fit the plane or axis datum in software (e.g. use least-squares regression \u0026ndash; advanced software available for this) The number of datums needed to define a given tolerance depends on the geometry of the component and the type of tolerance Example: Physical Planar Datum Simulators # In measurement, we place a flat surface against the component as a datum simulator Needs to be at least $\\approx 10$ times smoother than the target surface We measure wrt this datum rather than the part\u0026rsquo;s features For any given dimension specified in GD\u0026amp;T, datums are defined with an order of preference. During measurement: First plane makes three-point contact with surface Second plane makes two-point contact Third plane: one-point contact Introductory examples # Example: hole in a block # Surfaces are not smooth in a real part, so we measure the hole center from the datums $A,B,C$ Example: hole spacing in plate # Four pins need to make a clearance fit with four holes \u0026ndash; tolerance limits on pin and hole spacing need to be specified Center positions # MMC: Maximum Material Condition # Use when want clearance MMC means least \u0026ldquo;play\u0026rdquo; is available between the components Bonus tolerance is added if components are not in MMC. Example: if actual size of hole is .252\u0026quot; rather than .250\u0026quot;, size of tolerance zone increases from .006\u0026quot; to .008\u0026quot; diameter LMC: Least Material Condition # Use if want interference Or for, e.g., holes very close to the edge of a part Bonus tolerance now added if more material is left on component Feature Control Frames # (Left) Geometric characteristic: symbol \u0026ndash; see more below (Middle) Size of the tolerance zone \u0026ndash; either a linear dimension or a diameter of a tolerance zone, depending on the type of tolerance (Right) Datum reference letters, if applicable Noted in the order in which they are to be established during measurement. Range of GD\u0026amp;T symbols # Categories of Tolerance # Form Tolerances # Straightness # 1 Straightness of an edge, or a line on a surface (top) # Tolerance defined as the maximum distance between the closest two parallel lines including all points on the edge under consideration Single dimensional value (no diameter symbol, Ø) Controls bowing, waisting, and barreling of cylindrical features 2. Straightness of an axis (bottom) # Cylindrical tolerance zone must fit inside this diameter Needs diameter symbol, Ø Compute axis position by forming an equidistant set by bisecting pairs of measured points on surface of the cylindrical feature Matters because we are generally trying to mate one cylindrical feature with another e.g. peg and hole Note that a cylindrical feature can have a straight axis without having straight edges, as would be the case when a feature exhibits waisting or barreling. Flatness # 2D version of straightness \u0026ndash; applies only to planar feature To measure, we sample a series points on the surface (physical edge) Can be accomplished with optical interferometry or the entire surface in one scan, or, approximately, by stylus profilometry \u0026ndash; e.g. by running a dial gage across the surface multiple times and measuring the needle deflection Tolerance zone is defined as the maximum allowable spacing between the closest two planes that enclose all points on the manufactured surface Tells us nothing about relationship with any other surface Not used, e.g. , to control thickness uniformity of a plate Disadvantage of using multiple linear measurements to check flatness is that the relative locations of these measurements are not known, so checking the flatness tolerance against the exact definition described above (maximum distance between two parallel planes) is then not strictly possible Circularity (Roundness) # Controls out-of-roundness within any plane perpendicular to the axis of the cylinder The tolerance value specifies the maximum allowable difference between radii of two concentric circles that are as close together as possible and that enclose all points of the manufactured circumference Center position of circles is fit to measurement data to give minimal difference in radii (minimum zone circle) To test this tolerance strictly, one would have to measure the shape of the circumference with stylus, and then use a data-fitting algorithm to identify the two circles with the smallest difference in radii that enclose all points How far from perfectly circular any particular circumference of a cylindrical feature is allowed to be. In many machine shops, an approximate measure of circularity is often obtained by placing the component in a V-block (right), and rotating it while a dial gage stylus rests on the upper surface. The peak-to-peak, $\\Delta d$, deflection of the gage needle is taken as an estimate of out-of-roundness. When a circularity tolerance is applied to a cylindrical feature, we require the tolerance to be independently met at all points along its axis. When we consider all longitudinal positions simultaneously, our tolerance is one of cylindricity: Cylindricity # \u0026lsquo;3D version\u0026rsquo; of circularity All points on surface of cylindrical feature must lie between two concentric cylinders separated by no more than the tolerance value Axial position and orientation of cylinders is fit to measurement data to give minimal difference between radii (MZC: minimum zone cylinder) Considers all points on the surface of the feature simultaneously Summary # The range of acceptable material positions is contained by the tolerance zone: 2D region for straightness/circularity 3D region for flatness/cylindricity Profile # Specify the allowable deviation of a line or surface from a perfect profile Can be bilateral (equal or unequal), or unilateral, shown above Can be specified with or without a datum reference Line profile tolerance example Any line trace along the profile and parallel to its edges must satisfy the tolerance Every point along a line tracing the surface of the toleranced feature must lie within a band whose width is given by the tolerance zone size. The tolerance is defined between two endpoints that are labeled using letters $X \\iff Y$ in the example below Can be (but does not need to be) referenced to one or more datums Unless otherwise stated, the tolerance band is equilateral around the nominal position of the line. Surface profile tolerance # All points on the indicated surface must lie within the tolerance zone, considered simultaneously\nProfile tolerances: with vs without datum # Yellow shows looser tolerance zone without datums; just says that the line\u0026rsquo;s/surface\u0026rsquo;s shape has to fit the tolerance \u0026ndash; not that that surface itself has to be constrained by it\u0026rsquo;s to fit with other parts Unilateral vs Bilateral # Orientation tolerances # While form tolerances dictate the required “quality” of a single feature, and profile tolerances may or may not be anchored to a particular datum or datums, orientation tolerances inherently require the use of datums, because they are about relating one feature to another. We need to establish a datum to represent one feature before we can determine how the second feature relates to it. Orientation tolerances are as follows.\nParallelism # Can be applied to surfaces and axes One of the features is assigned a datum letter, and the other feature is referenced to it Surface parallelism: when the non-datum feature is a plane, the tolerance zone tells us the maximum allowable distance between two planes which are: Parallel to the established datum As close together as possible Enclose all points on the referenced surface As with flatness, though, a parallelism tolerance makes no statement about how the distance between the two features should be controlled/where the tolerance zone lies That distance can be controlled by a separate dimensional tolerance as shown on the left side E.x Two surfaces could be parallel but the wrong distance apart Note that a parallelism tolerance is quite different from a flatness tolerance in that the planes being used to check a parallelism tolerance must lie parallel to a specified datum; this requirement does not exist for a flatness tolerance Axis # When the toleranced feature is an axis, we specify a cylindrical tolerance zone with a particular diameter — this tolerance zone is, however, constrained to be parallel to the datum, which differs from a simple straightness tolerance.\nEstablishing an axis datum # If the axis datum is the first datum being established: find best-fit center-line for points measured on the surface of the cylindrical feature (e.g. by CMM) Best-fit could be found by minimizing, e.g. sum of squared differences from surface points to axis If other datum(s) have been previously established: the axis datum is also constrained to be perpendicular to previous datum(s) Fitting process incorporates this constraint Checking an axis tolerance # Measure multiple points on the surface of the cylindrical manufactured feature Find the effective position of the axis at all points along its length Find \u0026ldquo;median position\u0026rdquo; between measured surface points Manufactured axis will not be perfectly straight Ask whether all points along the manufactured axis fit within a cylindrical tolerance zone of the specified diameter that has the prescribed relationship to the given datum: Tolerance zone may be parallel, perpendicular, concentric, etc. relative to datum Tolerance zone can take the most favorable possible position for meeting the tolerance Applies along the whole length of the toleranced feature Perpendicularity # Perpendicularity tolerances are very similar conceptually to parallelism tolerances, but the tolerance feature and datum lie at 90˚ to each other rather than parallel Can be applied to surfaces and axes Surface perpendicularity: all points must lie between two planes, separated by the tolerance value and perpendicular to the referenced datum Axes # Axis perpendicularity: all points on the axis must lie within a cylinder with the diameter of the tolerance zone and perpendicular to the referenced datum Angularity # Traditional (non-GD\u0026amp;T) approach Angular tolerance is specified as a number of degrees Tolerance zone is wedge- shaped GD\u0026amp;T approach \u0026ndash; requires that all points on the tolerance feature lie between two planes that are: At the specified angle from the given datum, and Separated by no more than the size of the tolerance zone An angularity tolerance in GD\u0026amp;T Note that an angularity tolerance of this kind is rather different from a “traditionally” defined angular tolerance, which would place a range on the effective angle between two features Note that an angularity tolerance of this kind is rather different from a “traditionally” defined angular tolerance, which would place a range on the effective angle between two features (as shown in the left above). In the traditional approach, the tolerance zone gets wider as the distance from the intersection point between the two features increases. Depending on what the component is supposed to do in use, one might argue that either the traditional approach or the GD\u0026amp;T approach would be the more useful way of tolerancing an angled feature. For example, if the component is intended to serve as a wedge to set the angle between two flat components, the traditional tolerancing approach might work best. On the other hand, if the component is supposed to be a mirror on to which a plane-wave of light is incident, we might rather use the GD\u0026amp;T approach so that the local angle of the surface is kept within certain bounds all the way along its length. The designer needs to think carefully about exactly what manufacturing defect is to be avoided\nLocation # Concentricity # This tolerance is useful, for example, when designing rotating machinery, in which multiple components may need to be placed on a shaft without resulting in out-of-balance loads. Concentricity relates two axes One datum axis and one toleranced cylindrical feature All points on the axis of the tolerance feature must lie within a cylindrical tolerance zone centered on the datum axis Symmetry # Relates two planar features All median points lying mid-way between the two surfaces of the toleranced feature must lie within the tolerance zone Tolerance zone is centered on the datum feature Tolerance zone defined by two planes In some components, one feature is intended to be centered relative to another, and a symmetry tolerance defines how far those two features can deviate from symmetry. Here when we say “feature” we refer to opposing pairs of surfaces with equal size — a particular type of feature known as a feature of size. The opposite sides of the slot above constitute a feature of size, as do the opposite outside faces of the component itself. As shown above, one feature is specified as a datum (by labeling one side of the feature with a lettered symbol). The other feature is labeled with its nominal dimension and also with a feature control frame that contains the symmetry symbol, the size of the tolerance zone, and the relevant datum. The tolerance zone tells us how far the midplane of the toleranced feature can deviate relative to the midplane of the datum feature (which would be established by measuring points on both sides of the feature — i.e., the height of the component in the example\nRunout # Runout tolerances differ from both circularity/cylindricity and concentricity tolerances in that they constrain peak-to-peak deviation of the surface of one feature when it is rotating about a datum axis. (No fitting of bounding circles or cylinders or of effective central axes is required.) Runout tolerances are thus helpful for tolerancing components that must rotate relative to other components while maintaining a certain amount of clearance — for example, a shaft turning within a cylindrical cavity. Runout is also important for ensuring cutting tool performance — in a milling tool or drill for example, limiting runout enables clean cuts to be made, as the tool will not periodically collide with the workpiece.\nCircular runout # Runout considers how the surface profile of an object varies as it rotates about a specified axis Useful for specifying performance of rotating machinery, cutting tools etc. Circular runout: Imagine placing a dial gage on the surface of a cylindrical feature Peak-to-peak variation of the reading as it rotates is circular runout Circular runout tolerance must not be exceeded at any location along the axis of the feature A circular runout tolerance limits the peak-to-peak deviation of points on any given circumference of a cylindrical feature, when it rotates about the specified datum axis. In the example shown on above, for example, circular runout could be measured by clamping the component in a chuck to establish datum A, then rotating the component with a dial gage in contact with the toleranced feature. The peak-to-peak movement of the indicator would need to be less than the tolerance specified. A circular runout tolerance must be satisfied independently at any circumference on the cylindrical feature to which it applies (not just the location at which the tolerance arrow appears to point).\nPlanar features # Can also apply to planar features perpendicular to specified datum axis Runout tolerance must not be exceeded at any radial location Cylindrical, or total, runout # Total runout is more restrictive than circular runout Imagine taking dial gage readings at many locations along length of feature Gage readings referenced to the rotation axis (axis = 0) Peak-to-peak variation of all readings taken together must not exceed tolerance value This is unlike circular runout, where each axial location is independent This is the “3D” version of circular runout: the datum is established, and the component rotated, and the total peak-to-peak variation across all axial positions is found (e.g. by scanning a dial gage along the component as it rotates). Total runout will usually be larger than circular runout; the example below illustrates why:\nTotal runout for planar features # Total runout can be applied to a planar feature perpendicular to the datum axis Tolerance value must not be exceeded when considering overall peak-to-peak positional variation of all radial locations together Often, a rotating component will be mounted at two locations (e.g. on two bearings at opposite ends of a shaft). Then, a runout tolerance will be specified relative to both of those datums using the convention illustrated below. The runout using datums A and B in this example is likely to be different from the runout that we would measure using only one of the datums, because the rotational axis established could change position as more points on the component’s surface are included in the fitting process. Multiple datums # It is common for rotating components to be held in bearings at both ends Associate a datum feature with each end of the component Rotation axis for evaluating runout can be based on these two datums together Fit the axis to points measured along both datums Reflects operation of component more accurately Range of GD\u0026amp;T symbols # Examples # Perpendicularity # Add perpendicularity tolerances to show: 40-high left-hand edge, perpendicular to datum A, tolerance zone 0.2 in size Hole axis perpendicular to datum A, tolerance zone 0.2 dia at MMC Center Position # Symmetry # "},{"id":32,"href":"/eecs-16a/11/","title":"11: Locationing \u0026 Trilateration","section":"EECS 16A","content":" \\(\\) Positioning Systems # Just read Note 21.1-2\nVectors # Inner Products # The Euclidean inner product between two vectors $\\vec u, \\vec v$ is defined as\u0026hellip; $$\\langle \\vec u, \\vec v \\rangle \\equiv \\vec u \\cdot \\vec v \\equiv \\vec u^T \\vec v$$ $$\\dots = \\begin{bmatrix} v_1 v_2 \\dots v_n \\end{bmatrix}\\begin{bmatrix} u_1\\\\ u_2\\\\ \\vdots \\\\ u_n \\end{bmatrix}$$ $$\\dots = v_1 u_1 + v_2 u_2 + \\dots v_n u_n$$ $$\\dots = \\sum_{i=1}^n v_i u_i$$ Naming In physics, the inner product is called dot product; $\\vec u \\cdot \\vec v$ Scalar product is also occasionally used because it produces a scalar output Properties # In general, if $\\mathbb{V}$ is a real vector space$^{[1]}$, we say that the mapping $\\vec u, \\vec v \\in \\mathbb{V} \\to \\langle \\vec u, \\vec v \\rangle \\in \\mathbb{R}$ is an inner product iff it satisfies the following properties:\n1. Symmetry: $\\langle \\vec u, \\vec v \\rangle = \\langle \\vec v, \\vec u \\rangle$ for all $\\vec u, \\vec v \\in \\mathbb{V}$ # 2. Linearity: # Homogeneity: $\\langle \\gamma \\vec u, \\vec v \\rangle = \\gamma \\langle \\vec u, \\vec v \\rangle$ Super position: $\\langle \\vec u + \\vec w, \\vec v \\rangle = \\langle \\vec u, \\vec v \\rangle + \\langle \\vec w, \\vec v \\rangle$ for $\\vec w \\in \\mathbb{V}, \\gamma \\in \\mathbb{R}$ 3. Positive-definiteness: $\\langle \\vec v, \\vec v \\rangle \\geq 0$ with equality iff $\\vec v = \\vec 0$ # $^{[1]}$In 16A we do not want to think about complex numbers so we tend to have $\\vec u, \\vec v \\in \\mathbb{R}^n$\nGeometric Interpretation # The inner product of vectors two vectors is their magnitudes multiplied by the angle between them $$\\langle \\vec u, \\vec v \\rangle = \\Vert \\vec u \\Vert \\Vert \\vec v \\Vert \\cdot \\cos \\theta$$\nProof on page 6 The inner product does not depend on the coordinate system the vectors are in\u0026ndash; it only depends on the relative angle between these vectors and their length. Note that we cannot just look at the value of the inner product $\\vec u, \\vec v$ in making the judgement about the vectors’ similarity, due to the scaling property Making the vectors 10 times longer/shorter doesn’t make them any more or less aligned with each other! What we can do is normalize the vectors \u0026ndash; maintain direction and set magnitude to 1 E.x. $\\hat v = \\frac{\\vec v}{\\Vert \\vec v \\Vert}$ \u0026ndash; the $\\hat{ }$ symbol indicates a unit vector (magnitude = 1) Nice derivation on page 108 Special Operations # Motivation: The act of computing an inner product is very simple computationally; it’s just a few additions and multiplications, so computers are highly optimized for computing inner products. Therefore, it is useful to represent other common operations in terms of inner products.\nFor $\\vec v \\in \\mathbb{R}^n$\nSum of Components: $\\langle \\vec 1, \\vec v \\rangle = v_0 + v_1 + \\dots + v_n$ Average: $\\langle \\hat n^{-1}, \\vec v \\rangle = \\frac{v_0}{\\hat n} + \\frac{v_1}{\\hat n} + \\dots + \\frac{v_n}{\\hat n} = \\frac{v_0 + \\dots + v_n}{n}$ Sum of Squares: $\\langle \\vec v, \\vec v \\rangle = v_0^2 + v_1^2 + \\dots + v_n^2$ Selective Sum: $\\langle \\vec e_2 + \\vec e_5 + \\dots + e_n, \\vec v \\rangle = v_2 + v_5 + \\dots v_n$ Orthogonal Vectors # Two vectors $\\vec v, \\vec u \\in \\mathbb{R}^n$ are orthogonal if their inner product is zero, i.e. $\\langle \\vec v, \\vec u \\rangle = 0$\nOrthogonality can be thought of perpendicularity to higher dimensions than two/three Note that the standard unit vectors are always orthogonal to each other. E.x. orthogonal vectors in $\\mathbb{R}^2$ to the right: Norms # The Euclidean Norm (or 2-norm) of a vector represents the vector’s length, or magnitude. Defined as $\\Vert \\vec v \\Vert = \\sqrt{v_1^2 + \\dots v_n^2} = \\sqrt{\\langle \\vec v, \\vec v \\rangle}$ where the inner product on the right is the Euclidean inner product\nThe norm of a vector is the magnitude This corresponds to the usual notion of distance in $\\mathbb{R}^2, \\mathbb{R}^3$ The set of points with equal Euclidean norm is a circle in $\\mathbb{R}^2$ or a sphere in $\\mathbb{R}^3$ Aside: More generally, any choice of inner product will give rise to a corresponding norm via defining $\\Vert \\vec v \\Vert := \\sqrt{ \\langle \\vec v, \\vec v \\rangle}$ Properties # For $\\vec v, u \\in \\mathbb{R}^2$\u0026hellip;\n1. Non-negativity: $\\Vert \\vec v \\Vert \\geq 0$ # 2. Zero Vector: $\\Vert \\vec v \\Vert = 0 \\iff \\vec v = \\vec 0$ # 3. Scalar Multiplication: $\\Vert \\gamma \\vec v \\Vert = \\vert \\gamma \\vert \\Vert \\vec v \\Vert$ # 4. Triangle Inequality: $\\Vert \\vec v + \\vec u \\Vert \\leq \\Vert \\vec v \\Vert + \\Vert \\vec u \\Vert$ # Cauchy-Schwarz Inequality # $$\\vert \\langle \\vec v, \\vec u \\rangle \\vert \\leq \\Vert \\vec v \\Vert \\Vert \\vec u \\Vert$$\nWe can prove this knowing $\\vert \\cos \\theta \\vert \\leq 1$:\n$$\\vert \\langle \\vec v, \\vec u \\rangle \\vert = \\big\\vert \\Vert \\vec u \\Vert \\Vert \\vec u \\Vert \\cdot \\cos \\theta \\big\\vert $$ $$\\vert \\langle \\vec v, \\vec u \\rangle \\vert = \\Vert \\vec u \\Vert \\Vert \\vec u \\Vert \\cdot \\big\\vert\\cos \\theta \\big\\vert $$ $$\\therefore \\vert \\langle \\vec v, \\vec u \\rangle \\vert \\leq \\Vert \\vec u \\Vert \\Vert \\vec u \\Vert $$ Projections # The vector projection of $\\vec x$ onto $\\vec y$ \u0026mdash; $\\text{proj}_{\\vec y}\\vec x$ \u0026mdash; refers to the component of $\\vec x$ that is aligned in the same direction as $\\vec y$\u0026ndash; or exactly opposite; it helps to imagine a line going through $\\vec y$ By definition, we see that $\\text{proj}_{\\vec y}\\vec x \\in \\text{Span} \\{\\vec y \\}$ If the projection is zero, then the vectors are orthogonal \u0026ndash; there is no component of $\\vec y$ that is aligned with $\\vec x$ We can break up $\\vec x$ into it\u0026rsquo;s parallel and perpendicular components to $\\vec y$, $\\vec x_\\parallel, \\vec x_\\perp$ The length of the parallel component $\\Vert \\vec x_\\parallel \\Vert$ gives the scalar projection of $\\vec x$ onto $\\vec y$ $$\\cos \\theta = \\frac{\\langle \\vec x, \\vec y \\rangle}{\\Vert \\vec x \\Vert \\Vert \\vec y \\Vert}$$ $$\\therefore \\text{comp}_{\\vec{y}} \\vec x = \\Vert \\vec x_\\parallel \\Vert = \\cos\\theta \\Vert \\vec x \\Vert$$ $$\\dots = \\frac{\\langle \\vec x, \\vec y\\rangle}{\\Vert \\vec y \\Vert} $$ This is the scalar projection, which we can give direction by multiplying it by $\\hat y$ $$\\text{proj}_{\\vec y}\\vec x = ( \\text{comp}_{\\vec{y}} \\vec x)\\hat y = \\left( \\frac{\\langle \\vec x, \\vec y\\rangle}{\\Vert \\vec y \\Vert}\\right) \\frac{\\vec y}{\\Vert \\vec y \\Vert} = \\frac{\\langle \\vec x, \\vec y\\rangle}{\\Vert \\vec y \\Vert ^2} \\vec y = \\frac{\\langle \\vec x, \\vec y\\rangle}{\\langle \\vec y, \\vec y\\rangle} \\vec y$$\nThis new, unique vector is closest to $\\vec x$ as measured by the norm \u0026ndash; page 9 The above only uses generic properties of the inner product, and does not need to the specific choice of the Euclidean inner product to work out. See also: Math 53 12.3: Projections Trilateration # I\u0026rsquo;m on a journey to the center of three We know the locations of the beacons $\\vec a_1,\\vec a_2,\\vec a_3$ We know the distances $d_1,d_2,d_3$ We want to find the position of $\\vec x$ These two equations are linear in $\\vec x$, so we can then stick them into a matrix This system can be solved for a location Notice that three circles uniquely define a point in 2D; this argument extends in 3D to four spheres, 4D to five hyper-spheres, etc. $$\\Vert \\vec x - \\vec a_1 \\Vert^2 = d_1^2$$ $$\\Vert \\vec x - \\vec a_2 \\Vert^2 = d_2^2$$ $$\\Vert \\vec x - \\vec a_3 \\Vert^2 = d_3^2$$ $$\\equiv$$ $$(\\vec x - \\vec a_i)^T (\\vec x - \\vec a_i) = d_i^2$$ $$\\equiv$$ $$\\vec x^T \\vec x - \\vec x^T\\vec a_i -\\vec a_i^T\\vec x + \\vec a_i^T \\vec a_i - = d_i^2$$ $$\\equiv$$ $$\\Vert \\vec x \\Vert^2 - 2 \\langle \\vec x, \\vec a_i \\rangle + \\Vert \\vec a_i \\Vert^2 = d_i^2$$\nWe have squared terms (not linear!) involving unknown variable $\\vec x$\nWe\u0026rsquo;ll subtract equation 1 from equation 2, and separately again from equation 3. $$2 (\\vec a_1 - \\vec a_2)^T \\vec x = \\Vert \\vec a_1 \\Vert^2 - \\Vert \\vec a_2 \\Vert^2 - d_1^2 + d_2^2$$ $$2 (\\vec a_1 - \\vec a_3)^T \\vec x = \\Vert \\vec a_1 \\Vert^2 - \\Vert \\vec a_3 \\Vert^2 - d_1^2 + d_3^2$$ $$\\equiv$$ $$\\begin{bmatrix} 2 (\\vec a_1 - \\vec a_2)^T \\\\ 2 (\\vec a_1 - \\vec a_3)^T \\\\ \\end{bmatrix}\\vec x = \\begin{bmatrix} \\Vert \\vec a_1 \\Vert^2 - \\Vert \\vec a_2 \\Vert^2 - d_1^2 + d_2^2 \\\\ \\Vert \\vec a_1 \\Vert^2 - \\Vert \\vec a_3 \\Vert^2 - d_1^2 + d_3^2 \\\\ \\end{bmatrix}$$\nSignals # Signal: a message that contains information as a function of, in this class, time \u0026ndash; can also be a function of space (i.e. an image)\nDiscrete-time Signal: defined at specific points in time (for example, every minute) \u0026ndash; can represent as a list of numbers Continuous-Time Signal: defined over all time \u0026ndash; not focused on in 16A We can represent a discrete-time signal as a list of numbers, and thus as a vector where each element is the value at a single time point Every element of the vector represents the signal value at one timestep. We’ll use the notation $s[k]$ to represent the $k$-th element of the vector where initial element is at $k = 0$. E.x. in the signal $\\vec s$ above, $s[0] = 0$, $s[1] = 1$, etc.\nCross-Correlation # Cross-correlation: Measure of the similarity between two signals $\\vec x$ and $\\vec y$ based on the inner product.\n$$\\text{corr}_{\\vec x} ( \\vec y) [k] = \\sum^{\\infty}_{i=-\\infty} x[i] y[i−k]$$ $$\\dots = \\left\\langle \\vec x, \\vec y^{(k)} \\right\\rangle$$\n$\\vec x, \\vec y$: input signals $\\text{corr} _{\\vec x} ( \\vec y) [k]$: $k$-th element of their cross-correlation We typically iterate over $[-l, L+l]$ $L, l$ The larger and smaller signal lengths, respectively $x[n], y[n] = 0$ for any $n$ that they are not defined for When the inner product is large, $\\vec x, \\vec y$ are more similar, and when it’s small, they are more different. The cross-correlation checks the inner product at all relative shifts between the signals, so it tells us how similar two signals are at every shift. Autocorrelation: a correlation between a signal and itself Tells us how similar a signal is to all shifts of itself. Not commutative: $\\text{corr}_{\\vec x} ( \\vec y) \\neq \\text{corr}_{\\vec y} ( \\vec x)$ Aside: convolution is an operation similar to correlation but commutative Convolutional Neural Network Aside: Circular Correlation E.x. for $\\vec s_1 = \\begin{bmatrix}1 \\ 3 \\ 2\\end{bmatrix}^T$, $\\vec s_2 = \\begin{bmatrix}2 \\ 1\\end{bmatrix}^T$: $\\text{corr}_{\\vec s_1} ( \\vec s_2) [0] = (1)(2) + (3)(1) + (2)(0) = 5$ Using numpy: numpy.correlate([1, 3, 2], [2, 1], ‘full’) "},{"id":33,"href":"/cogsci-c100/mem-topics/","title":"11: Memory Topics","section":"CogSci C100","content":" Neurological Mind Reading # Team of neuroscientists at Carnegie Mellon led by Marcel Just are learning to read people’s minds based on fMRI cortical activation patterns\nIn early study, researchers were able to identify which of 10 similar objects (e.g., hammer vs. drill vs. screwdriver) people were viewing based on scans (Shinkareva, Mason, Malave et al., 2008) Lab has now also identified activation patterns associated with (Wang, Cherkassky, \u0026amp; Just, 2017) Different abstract ideas (e.g., forgiveness vs. gossip vs. spirituality) Thinking in different languages Different emotions Participant is asked to think of scenario that would conjure up disgust, envy, etc. (e.g., someone vomiting at baseball game, beautiful model) Computer is able to identify the emotion fMRI cortical activation patterns can now also be used to distinguish between participants with autism and controls with 97% accuracy (Just, Cherkassky, Buchweitz et al., 2014) When those with autism were asked to think of social interactions like adore, hug, humiliate, they showed significantly less activation in areas associated with the self than controls Thought of words more like definitions, rather than experiences Patients with suicidal ideation were asked to think about death related words (e.g., funeral, death) and positive words (e.g., praise, good, carefree) Compared with non-suicidal controls, self-related brain regions showed significantly more activation for death-related words and less activation for positive words (Just, Pan, Cherkassky et al., 2017) The Online Brain: Effects of the Internet on Cognition # 45% of US teens report that they are online \u0026ldquo;almost constantly\u0026rdquo; (Pew Research Center, 2018) According to review article published in World Psychiatry, research suggests that this increase in Internet use may be\u0026hellip; (Firth, Torous, Stubbs et al., 2019) Impairing our ability to sustain attention Negatively impacting our memory and analytical thinking ability Promoting a type of social comparison that increases risk of depression and mental health problems, especially among adolescents Due to constant social comparison Attention and impulse control # Studies have shown that adopting a less physically and cognitively active lifestyle across the lifespan may accelerate loss of cognitive function Similar to Alzheimer\u0026rsquo;s prevention Emerging evidence indicates that disengaging from the \u0026ldquo;real world\u0026rdquo; in favor of virtual settings may similarly induce adverse neurocognitive changes Ex: Adult participants (both gamers and non-gamers) engaged in an online role playing game daily for 6 weeks (55 hours total on average) Showed significant reductions in gray matter within the orbitofrontal cortex relative to matched controls (Zhou, Montag, Sariyska et al., 2019) The OFC is a brain region important in executive function, impulse control, and decision making In addition, leading technology companies have been accused of intentionally capitalizing on the addictive potential of the Internet without concern for user well-being They study, test, and refine attention-grabbing aspects of their websites and apps to promote extremely high levels of use Tech companies try to maximize the amount of time you spend on your phone/computer because that is how they make their money You don’t pay for Facebook; ads do The more time you spend on Facebook, the more money they make from ads Ad spending on social media has reached 40 billion dollars a year So\u0026hellip; computer scientists study huge data sets extensively to figure out how to get you to spend more time online Example: Holding some \u0026ldquo;likes\u0026rdquo; back for you to give you big burst later at exactly the right moment (based on complex algorithmic analyses) to effect \u0026ldquo;improvement in behavior\u0026rdquo; Similar to gambling \u0026ndash; you never know which roll (picking up the phone) will result in a rush, so you do it often The typical person checks their phone every 15 minutes, and half the time there is no sort of alert\u0026hellip; because checking the phone results in Release of dopamine (associated with pleasure and \u0026ldquo;rewards\u0026rdquo;) Reduction in cortisol (anxiety from not having checked phone in a while results in increased cortisol) In addition, the intermittent reinforcement (which is what makes gambling so addictive) inherent to device-checking further perpetuates these compulsive behaviors Cognitive consequences of the attention-grabbing Internet As early as 2012, researchers found that 85% of teachers endorsed the statement that \u0026ldquo;today\u0026rsquo;s digital technologies are creating an easily distracted generation\u0026rdquo; (Pew Research Center, 2012) Study of individuals who engaged in \u0026ldquo;heavy\u0026rdquo; (i.e., frequent and extensive) media multi-tasking compared to those who did not Cognitive testing of the two groups found that, surprisingly, those involved in heavy media multi-tasking performed worse in task-switching tests than their counterparts Closer inspection of data indicated that this was due increased susceptibility to distraction from irrelevant environmental stimuli (Ophir, Nass, \u0026amp; Wagner, 2009) Though some studies have failed to find adverse effects of internet use on attention, overall, the literature does seem to indicate that those who engage in frequent and extensive media multi-tasking perform worse in various cognitive tasks, particularly for sustained attention (Uncapher \u0026amp; Wagner, 2018) Lastly, the first longitudinal study (3-6 months) of media multi-tasking in young people found that frequent multi-tasking behaviors Predicted the development of attentional deficits in early adolescents (11-13), though not in older teens (14-16) (Baumgartner, van der Schuur, Lemmens et al., 2017) Neuroimaging research may account for these cognitive deficits Heavy media multi-taskers require greater cognitive effort (increased activation of right prefrontal) to maintain concentration when faced with distractor stimuli (Moisala, Salmela, Hietajärvi et al., 2016) Structurally, high levels of Internet usage and heavy media multi-tasking are associated with decreased gray matter in prefrontal regions associated with maintaining goals in face of distraction (e.g., right frontal and anterior cingulate cortex) (Kühn, Gallinat, \u0026amp; Brains, 2015; Loh \u0026amp; Kanai, 2014) Even short-term engagement with an extensively hyperlinked online environment (e.g., online shopping for 15 minutes) reduces attentional scope for a sustained duration after coming offline Reading a magazine does not produce these deficits (Peng, Chen, Zhao et al., 2018) Memory # Research has shown that the ability to access information online causes people to become more likely to remember where these facts could be retrieved rather than the facts themselves Although storage of information online is beneficial at a group level, using this type of externally stored or \u0026ldquo;transactive memory\u0026rdquo; reduces an individual’s ability to recall the specifics of the externally stored information (Sparrow, Liu, \u0026amp; Wegner, 2011) In addition, there is evidence that this is impairing people’s ability for analytical or critical thinking If you know where to find the information (online) rather than the information itself, you\u0026rsquo;ll be worse off Research has shown that analytical thinkers, with higher cognitive capacities, actually use their smartphone less for transactive memory in day-to-day situations compared to individuals with non-analytical thinking styles (Barr, Pennycook, Stolz et al., 2015) May this in part explain the dramatic rise in popularity of conspiracy theories?! Also, studies have found that increased reliance on the internet for information may cause individuals to \u0026ldquo;blur the lines\u0026rdquo; between their own capabilities and their devices’ Result is an over-inflation of self-perceived knowledge (Hamilton \u0026amp; Yao, 2018; Fisher, Goddu, \u0026amp; Keil, 2015) Social cognitive effects # Social media companies capitalize on people’s innate desire for positive feedback from others to maximally engage (and \u0026ldquo;addict\u0026rdquo;) users (e.g., via \u0026ldquo;friends,\u0026rdquo; \u0026ldquo;followers,\u0026rdquo; and \u0026ldquo;likes\u0026rdquo;) However, growing evidence indicates that relying on online feedback for self-esteem can have adverse effects on young people, increasing anxiety and depression (Vannucci, Flannery, \u0026amp; Ohannessian, 2017; Lin, Sidani, Shensa et al., 2016) In addition, the tendency to make upward social comparisons can be hijacked by the artificial environment manufactured on social media \u0026ndash; no one really has a \u0026ldquo;Facebook life\u0026rdquo;! This may lead to unrealistic expectations of oneself, poor body image, and negative self-concept, especially in younger people Among adolescents (particularly females), those who spent more time on social media and smartphones have a greater prevalence of mental health problems, including depression (Twenge, Joiner, Rogers et al., 2017) Compared with those who spend 1 hour/day, those who spend more than 5 hours/day had a 66% increased risk of a suicide-related outcome (Twenge, Joiner, Martin et al., 2018) On the other hand though, the Internet may also potentially provide mental health benefits As we will see later, AI programs have also been developed that can analyze Facebook or Instagram posts to effectively screen for depression and suicide risk (Eichstaedt, Smith, Merchant et al., 2018; Reece \u0026amp; Danforth, 2017) Many types of online \u0026ldquo;virtual therapies\u0026rdquo; for mental illness are also being developed Age Factor # As discussed above, adverse attentional effects of digital multi-tasking are particularly pronounced in early adolescence (even compared to older teens) In addition, higher frequency of internet use over 3 years in children has been linked to Decreased verbal intelligence at follow-up and Impeded maturation of both gray and white matter regions (Takeuchi, Taki, Asano et al. , 2018) On the other hand, for older adults experiencing cognitive decline, Internet use may provide cognitive benefits The online environment may provide a new source of positive cognitive stimulation Studies have found that computer games available online and through smartphones can be used to attenuate aging-related cognitive decline (Kühn, Gleich, Lorenz et al., 2014; Anguera, Boccanfuso, Rintoul et al, 2013) In addition, older adults may potentially be able to use social media to overcome isolation This in turn enables them to reap the physical, mental and neurocognitive benefits associated with social connection (Wellman B., 2001) "},{"id":34,"href":"/eecs-16a/12/","title":"12-13: Least Squares \u0026 ML","section":"EECS 16A","content":" \\(\\) Least Squares # Motivation # We want to find an approximate solution \u0026ndash; one that satisfies all the given equations/information as closely as possible to\u0026hellip; Minimize the impact of noise/errors Solve overdetermined systems \u0026ndash; we\u0026rsquo;ll often be collecting abundant information, thus we have more equations than unknowns Applications: Least squares is the fundamental idea behind data fitting and machine learning In data (curve) fitting, we find lines or curves that best match the data In machine learning, we use a best-fit curve to make predictions about new, unseen data. What # The goal of least squares is to minimize the error vector\u0026rsquo;s magnitude (norm) \u0026ndash; the square of each of it\u0026rsquo;s components: $$\\Vert \\vec e \\Vert = \\sqrt{\\sum_{i=1}^n e_i^2 }$$ With $m$ measurements (equations) and $n$ unknowns; $m \u0026gt; n$ $\\text{col}(A)$ is an $n$-dimensional subspace within the larger $m$-dimensional space that $\\vec b$ lies in That is, $\\vec b$ cannot be exactly reached by $A\\vec x$ $\\vec x$: Best-guess that minimizes $\\Vert \\vec e \\Vert$ $\\vec x \\in \\mathbb{R}^{n}$ $\\vec e = \\vec b - A\\vec x = \\vec b - \\hat x$: Error vector $\\vec e \\in \\mathbb{R}^{n}$ We want to find $\\vec x$ where $\\vec e \\perp \\text{col}(A)$ $\\vec b$: Actual, observed values $\\vec b \\in \\mathbb{R}^{m}$ $\\hat x = A \\vec x = A (A^T A)^{-1} A^T \\vec b$: Predicted values $A \\in \\mathbb{R}^{m \\times n}; A\\vec x, \\hat x \\in \\mathbb{R}^{m}$ Can be thought of a linear combination of the columns of $A$, with $\\vec x$ acting as weights Proof # We know from the last section that the projection of $\\vec a$ onto $\\vec b$ results in a vector within the span of $\\vec b$ that is closest and orthogonal to $\\vec a$ So we\u0026rsquo;ll project $\\vec b$ onto $\\text{col}(A)$ to find the smallest possible $\\vec e$ That is, we know that $\\vec e \\perp \\text{col}(A)$ $\\Longrightarrow \\vec e \\perp \\vec a_i $ for each column of $A$ so we can write: $$\\langle \\vec a_i, \\vec e \\rangle = 0$$ $$\\langle \\vec a_i, \\vec b - A\\vec x \\rangle = 0$$ $$\\vec a_i^T (\\vec b - A\\vec x) = 0$$ $$\\therefore A^T (\\vec b - A \\vec x) = \\vec 0$$ Then solving for $\\vec x$ in terms of the known $A$ and $\\vec b$: $$A^T (\\vec b - A \\vec x) = \\vec 0$$ $$A^T \\vec b - A^T A \\vec x = \\vec 0$$ $$A^T \\vec b = A^T A \\vec x$$ $$ \\therefore \\vec x = (A^T A )^{-1} A^T \\vec b$$ Theorems # Theorem 23.1 # Vector $\\vec e$ is orthogonal to every vector in the column space of $A$ iff it is orthogonal to each of the columns $\\vec a_i$ that form the basis of its column space\nIf $\\vec e$ is orthogonal to every vector in the column space of $A$, then it is orthogonal to each of the $\\vec a_i$, as each of the $\\vec a_i$ are in the column space of $A$ too Now, we will try to prove the converse: consider an arbitrary vector $\\vec v \\in \\text{span}(A)$\u0026ndash; by definition, we know that there exist coefficients $\\alpha_i$ such that we can express $\\vec v = \\sum_{i=1}^m \\alpha_i \\vec a_i$ We also know that $\\vec e$ is orthogonal to each $\\vec a_i$; that is $\\langle \\vec e, \\vec a_i \\rangle = 0$ We wish to show that $\\vec{e}$ is orthogonal to $\\vec{v}$ too: $$\\langle\\vec{e},\\vec{v}_i \\rangle = \\left\\langle \\vec{e}, \\sum_{i=1}^m \\alpha_i \\vec a_i \\right\\rangle$$ $$\\dots = \\sum \\left\\langle \\vec{e}, \\alpha_i \\vec a_i \\right\\rangle$$ $$\\dots = \\sum \\alpha_i \\left\\langle \\vec{e}, \\vec a_i \\right\\rangle$$ $$\\dots = 0$$ That is, if $\\vec e$ is orthogonal to all the basis vectors of a subspace, it is orthogonal to every vector in that subspace as well! Theorem 23.2 # $\\text{Null}(A^TA) = \\text{Null}(A)$ even when $A$ has a nontrivial nullspace\nConsider an arbitrary $\\vec v \\in \\text{Null}(A^T A)$\u0026ndash; by definition, we have $$A^T A \\vec v = \\vec 0$$ $$\\vec v^T (A^T A \\vec v) = \\vec v^T (\\vec 0)$$ $$(A \\vec v)^T (A \\vec v) = 0$$ $$\\langle A \\vec v, A \\vec v \\rangle = \\Vert A\\vec v \\Vert ^2 = 0$$ Thus, it is clear that $A \\vec v = 0$, so $\\vec v \\in \\text{Null}(A)$ That is, $\\text{Null}(A^T A) \\subseteq \\text{Null}(A)$ Consider an arbitrary vector $\\vec v\u0026rsquo; \\in \\text{Null}(A)$\u0026ndash; pre-multiplying by $A^T$ we have that\u0026hellip; $$A \\vec v\u0026rsquo; = \\vec 0$$ $$A^T A \\vec v\u0026rsquo; = \\vec 0$$ Therefore $\\vec v\u0026rsquo; \\in \\text{Null}(A^T A)$ That is, $\\text{Null}(A) \\subseteq \\text{Null}(A^T A)$ Combining this with the prior statement, we find our desired $\\text{Null}(A) = \\text{Null}(A^T A)$ "},{"id":35,"href":"/cogsci-c100/imagery/","title":"12: Imagery","section":"CogSci C100","content":" Controversies in Cognitive Science # How is information stored?\nIn analog code (i.e., as a pictorial representation) OR As a propositional code (i.e., descriptive representation)? Mental imagery # How Are Mental Representations Stored? # Experiments on mental imagery by Roger Shepard and Jacqueline Metzler in early 1970s spawned the imagery debate Suggested that some types of cognitive information processing involve forms of representation that are very different from how information is represented in, and manipulated by, a digital computer Imagery and rotation studies: Rotate each object on the left to see if it matches the object on the right: Amount of time it takes to rotate a mental image depends on the extent of the rotation One feature of digitally encoded information is that the length of time it takes to process a piece of information is typically a function only of the quantity of information (the number of bits that are required to encode it)\nThe particular information that is encoded ought not to matter But what the mental rotation experiments show is that there are informationprocessing tasks that take varying amounts of time even though the quantity of information remains the same\nThis suggests that mental rotation tasks tap into ways of encoding information that are very different from how information is encoded in a digital computer More specifically, the information may be encoded in pictorial form, similar to the way a map represents a geographical region (analog code), rather than as a description (propositional code) So which viewpoint is correct: analog or propositional?\nResearch support for analog code (pictorial representation) # Imagery and size: Condition #1: Imagine a rabbit standing next to an elephant Condition #2: Imagine a rabbit standing next to a fly \u0026ldquo;Does the rabbit have two front paws?\u0026rdquo; People make faster judgments about the characteristics of large mental images than of small mental images; also, they take longer to travel a large mental distance, whether that’s visual or auditory Imagery and interference: Create a clear mental image of a friend’s face Keeping that image in mind, simultaneously let your eyes wander over the scene in front of you Visual imagery activates about 70-90% of the same brain regions that are activated during visual perception Imagery and neuroimaging research: The primary visual cortex is activated when people work on tasks that require visual imagery Visual imagery may interfere with visual perception, and motor imagery with motor images (Kosslyn, Ganis, \u0026amp; Thompson, 2010) Similar findings have been reported for auditory and motor imagery People with prosopagnosia cannot create a mental image of a face Research support for propositional code (descriptive representation) # Imagery and parts of figures: Form a clear mental image of the figure to the right: Without glancing back at the figure in the previous slide, consult your mental image. Does that mental image contain a parallelogram? People have difficulty identifying that a part belongs to a whole if they have not included the part in their original verbal description of the whole\nImagery and ambiguous figures: Create a clear mental image of the right figure: Write down what the figure in the previous slide depicted. Then give a second, different interpretation of the figure you saw Some ambiguous figures are difficult to reinterpret in a mental image\nSo which viewpoint is correct: analog or propositional? The majority of research supports the analog viewpoint, but some people on some tasks use a propositional code In one study, researchers administered questionnaire to assess whether participants were more visualizers or verbalizers Magnetoencephalography (MEG) found that visualizers showed more activation in occipital region when visualizing famous landmark Verbalizers showed more activation in areas associated with linguistic processing (Nishimura, Aoki, Inagawa et al., 2016) Neurological Disorder of Visual Imagery # Aphantasia: Inability to visualize\nMay be congenital or acquired (e.g., brain injury) Some researchers say 2-3% of population may be affected, and many may be unaware of the condition \u0026ldquo;I thought counting sheep was a metaphor, just like bee’s knees or cat’s pajamas\u0026rdquo; May be unable to picture loved one’s face or remember directions; even telling someone how their day was can be difficult because they can’t visualize it \u0026ldquo;It is hard not to feel like a sociopath when you’re lying about how you spent your Monday and you don’t even know why.\u0026rdquo; When looking at and naming faces, normal regions of brain are active, but when asked to picture people’s faces, facial-recognition brain regions do not become active like they normally would Aphantasia characterizes only voluntary visualizations; the aphantasiacs were still able to have involuntary visualizations (i.e., dreams) Suggests that condition may involve neurological deficits between frontal and visual cortex Imagery Rehearsal # Imagery rehearsal/mental practice: rehearsing a task mentally without observable movement in order to learn it\nMuch of the research on visualization or imagery rehearsal/mental practice has focused on skills acquisition and performance enhancement in sports and in surgery (Arora, Aggarwal, Sirimanna et al., 2011) Has also been used to improve musical/dramatic performance, etc. May also improve academic performance Middle school students whose families struggle financially are more likely to earn high grades if they have a clear vision of themselves succeeding in school (Duckworth, 2016)\nUse in sports and other types of skills training # Many famous athletes have attributed their success to imagery rehearsal, and there is extensive empirical support for its effectiveness Imagery rehearsal can be used not only to perfect routines, but also to work on acquiring flexibility of action in the face of unusual or stressful situations The same neurological circuits are activated and the same type of LTP takes place during imagery rehearsal as during actual physical practice(Avanzino, Gueugneau, Bisio et al., 2015; Filgueiras, Conde, \u0026amp; Hall, 2017) Mental practice has become a standard part of training for Olympic athletes (Blumenstein \u0026amp; Orbach, 2012; Ungerleider, 2005) Ex: University of Tennessee women’s basketball team used imagery rehearsal to increase free-throw accuracy 52% in games following standard physical practice 65% after mental practice (Savoy \u0026amp; Beitel, 1996) Wound up winning the national championship game Players repeatedly imagined making free throws under various conditions, including being \u0026ldquo;trash-talked\u0026rdquo; by their opposition To be effective, imagery rehearsal should be a multisensory endeavor, which is why the term \u0026ldquo;imagery\u0026rdquo; is now often preferred to \u0026ldquo;visualization\u0026rdquo; Visualization is also used in evidencebased psychotherapies, such as Cognitive Behavioral Therapy and Stress Inoculation Therapy, which is used to help clients prepare themselves to handle stressful events \u0026ldquo;To feel cheerful, sit up cheerfully, look around cheerfully, and act as if cheerfulness were already there.\u0026rdquo; \u0026ndash; William James\nFactors influencing effectiveness of visualization/imagery rehearsal # Imagery rehearsal is more effective if preceded by relaxation Imagery rehearsal is more effective when one fully experiences the task, engaging all the senses, rather than merely \u0026ldquo;thinking about\u0026rdquo; or watching oneself perform the task The key is to make the sensory experience as vivid and real as possible and to focus on the feelings evoked by the scene, as well as the various details, e.g., how other people are responding to you, etc. Spreading out imagery rehearsal sessions over a period of time is more effective than massing them Experienced performers profit more from imagery rehearsal; beginners profit more from actual physical practice Mental practice works better if the task is easy or well-practiced (Richard Suinn) Study found that imagining yourself studying (and refusing invitations to party instead) is more effective than simply visualizing yourself getting an \u0026ldquo;A\u0026rdquo; on the test! (Taylor, Pham, Rivkin et al., 1998) Possible mechanisms of imagery rehearsal # Neurobiological mechanism: The same neurological circuits are activated and the same type of LTP takes place during imagery rehearsal as during actual physical practice (Avanzino, Gueugneau, Bisio et al., 2015; Filgueiras, Conde, \u0026amp; Hall, 2017) Possible psychological mechanism: Visualization probably works in part by strengthening optimism, which in turn strengthens persistence and effectiveness \u0026ldquo;Belief must strengthen the imagination for imagination establishes the will.\u0026rdquo; \u0026ndash; Paracelsus\nThe act of creating a concrete mental image in which we see ourselves as happy or successful reinforces our intentions to behave in ways that help us achieve the image in our heads \u0026ldquo;Images of desirable future events tend to foster the behavior most likely to bring about their realizations\u0026rdquo;\nOther applications of mental imagery # In psychotherapy # Cognitive therapy: cognitive rehearsal or practicing a role under conditions that are supposed to represent the real situation\nClient is asked to imagine going through all the steps necessary to obtain a goal Case of a depressed woman who had intended to take an exercise class the following day When therapist encouraged her to use imagery rehearsal, she realized that she lacked the proper clothes, might not have access to the car, etc. Mental imagery can be a way of seeing and testing radically new ways of behaving and reacting Stress Inoculation Therapy: used to help clients prepare themselves to handle stressful events Imagery Rehearsal Therapy (IRT) for treatment of nightmares\nRehearsal redreaming: Research indicates that nightmares can be effectively treated using simple rehearsal of the nightmare and/or rehearsal with an altered ending Reduction in recurrent nightmare of 15 years’ duration after 7 rehearsal sessions in which patient mentally re-experienced the nightmare while saying to himself \u0026ldquo;It’s just a dream\u0026rdquo; Recurrent nightmare of 14 years’ duration disappeared after patient relived the dream 3 times while awake, then wrote 3 accounts of the nightmare with \u0026ldquo;triumphant endings\u0026rdquo; Dream reentry technique: Imagine yourself back in the dream and determine what else you can do \u0026ndash; come up with alternative ways of acting in the dream to effect a more favorable ending Ex: Dream in which dreamer finds himself in a room in which walls are closing in, threatening to crush him. Door is locked. Dream reentry: Dreamer imagines the dream again. He looks around the room and notices that there is no ceiling and climbs out. Imagery and Marketing # Cable TV study Group 1: Residents were provided with information about benefits of cable television Group 2: Residents were asked to imagine how much more convenient and inexpensive the cable service would be 20% of people in the information condition ended up subscribing as opposed to 47% in imagination condition (Matlin, 1989) Red Cross study Donations to Syrian refugees were 55 times greater in response to: Publication of iconic photo of a child killed vs. Statistics describing the hundreds of thousands of other refugee deaths (Slovic, Vastfjalla, Erlandsson et al., 2017) In creative arts # Creativity: art, music, and literature \u0026ndash; artists translate their internal images into concrete form\nRobert Louis Stevenson claimed that he would see his stories acted out before his eyes in a vision and that he merely strove to transcribe them Mozart similarly claimed that he merely transcribed his music which he heard, complete and finished, in his imagination \u0026ldquo;The music of the spheres is always playing \u0026ndash; we simply have to tune in to hear it.\u0026rdquo;\n"},{"id":36,"href":"/math-53/12/","title":"12: Vectors \u0026 Geometry of Space","section":"Math 53","content":" 12.1 3D Coordinate Systems # Right hand rule: Index point to .$x$, thumb to .$z$, and write through .$y$ If you have point .$P(a,b,c)$ and drop a perpendicular dot on the .$xy$-plane at .$a,b,0$, you now have a projection of .$P$ onto the .$xy$-plane The distance .$|P_1 P_2|$ between two points .$P_1(x_1, y_1, z_1)$ and .$P_2(x_2, y_2, z_2)$ is $$|P_1 P_2| = \\sqrt{(x_2-x_1)^2 + (y_2 - y_1)^2 + (z_2 - z_1)^2}$$ We can define the equation for a sphere with a center at .$C(h,k,l)$ and radius .$r$ as $$(x-h)^2 + (y-k)^2 + (z-l)^2 = r^2$$ 12.2 Vectors # Written as .$\\overrightarrow{AB}$ Has initial point .$A$ at the tail and the a terminal point .$B$ at the tip If the initial point .$A$ is at .$(x_1, y_1, z_1)$ and terminal point .$B$ is at .$(x_2, y_2, z_2)$, then we can write .$\\overrightarrow{AB} = \\langle x_2 - x_1, y_2 - y_1, z_2 - z_1\\rangle$ Vectors with the same length/magnitude are called equivalent or equal, despite not necessarily having the same initial/termination points Vector addition (order doesn\u0026rsquo;t matter) $$ \\overrightarrow{AC} = \\overrightarrow{AB} + \\overrightarrow{BC} = \\overrightarrow{BC} + \\overrightarrow{AB}$$ $$ \\vec u - \\vec v = \\vec u + (- \\vec v)$$ $$ \\vec a + (\\vec b + \\vec c) = (\\vec a + \\vec b) + \\vec c$$ Vector multiplication Given scalar .$c$ and vector .$\\vec v$, .$c\\cdot\\vec v$ is like .$\\vec v$ but with length changed by a factor of .$\\Vert c\\Vert$ If .$c\u0026lt;0$, then the vector is flipped around $$c\\cdot\\vec v = \\langle cv_x, cv_y, cv_z\\rangle$$ Magnitude for .$\\vec a = \\langle a_x, a_y, a_z \\rangle$: $$ \\Vert \\vec a \\Vert = \\sqrt{a_x^2 + a_y^2 + a_z^3}$$ Unit vector Has length of one If .$\\vec a \\neq 0$ then the unit vector .$\\vec u$ in the same direction as .$\\vec a$ is: $$\\vec u = \\frac{\\vec a}{\\Vert \\vec a \\Vert} = \\frac{1}{\\Vert \\vec a \\Vert} \\vec a$$ Notice that .$\\frac{1}{\\Vert \\vec a \\Vert}$ is a scalar 12.3 Dot Product # The dot product measures the extent which two vectors are parallel to one another Two vectors are perpendicular/orthogonal .$\\perp$ (.$90^\\circ$ from one another) iff the dot product is 0 .$\\vec a \\cdot \\vec b$ is the length of .$\\vec a$ times the scalar projection of .$\\vec b$ onto .$\\vec a$ Notice that the dot product gives a scalar $$\\langle a_1, a_2, a_3 \\rangle \\cdot \\langle b_1, b_2, b_3 \\rangle = a_1b_1 + a_2b_2 + a_3b_3$$ $$\\vec a \\cdot (\\vec b + \\vec c) = \\vec a \\cdot \\vec b + \\vec a\\cdot \\vec c$$ $$ \\vec a \\cdot \\vec a = \\Vert \\vec a \\Vert ^2$$ $$ \\vec a \\cdot \\vec b = \\vec b \\cdot \\vec a $$ $$ (c \\vec a) \\cdot \\vec b = c (\\vec a \\cdot \\vec b) = \\vec a \\cdot (c \\vec b)$$ $$\\vec a \\cdot \\vec b = \\Vert\\vec a\\Vert\\ \\Vert\\vec b\\Vert \\cos \\theta$$ $$\\Vert\\vec a - \\vec b \\Vert^2 = \\Vert\\vec a\\Vert^2+\\Vert\\vec b\\Vert^2 - 2\\Vert\\vec a\\Vert \\Vert\\vec b\\Vert \\cos\\theta\\ $$ $$\u0026hellip; = \\Vert\\vec a\\Vert\\ \\Vert\\vec b\\Vert \\cos \\theta$$ $$\\cos\\theta = \\frac{\\vec a \\cdot \\vec b}{\\Vert\\vec a\\Vert\\ \\Vert\\vec b\\Vert}$$ $$\\cos \\theta = \\frac{\\vec a}{\\Vert \\vec a \\Vert} \\cdot \\frac{\\vec b}{\\Vert \\vec b \\Vert} \\Longrightarrow \\cos^{-1}\\bigg(\\frac{\\vec a}{\\Vert \\vec a \\Vert} \\cdot \\frac{\\vec b}{\\Vert \\vec b \\Vert} \\bigg) = \\theta $$ Direction Vectors # The direction angles of a nonzero vector .$\\vec a$ are the .$\\alpha, \\beta, \\gamma$ that a makes with the positive .$x,y,z$-axes respectively $$\\cos\\alpha = \\frac{\\vec a \\cdot \\vec i}{\\Vert \\vec a \\Vert \\Vert \\vec i \\Vert} = \\frac{a_1}{\\Vert \\vec a \\Vert}$$ $$\\vec a = \\Vert \\vec a \\Vert \\langle \\cos \\alpha, \\cos \\beta, \\cos \\gamma \\rangle$$ Projections # Scalar projection of .$\\vec b$ onto .$\\vec a$: $$\\text{comp}_\\vec a\\vec b = \\frac{\\vec a\\cdot\\vec b}{\\Vert\\vec a\\Vert}$$ Vector projection of .$\\vec b$ onto .$\\vec a$: $$\\text{proj}_\\vec a\\vec b = \\bigg(\\frac{\\vec a\\cdot\\vec b}{\\Vert\\vec a\\Vert}\\bigg)\\frac{\\vec a}{\\Vert\\vec a\\Vert} = \\frac{\\vec a\\cdot\\vec b}{\\Vert\\vec a\\Vert^2}\\vec a$$ 12.4 Cross Product # The cross product measures how orthogonal two vectors are Therefore, .$(\\vec a \\times \\vec b) \\cdot \\vec b = 0$ because measures how similar (close to .$0^\\circ$) two vectors are and cross product outputs a vector orthogonal (.$90^\\circ$ to both .$\\vec a, \\vec b$) Two nonzero vectors are only parallel iff their cross product is zero The magnitude is also the area of a parallelogram formed by the two vectors We can find the volume of the 3D parallelogram formed by three vectors with the following equation: $$\\vec a \\cdot (\\vec b \\times \\vec c) = (\\vec a \\times \\vec b) \\cdot \\vec c$$ Realize that if the resulting area is .$0$, then all of the points exist on the same plane Notice that the cross product gives a vector that is orthogonal to both original vectors Direction is determined with the right-hand rule We can also calculate the determinant with the unit vectors .$\\langle \\hat i, \\hat j, \\hat k \\rangle$to find the cross product $$\\langle a_1, a_2, a_3 \\rangle \\times \\langle b_1, b_2, b_3 \\rangle = \\langle a_2b_3 - a_3 b_2, -(a1_b3 - a_3 b_1), a_1b_2 - a_2 b_1 \\rangle$$ $$\\vec a \\times \\vec b = \\Vert \\vec a \\Vert \\Vert \\vec b \\Vert \\sin\\theta$$ $$(\\vec a \\times \\vec b) \\times \\vec c \\neq \\vec a \\times (\\vec b \\times \\vec c) \\Longrightarrow \\vec a \\times (\\vec b \\times \\vec c) = (\\vec a \\cdot \\vec c) \\vec b - (\\vec a \\cdot \\vec b ) \\vec c$$ $$\\vec a \\times \\vec b \\neq \\vec b \\times \\vec b \\Longrightarrow \\vec a \\times \\vec b = - \\vec b \\times \\vec a$$ $$(c \\vec a) \\times \\vec b = c(\\vec a \\times \\vec b) = \\vec a \\times (c \\vec b)$$ $$\\vec a \\times (\\vec b + \\vec c) = \\vec a \\times \\vec b + \\vec a \\times \\vec c$$ $$(\\vec a + \\vec b) \\times \\vec c = \\vec a \\times \\vec c + \\vec b \\times \\vec c$$ $$\\Vert \\vec a \\times \\vec b \\Vert ^2 = \\Vert \\vec a \\Vert^2 \\Vert \\vec b \\Vert^2 - (\\vec a \\cdot \\vec b)^2 = \\Vert \\vec a \\Vert^2 \\Vert \\vec b \\Vert^2 \\sin^2 \\theta$$ $$\u0026hellip; \\Longrightarrow \\Vert \\vec a \\times \\vec b \\Vert \\Vert \\vec a \\Vert \\Vert \\vec b \\Vert \\sin \\theta$$ 12.5 Equation of Lines and Planes # Line # If given a point .$P_0 (x_0, y_0, z_0)$ and a vector .$\\vec v = \\langle a,b,c \\rangle$ that passes through said point (parallel), we can write a line equation as: $$\\langle x, y, z \\rangle = P_0 + t \\vec v = \\langle x_0 + ta, y_0 +tb, z_0 + tc \\rangle;\\ \\ t \\in \\mathbb{R}$$ We can then isolate .$t$ to find the symmetric equation of the line $$ t = \\frac{x-x_0}{a} = \\frac{y-y_0}{b} = \\frac{z-z_0}{c} $$ And we can extrapolate this to write an equation we can use to verify if two points are on a line: $$ t = \\frac{x-x_0}{x_1-x_0} = \\frac{y-y_0}{y_1-y_0} = \\frac{z-z_0}{z_1-z_0} $$ Note: If we have no change in an axis (such as .$x$), then .$x_1-x_0$ is zero. Therefore, we can write .$x = x_0$ (because we know it never changes) and the other relations for .$y$ and .$z$ still work We can also write our original equation in vector form as $$\\vec r(t) = \\vec r_0 + t\\vec v$$ Which we can use to write a specific line between point .$A$ and .$B$ from .$t \\in [0,1]$ as $$\\overrightarrow{AB} = \\langle x_1 - x_0, y_1 - y_0, z_1 - z_0 \\rangle$$ $$r(t) = \\vec A(1-t) + \\vec B(t)$$ Two vectors are equal iff corresponding components are equal 3D Lines can be parallel (same direction vector), intersect at one point (.$r_1 = r_2\\ @\\ t_1, t_2$), or skew (not intersecting nor parallel \u0026ndash; not possible in 2D) Planes # Planes need a point and direction Point .$P_0 (x_0, y_0, z_0)$ is trivial; a plane is a set of various points Direction is described by the normal vector .$\\vec n = \\langle a,b,c \\rangle$; only one normal unit vector per plane Given three points .$P_1, P_2, P_3$ on the plane, we can write two vectors .$\\vec a = \\overrightarrow{P_1 P_2}, \\vec b = \\overrightarrow{P_1, P_3}$ as we can find the normal vector as .$\\vec n = \\vec a \\times \\vec b$ Knowing this, we can write a vector equation: $$ \\vec n \\cdot \\overrightarrow{P_0 P} = 0$$ $$ \\vec n \\cdot (\\vec r - \\vec r_0) = 0$$ This can also be parameterized as both the scalar and linear equation respectively: $$a(x-x_0) + b(y-y_0) + c(z-z_0) = 0$$ $$ax + yz + cz + d = 0$$ Planes are parallel if their normal vectors are parallel Otherwise, they intersect and form a straight line Common Questions # Intersection point(s) of plane and parametric curve Given parametric curve .$C = \\langle \\alpha + a\\cdot t, \\beta + b\\cdot t, \\gamma + c \\cdot t \\rangle$ and plane equation .$P = x + y + z + d = 0$ \u0026hellip; Plug in each component of .$C$ as .$x,y,z$ in the .$P$ equation Solve for .$t$(s), then plug (it/them) into the prior equation to get intersection point(s) Angle between planes Given two planes with .$\\vec n_1, \\vec n_2$ respectively Use .$\\theta = \\cos^{-1}\\Big(\\frac{\\vec n_1 \\cdot \\vec n_2}{\\Vert \\vec n_1 \\Vert \\Vert \\vec n_2 \\Vert}\\Big)$ to find .$\\theta$ Angle between parametric lines that intercept at point .$P_3$ Given parametric equations .$L_1 = P_1 + t\\vec v_1$ and .$L_2 = P_2 + s\\vec v_2$ Find the .$t_3$ and .$s_3$ for when .$L_1 = P_3$ and .$L_2 = P_3$ (point of intersection) Use .$\\theta = \\cos^{-1}\\Big(\\frac{\\vec v_1\u0026rsquo;(t_3) \\cdot \\vec v_2\u0026rsquo;(s_3)}{\\Vert \\vec v_1\u0026rsquo;(t_3) \\Vert \\Vert \\vec v_2\u0026rsquo;(s_3) \\Vert}\\Big)$ to find .$\\theta$ Intersection of two lines: .$L_1: \\langle x_1, y_1, z_1 \\rangle + t \\cdot \\langle a_1, b_1, c_1\\rangle; L_2: \\langle x_2, y_2, z_2 \\rangle + s \\cdot \\langle a_2, b_2, c_2 \\rangle;$ First, check if they are parallel by checking if the directions of the two lines are scalar multiples of one another. Second, check if they intersect by setting the two parametric equations equal to one another: .$x_1 + t a_1 = x_2 + s a_2; y_1 + t b_1 = y_2 + s b_2; z_1 + t c_1 = z_2 + s c_2$ If an equation exists (that is, there is a valid .$t$ and .$s$) then they intersect at .$L_1(t)$ or .$L_2(s)$ Intersection line .$L$ of two planes Set one of the 3D variables to zero (e.x. .$z = 0$) to find where the two planes intersect the remaining plane (in this case the .$z$ plane) We find the intersection point .$P (\\alpha, \\beta, 0)$ by solving when our plane equations in the prior equation are equal Since the line .$L$ is orthogonal to both planes\u0026rsquo; normal vectors, our direction vector is .$\\vec v = n_1 \\times n_2 = \\langle a,b,c \\rangle$ We can then plug in everything into the symmetric equation: .$\\frac{x-\\alpha}{a} = \\frac{y-\\beta}{b} = \\frac{z-0}{c}$ Distance from point to plane Given point .$P_1(x_1, y_1, z_1)$ and plane .$ax+by+cz+d = 0$ $$D = \\Vert \\text{comp}_{\\vec n}{\\vec b} \\Vert = \\frac{\\vert ax_1 + by_1 + cz_1 +d\\vert}{\\sqrt{a^2 + b^2 + c^2}}$$ Distance from skew line to skew line Given .$L_1 = \\langle x_1, y_1, z_1 \\rangle + t \\vec v_1$ and .$L_2 = \u0026hellip;$ Find the plane containing .$L_2$ that is parallel to .$L_1$. .$P_3 = \\langle x- x_2, y - y_2, z - z_2 \\rangle \\cdot (v_1 \\times v_2)$ Find the .$D$istance from .$x_1, y_1, z_1$ on .$L_1$ to the plane .$P_3$ above $$D = \\frac{\\vert a_3 x_1 + b_3 y_1 + c_3 z_1 +d\\vert}{\\sqrt{a_3^2 + b_3^2 + c_3^2}}$$ Distance from point to line Given line .$L = \\langle x_0, y_0, z_0 \\rangle + \\langle a, b, c \\rangle$ and point .$P = \\langle P_x, P_y, P_z \\rangle$ Write a distance equation as .$D^2 = ((x_0 + ta) - P_x)^2 + ((y_0 + tb) - P_y)^2 + ((z_0 + tc) - P_z)^2$ Find the minimum value of .$t$ and plug in into the equation above and find .$D$ (but not .$D^2$!) 12.6 Cylinders and Quadratic Surfaces # Traces # Cross sections of a surface found by taking a three-variable equation and setting one of the equations variables equal to a constant. Help us visualize 3D curves by thinking about them from different axis $$z = \\frac{x^2}{4} + \\frac{y^2}{9} \\Longrightarrow k = \\frac{x^2}{4} + \\frac{y^2}{9}$$ $$\u0026hellip; \\Longrightarrow z = \\frac{x^2}{4} + \\frac{k^2}{9}$$ In the left example, for every .$z$ value we have a ellipse In the right example, we have a parabola for every .$y$ value Cylinder # A cylinder is a surface that consists of all rulings that at parallel to a given line that passes through a given plane curve Two variable equation in 3D space E.x. .$y =\\sin x, z = x^2$ Quadratic Surfaces # A quadric surface is the graph of a second-degree equation with three variables Two standard forms (when centered about origin): $$Ax^2 + By^2 + Cz^2 + J = 0$$ $$Ax^2 + By^2 + Iz = 0$$ "},{"id":37,"href":"/cogsci-c100/language/","title":"13: Language","section":"CogSci C100","content":" Controversies in Cognitive Science # How is language acquired and how do we process words when reading?\nIs language learning innate, and if so, to what extent? How do we learn language? Skill building hypothesis: Language is acquired as a result of learning language skills, such as vocabulary and grammar Comprehension hypothesis: Language skills such as vocabulary and grammar, result from language acquisition How are words processed when we read? Direct access hypothesis: Readers recognize words directly from the printed letters Indirect access hypothesis: Readers convert the printed letters into a phonological code to access the word and its meaning Language # Language: our spoken, written, or signed words and the ways we combine them to communicate meaning\nTrivia: How many words does the average person know? How many languages exist in the world today? What percentage of the world’s children are bilingual? Of American children? Do bilingual children perform better or worse on tests of language ability (in first language)? Of mathematical ability? Of following instructions? Answers to language trivia questions Average adult has an active vocabulary of around 20,000 words and a passive vocabulary of around 40,000 words That averages to nearly 7 new words per day between ages 2 and 18! Linguists estimate that 6500 languages exist in the world today 250 languages are spoken by more than 1 million people Only 600 languages have speaking populations robust enough to support their survival past the end of the century. Languages need at least 100,000 speakers to survive the ages. 66% of the world’s children are raised as bilingual speakers; only 6.3% of U.S. residents are bilingual Development of Language # Prenatal language comprehension # \u0026ldquo;Dr. Seuss\u0026rdquo; study in which pregnant women read one of three children’s stories aloud once every day during the last 6 weeks of pregnancy (Decasper \u0026amp; Spence, 1986) Infants tested when 2 days old They were played two stories through earphones, one of which was the one they had heard By changing their sucking patterns, they could control which story they heard Newborns showed preference for the story they had heard prenatally Sound of a newborn baby’s cry may depend on the language the parents speak: babies imitate the general sound of their parents’ language (Mampe, Friederici, Christophe et al., 2009) French babies cry with a rising melody German babies prefer a falling melody Speech perception in infancy # We are all born with the ability to recognize speech sounds or phonemes (e.g., b vs. p) from all the world’s languages but gradually lose this ability Adult Hindi-speakers and young infants from English-speaking homes can easily discriminate two Hindi t sounds not spoken in English. By age one, however, English-speaking listeners rarely perceive this sound difference Japanese speakers have difficulty distinguishing between English r and l At birth or a few weeks after, infants can perceive almost all (95%) of the subtle phoneme differences in nonnative languages. However, by 8-10 months, accuracy drops to 70% and, by 10-12 month, to 20% (Werker, J.F., 1989) Chinese adoptees living in Canada since age 1 process Chinese sounds as do native speakers, even if they have no conscious recollection of Chinese words (Pierce, Klein, Chen et al., 2014) In general, it’s much easier to learn a language at an early age Child-directed speech (motherese) # Language comprehension and motherese (child-directed speech) Motherese or child-directed speech: characterized by repetition, simple vocabulary and syntax, clear pronunciation, slow pace, high pitch with varying intonation, a focus on the here and now, and exaggerated facial expressions Designed to make it easier for infant to decode the language Even young children will do this when speaking to younger siblings/playmates Theories of Language Development # Chomsky’s Nativist View # Children are born with an innate understanding of grammar and a predisposition to learn language (Language Acquisition Device) There is a critical learning period for the acquisition of language \u0026ndash; Evidence against this view: Competency in a second language can be attained even when initial exposure to the language happens relatively late However, those who learn a second language during early childhood are less likely to have an accent (Flege, Yeni-Komshiam, \u0026amp; Lui, 1999) Also, in general, the older the age at immigration, the poorer the grammatical mastery of the second language (Johnson \u0026amp; Neport, 1991) Evidence in support of critical learning period: Cases of child abuse where a child is kept isolated from birth. When child is discovered and rescued early (before age 6), s/he can learn language but attempts at rehabilitation are not so successful when child kept in seclusion till later childhood (age 12) Deaf children born to non-signing family never become grammatically fluent in sign language Chomsky’s nativist view (i.e., view that language learning is innate) is based on poverty of stimulus arguments: young children are simply not exposed to enough information to allow them to learn a language Much of the speech that children hear is actually ungrammatical, but not flagged as such Children are typically only exposed to positive information, i.e., they are not told what counts as ungrammatical (e.g., the bell ringed) Arguments against nativist view # Connectionist models demonstrate that it is possible to learn complex linguistic skills without having any explicit linguistic rules encoded in it The learning trajectory of these networks strongly resembles the learning trajectory of human infants Ex: learning how to form the past tense of English verbs, both regular and irregular Children learning the English past tense go through three easily identifiable stages: Stage 1: They employ a small number of very common verbs in the past tense (e.g., \u0026ldquo;got,\u0026rdquo; \u0026ldquo;gave,\u0026rdquo; \u0026ldquo;went,\u0026rdquo; \u0026ldquo;was\u0026rdquo;) Most of these verbs are irregular, and assumption is that children learn these by rote At this stage, children are not capable of generalizing from the words they have learned. They also tend not to make too many mistakes. Stage 2: They use a much greater number of verbs in the past tense, some of which are irregular but most of which employ the regular past tense ending of \u0026ldquo;-ed\u0026rdquo; During this stage, they can generate a past tense for an invented word (e.g., \u0026ldquo;ricked\u0026rdquo;) Children at this stage take a step backward and make mistakes on the past tense of irregular verbs that they had previously given correctly (overregularization errors), Ex: saying, \u0026ldquo;gived\u0026rdquo; instead of \u0026ldquo;gave\u0026rdquo; Stage 3: They learn more verbs and cease to make overregularization errors Bayesian (probabilistic) models of language learning demonstrate that a surprising amount can be learned through sensitivity to statistical regularities in heard speech One of the most basic challenges in understanding speech is word segmentation: segmenting a continuous stream of sounds into individual words In English, an actual physical event, such as a pause, marks a word boundary less than 40% of the time How does 8-month-old infant (which is when this skill starts to emerge) figure out which combinations of syllables make words, and which ones don’t? Can be explained by model of transitional probabilities The transitional probability between any two sounds is the probability that the second will follow the first sound High transitional probabilities will tend to indicate syllables occurring within a word, while low transitional probabilities will tend to occur across the boundaries of words Infants are exquisitely sensitive to the frequency of correlations, and they exploit this sensitivity to parse streams of sound into words Transitional probabilities may also be used by adults to map the boundaries of phrases Research indicates that babies do show a remarkable ability to learn statistical aspects of human speech (Erickson \u0026amp; Thiessen, 2015; Werker, Gunthert, \u0026amp; German, 2012) They are able to discern word breaks As mentioned, an actual physical event, such as a pause, marks a word boundary less than 40% of the time in English They statistically analyze which syllables \u0026ndash; as in hap-py-ba-by \u0026ndash; most often go together (Safran, 2009; Safran, Aslin, \u0026amp; Newport, 1996 8-month-olds were exposed to 2 minutes of a computer voice speaking an unbroken, monotone string of nonsense syllables (bidakupadotigolabubidaku…) Were able to recognize (as indicated by attentional measures) three-syllable sequences that appeared repeatedly Vygotsky’s view # Vygotsky argued that language is the foundation for the development of higher human thought: children use words as a way to learn to think\nChildren of about 4 or 6 often talk aloud in a non-communicative manner When children are drawing at a table, one might say, \u0026ldquo;I’ll make it green\u0026rdquo; without indicating what he would make green; another looking at his own drawing, might respond, \u0026ldquo;Horses like sugar and oats.\u0026rdquo; Vygotsky contended that this type of non-communicative speech is a transition phase to the development of verbal thought Consistent with this view, researchers have found that first and secondgraders who manifested the most muttering and lip movement while solving arithmetic problems showed the greatest improvements in their arithmetic ability over the course of a year Inner speech and spoken language eventually become independent Language as means of developing thinking # Effect of Early Exposure of Language on Cognitive Development (Hart, \u0026amp; Risley, 1995, 2003) By age 3, a child growing up in poverty would have heard 30 million fewer words in his home environment than a child from a professional family Also, the greater the number of words children heard from their parents or caregivers before they were 3, the higher their IQ and the better they did in school TV talk not only doesn’t help, it is detrimental Study found that infants (12-18 months) actually could not learn vocabulary by merely watching (bestselling) DVD in which spoken words were linked with appropriate objects (DeLoache, Chiong, Sherman et al., 2010) Some researchers have argued that the racial and socioeconomic gap in academic performance can be wholly accounted for by disparities in exposure to language alone! New intervention programs target this problem Cognitive-functional approach: the importance of meaning # The cognitive-functional approach: emphasizes that the function of human language is to communicate meaning to other individuals\nThere is no such thing as a perfect synonym or two identical sentences Second Language Acquisition # How do we learn language?\nSkill building hypothesis: Language is acquired as a result of learning language skills, such as vocabulary and grammar Use skill building: learn grammar, study vocab lists, do drills, take tests General public (and government) believe this is the way to learn language \u0026ldquo;No pain, no gain\u0026rdquo; Comprehension hypothesis (Stephen Krashen): language skills such as vocabulary and grammar, result from language acquisition \u0026ndash; we acquire language in one and only one way: when we understand messages Use \u0026ldquo;comprehensible input\u0026rdquo;: listen to stories, read books, have conversations, watch movies Immediate gratification: have a good time \u0026ndash; the more you enjoy it, the better your comprehension will be Comprehensible input has won in pretty much every single study comparing the two methods Some evidence in support of comprehensible input Complexity of language learning, e.g., of vocab and grammar, wipes out skill building as a possibility Average native English speaker knows 40,000+ words Study found that second language readers who read a lot have larger vocabularies than native speakers who didn’t read a lot It’s possible to acquire language without any conscious learning Implications: What is of primary importance is not pushing a person to speak from Rather, it is listening, picking up comprehensible input Natural language approach to second language learning # 1. Storytelling # Find language partner who is fluent in language you are trying to learn, e.g., friend, family member, or language exchange partner Find magazines (20%), then children’s stories (80%) with tons of pictures, e.g., travel magazine with pictures related to travel, food, clothing, etc. Language partner is not going to translate story; will describe pictures and ask you simple questions, and you will ask simple questions Partner doesn’t just describe what is in pictures: they describe the picture the way they might to a young child they love Ex: \u0026ldquo;This is a spare tire. A spare tire is very important. You could have a blowout and then you would need to use that spare tire.\u0026rdquo; (Jeff Brown) Rules to tell language partner No English If we don’t understand each other, we’ll use gestures and act or draw If we still can’t understand, we’ll say \u0026ldquo;It’s not important\u0026rdquo; in target language No grammar: don’t teach me any grammar No corrections: don’t correct me at any time Factors that affect language acquisition Motivation: positive correlation Self-esteem: positive correlation Anxiety: negative correlation For language acquisition to really succeed, anxiety should be zero Affective filter Somewhere in brain is a language acquisition device, according to Chomsky. Our job is to get input into that device. High anxiety blocks the input. If a student thinks language class is a place where his weaknesses will be revealed, he may understand the input, but it won’t penetrate 2. Read, read, read \u0026ndash; whatever is of interest to you # \u0026ldquo;Free voluntary reading is the most powerful tool we have in all of language education\u0026rdquo;\nRead things that you’re passionate about Watch baseball game, then go read about it in newspaper This is also the key to teaching kids who don’t want to learn to read TPR # TPR (Total Physical Response): acquiring a language through movement\nAsk language partner to give you a list of commands or actions Ex: Jump, walk, run, turn around, sit down, stand up, dance, talk, yell, complain, look, watch TV, turn TV on, turn TV off, cry, laugh\nCan use hands to pantomime movements Can also use gestures to represent words Do 50-100 per session Social context of speech # Social context of speech: learning language is not merely a matter of learning vocabulary and grammar \u0026ndash; the goal is not just to express one’s thoughts but must take into account other people’s thoughts, feelings, and beliefs\nEstablishing common grounds We have social rules for the format of our conversations (\u0026ldquo;winding down\u0026rdquo; a conversation before saying \u0026ldquo;good-bye\u0026rdquo; on the phone) Phrasing of directives, a sentence that requests someone to do something Could you give me a ride? vs. Will you give me a ride?\nUse of indirect speech acts I wonder if there’s any butter in the refrigerator? and It’s cold in here\nThere are gender differences in the use of directives Speech Perception and AI # Speech perception is an extremely complicated process (which is why computer voice recognition systems are often problematic!)\nNeed to separate voice of speaker from irrelevant background noises, which might include other simultaneous conversations Pronunciation varies, depending on vocal characteristics of speaker Speakers often slur or mispronounce words Pronunciation of specific phoneme depends in large on previous and following phonemes, e.g., d in idle vs. d in don’t As mentioned, an actual physical event, such as a pause, marks a word boundary less than 40% of the time Children’s mispronunciations of lyrics in Christmas carols and Pledge of Allegiance People use visual cues to facilitate speech perception Study in which participants watched video of woman making one sound (ga) while different sound played (ba) Responses reflected compromise (participants reported hearing da) (McGurk \u0026amp; MacDonald, 1976) Lipnet, developed by team at Oxford’s AI lab, can now also lipread (i.e., translate lip movements to text) with 95% accuracy Computers can now replicate human voices extremely accurately Company Lyrebird has created program that can replicated voices of people, including powerful political figures, after analyzing only one minute of audio More challenging to replicate associated facial movements Creating video of Obama required 14 hours of Obama high quality footage to train system to translate audio into mouth shapes Reading # The eye makes saccadic movements (\u0026ldquo;jumps\u0026rdquo; that occur every 1/4 second) during reading Good readers show fewer fixations, larger saccadic jumps, and fewer regressions than poor readers Readers use contextual cues to extract meaning I cdnuolt blveiee taht I cluod aulaclty uesdnatnrd waht I was rdanieg. The phaonmneal pweor of the hmuan mnid. Aoccdrnig to a rscheearch at Cmabrigde Uinervtisy, it deosn\u0026rsquo;t mttaer inwaht oredr the ltteers in a wrod are, the olny iprmoatnt tihng is taht the frist and lsat ltteer be in the rghit pclae. The rset can be a taotl mses and you can sitll raed it wouthit a porbelm. Tihs is bcuseae the huamn mnid deos not raed ervey lteter by istlef, but the wrod as a wlohe. Amzanig huh? yaeh and I awlyas tghuhot slpeling was ipmorantt!\nWord Processing in Reading: Dual-route approach # Dual-route approach to reading: direct vs. indirect access hypotheses Do readers recognize a word directly from the printed letters (direct access)? Or do they convert the printed letters into a phonological code to access the word and its meaning (indirect access)? Petersen, Fox, Posner, et al. (1988) explored this question in PET study Condition 1 (looking): Participants asked to focus on a fixation point (a small crosshair) in the middle of a screen Condition 2a (reading silently): Participants were presented with words flashed on the screen but told not to make any response Condition 2b (listening): Participants listened to the same words being spoken Condition 3 (reading out loud): Participants were asked to say out loud the word appearing on the screen Condition 4 (speaking): Participants were presented with nouns on the screen and asked to utter an associated verb Ex: Saying \u0026ldquo;turn\u0026rdquo; when presented with word \u0026ldquo;handlebars\u0026rdquo; Results: Areas of activation in Conditions 3 and 4 (speaking words) did not include areas in Condition 2a and 2b (reading silently and listening to words, respectively) Conclusions: Patterns of activation identified across the different tasks thus supported a parallel rather than a serial model of single-word processing In addition, the results support the direct access hypothesis: we do not need to sound words out (or subvocalize) to access meaning of words Moreover, research in general has indicated that, though readers use both direct and indirect access when reading, direct access is more efficient Skilled adult readers are more likely to use direct access Beginning and less skilled readers are likely to sound out words to understand meaning Dyslexia # The direct and indirect approaches are reflected in two different types of dyslexia (learning disability that interferes with reading despite average or above average intelligence)\nPhonological dyslexia manifests as severe impairment in reading phonetic script (similar to alphabetic system), but preserved ability in reading pictographic script Surface dyslexia manifests as impairment in reading pictographic script (characters) Neurolinguistics # Neurolinguistics: study of relationship between the brain and language Hemispheric Specialization # Left hemisphere typically performs most language processing (95% for the right-handed; 50% for the left-handed) However, right hemisphere interprets a message’s emotional tone, decodes metaphors, and resolves ambiguities Aphasias # Aphasia: difficulty in producing or comprehending speech caused by brain damage\nBroca’s aphasia, or expressive aphasia Speech is meaningful, but halting, ungrammatical; function words (e.g., a, the, in, about) are omitted \u0026ldquo;Buy bread store\u0026rdquo;\nAssociated with damage to portion of left frontal lobe Recent research indicates area is also involved in resolving representational conflict (e.g., Stroop), processing mental imagery and music Wernicke’s aphasia, or fluent aphasia Loss of ability to understand speech and to produce meaningful words; speech is fluent and grammatical but consists of empty words: word salad \u0026ldquo;I called my mother on the television and did not understand the door. It was not too breakfast, but they came from far to near.\u0026rdquo;\nWernicke patients do not appear to recognize that they cannot produce meaningful speech or understand others. As a result, they can become quite paranoid. Associated with damage to part of left posterior superior temporal lobe There are many different kinds of aphasia, associated with damage to different cortical areas Some can speak but cannot read; others can read but cannot speak Some can write but not read; others can read but not write Some can read numbers but not letters Some can sing but not speak Language is complex and many different areas serve different language functions Bilingualism # Bilingualism: ability to use two languages that differ\nIf you’re fluent in two languages, your brain processes them in similar areas If you learned a second language after the first, you process them in different areas Disadvantages of bilingualism # May pronounce some speech sounds slightly differently May take longer to make some language-processing decisions (Berken, Gracco, Chen et al., 2015) Advantages of bilingualism # Early belief that bilingual education was \u0026ldquo;expensive, ineffective, and harmful\u0026rdquo; Early study found that bilingual French-Canadians had lower IQ’s than monolingual English-Canadians, but turned out this was primarily due to fact that French-Canadian bilinguals came from a lower socio-economic class Bilinguals tend to acquire greater expertise in their native (first) language Bilinguals also show superior performance on measures of ability to follow instructions, as well as certain types of concept formation and problem solving tasks Students in French immersion programs in Canada had higher aptitude scores not only in language but also in math than children with comparable abilities in control groups Bilingual education: children in \u0026ldquo;two-way\u0026rdquo; schools develop higher self-esteem, drop out less frequently and eventually attain higher levels of academic achievement and English proficiency A number of studies have indicated that lifelong use of two languages delays the onset of dementia by 4-5 years (Kim, Jeong, Nam et al, 2019) Language and Thought # Be impeccable with your word: Speak with integrity. Say only what you mean. Avoid using the word to speak against yourself or to gossip about others. Use the power of your word in the direction of truth and love. \u0026ndash; Don Miguel Ruiz (The Four Agreements)\nSapir-Whorf hypothesis # Sapir-Whorf hypothesis: view that language determines thought\nUnderlying assumption of use of affirmations in cognitive therapy Language and color: Natives in New Guinea who have words for two different shades of yellow more speedily perceive and better recall variations between the two yellows Those who speak Russian, which has distinct names for various shades of blue, remember the blues better (Davidoff, Davies, \u0026amp; Roberson, 1999) (Winawer, Witthoft, Frank et al., 2007) People who are bilingual may think differently in different languages Bilinguals reveal different personalities when taking the same personality test in their two languages (Chen \u0026amp; Bond, 2010) China-born students at University of Waterloo were asked to describe themselves in English or Chinese When describing themselves in English, they expressed mostly positive self-statements and moods When responding in Chinese, they reported more agreement with Chinese values and roughly equal positive and negative self-statements and moods (Ross, Xun, \u0026amp; Wilson, 2002) Bilinguals often switch languages, depending on which emotion they want to express (Chen, Kennedy, \u0026amp; Zhou, 2012) When responding in their second language, bilingual people’s moral judgments reflect less emotion \u0026ndash; they respond with more \u0026ldquo;head\u0026rdquo; than \u0026ldquo;heart\u0026rdquo; (Costa, Foucart, Hayakawa et al., 2014) Animal’s use of language # Can animals learn language?\nChimpanzees # Baby chimpanzee, Washoe, was taught American Sign Language By age of five, she had learned more than 130 signs Other chimpanzees trained to use lexigrams (geometrical shapes displayed on a keyboard and linked to a computer) as words showed performances similar to Washoe In general, chimpanzees seem to be able to acquire a vocabulary and comprehension roughly equivalent to a 2-1/2- year-old child Gorillas # Koko had a working vocabulary of over 1000 signs Understood approximately 2,000 words of spoken African grey parrot # Alex \u0026ndash; a parrot with an \u0026ldquo;attitude\u0026rdquo; that apparently had an understanding of what he said Could correctly answer questions about an object’s shape, color, or material "},{"id":38,"href":"/math-53/13/","title":"13: Vector Functions","section":"Math 53","content":" 13.1 Vector Functions and Space Curves # Vector Functions # We can write a vector function as $$\\vec r(t) = \\langle f(t), g(t), h(t) \\rangle = f(t) \\hat i + g(t) \\hat j + h(t) \\hat k$$ The domain of .$\\vec r$ consists of all values of .$t$ for which each of the terms are defined The limit of a vector function is $$\\lim_{t\\to a} \\vec r(t) = \\big\\langle \\lim_{t\\to a} f(t), \\lim_{t\\to a} g(t), \\lim_{t\\to a} h(t) \\big\\rangle$$ .$\\vec r$ is continuous at time .$a$ if .$\\lim_{t\\to a} \\vec r(t) = \\vec r(a)$ Space Curve # The set .$C$ of all points defined by a vector function .$\\vec r$ over interval .$I$ is called a space curve. Think of .$C$ as being traced out by a moving particle whose position follows .$\\vec r$ Space curves are parametrized by a vector function but isn\u0026rsquo;t necessarily that vector function! E.x. .$\\vec r(t) \\langle \\cos t, \\sin t, t \\rangle \\neq \\vec q(t) = \\langle \\cos 3t, \\sin 3t, 3t \\rangle$ Space curves exist on the same point, but don\u0026rsquo;t grow at the same rate 13.2 Derivatives and Integrals of Vector Functions # Derivatives # $$\\frac{d\\vec r}{dt} = \\vec r\u0026rsquo;(t) = \\lim_{h \\to 0} \\frac{\\vec r(t+h) - r(t)}{h}$$ $$\u0026hellip; = \\big\\langle f\u0026rsquo;(t), g\u0026rsquo;(t), h\u0026rsquo;(t) \\big\\rangle$$\nNote that the last equation only works if each component is differentiable The direction is the tangent line and the magnitude is the rate at which the particle is moving at that point If we just want the tangent line, we can write the unit tangent vector: $$\\vec T(t) = \\frac{\\vec r\u0026rsquo;(t)}{\\Vert \\vec r \u0026rsquo; (t) \\Vert}$$ Differentiation rules (notice .$f(t)$ is a scalar function) $$ \\frac{d}{dt} \\big[f(t) \\vec u(t) \\big] = f\u0026rsquo;(t) \\vec u(t) + f(t) \\vec u\u0026rsquo;(t)$$ $$ \\frac{d}{dt}\\big[\\vec u(t) \\cdot \\vec v(t)\\big] = \\vec u\u0026rsquo;(t) \\cdot \\vec v(t) + \\vec u(t) \\cdot \\vec v\u0026rsquo;(t) $$ $$ \\frac{d}{dt} \\big[\\vec u(t) \\times \\vec v(t)\\big] = \\vec u\u0026rsquo; (t) \\times \\vec v(t) + \\vec u(t) + \\vec v\u0026rsquo;(t)$$ $$ \\frac{d}{dt} \\big[ \\vec u(f(t)) \\big] = f\u0026rsquo;(t) \\vec u\u0026rsquo; (f(t))$$ Integrals # Let .$\\vec r (t) = \\langle f(t), g(t), h(t) \\rangle$, then $$\\int_a^b \\vec r(t)\\ dt = \\bigg\\langle \\int_a^b f(t)\\ dt, \\int_a^b g(t) \\ dt, \\int_a^b h(t)\\ dt \\bigg\\rangle$$ Fundamental theorem: If .$\\vec R(t)$ is an anti-derivative of .$\\vec r(t)$, (i.e. .$\\vec R\u0026rsquo;(t) = \\vec r(t)$), then $$ \\int_a^b \\vec r(t) \\ dt = \\Big[\\vec R(t)\\Big]_a^b = \\vec R(b) - \\vec R(a)$$ "},{"id":39,"href":"/math-53/14/","title":"14: Partial Derivatives","section":"Math 53","content":" 14.1 Functions of Several Variables # In the real world, most things don\u0026rsquo;t depend on a single variable Temperature may depend on the .$(x,y)$ (latitude/longitude) position Volume of a cylinder depends on radius .$r$ and height .$h$: .$V =\\pi r^2 h$ Formal Definition: A function .$f$ of two variables is a rule that assigns to each ordered pair of real numbers .$(x,y)$ in a set .$D$ a unique real number denoted by .$f(x,y)$. The set .$D$ is the domain of .$f$ and its range is the set of values that .$f$ takes on, that is, .$\\{f(x,y)\\ |\\ (x,y) \\in D \\}$. E.x. for .$f(x,y) = \\frac{\\sqrt{x+y+1}}{x-1}$, the domain is .$D = \\{(x,y)\\ |\\ x + y + 1 \\geq 0, x \\neq 1\\}$ which can be graphed with a solid line following .$y =-x-1$ with a dotted line at .$x = 1$ E.x. for .$f(x,y) = x\\ln(y^2-x)$, the domain is .$D =\\{(x,y)\\ |\\ x\u0026lt;y^2\\}$. This can be graphed with a dotted line following the curve .$x = y^2$ For the equation .$z = f(x,y)$, .$x,y$ are the independent variables and .$z$ is the dependent variable \u0026ndash; similar to single variable equations We can visualize functions of two variables (i.e .$f(x,y)$) by graphing them in 3D as .$(x,y,f(x,y))$ We can then write level curves for the function by setting .$f(x,y) = k$ for some .$k$onstant in the range of .$f$. This will result in a graph similar to a Topographic map Level Curve: The level curves of a function .$f$ of two variables are the curves with equations .$f(x,y) = k$, where .$k$ is a constant (in the range of .$f$). 14.2 Limits and Continuity # Limits # Vector Limit: Let .$f$ be a function of two variables whose domain .$D$ includes points arbitrarily close to .$(a, b)$. Then we say that the limit of .$f(x, y)$ as .$(x, y)$ approaches .$a, b.$ is .$L$ and we write $$\\lim_{(x,y)\\to(a,b)} f(x,y) = L$$ if for every number .$\\varepsilon \u0026lt; 0$ there is a corresponding number .$\\delta \u0026lt; 0$ such that if .$(x, y) \\in D$ and .$0 \u0026lt; \\sqrt{(x-a)^2 + (y-b)^2} \u0026lt; \\delta$ then .$\\vert f(x,y) - L \\vert \u0026lt; \\varepsilon$ Notice that .$\\vert f(x,y) - L \\vert$ is the distance between the numbers .$f(x, y)$ and .$L$, and .$\\sqrt{(x-a)^2 + (y-b)^2}$ is the distance between the point .$(x, y)$ and the point .$(a, b)$. If .$f(x,y) \\to L_1$ as .$(x,y) \\to (a,b)$ along a path .$C_1$ and .$f(x,y) \\to L_2$ as .$(x,y) \\to (a,b)$ along a path .$C_2$, where .$L_1 \\neq L_2$, then .$\\lim_{(x,y)\\to(a,b)} f(x,y)$, does not exist. We can test this by setting .$x$ and .$y$ to various different values (e.x. .$x = 0, y = 0, x = y, \u0026hellip;,$ etc) Continuity # A function .$f$ of two variables is called continuous at .$(a,b)$ if $$\\lim_{(x,y)\\to (a,b)} f(x,y) = f(a,b)$$ We say .$f$ is continuous on .$D$ if .$f$ is continuous at every point .$(a,b)$ in .$D$. That is, we need the limit to exist ands for .$f(a,b)$ to be defined Continuous functions: .$x,y, c, \\text{ trig (on domain)}$ Arithmetic, composition, exponent all preserve continuity (on domain!) Dividing doesn\u0026rsquo;t necessarily preserve continuity 14.3 Partial Derivatives # If .$f$ is a function of two variables, its partial derivatives are the functions .$f_x$ and .$f_y$ defined by $$f_x(x,y) = \\frac{\\delta f}{\\delta x} = \\lim_{h\\to0} \\frac{f(x+h,y)-f(x,y)}{h}$$ $$f_y(x,y) = \\frac{\\delta f}{\\delta y} = \\lim_{h\\to0} \\frac{f(x,y+h)-f(x,y)}{h}$$ Notice we use .$\\delta$ instead of .$d$ for partial derivatives These can be written at a single point .$(a,b)$ with respect to .$x$ and .$y$ by treating the remaining variables as a constant $$f_x(a,b) = g\u0026rsquo;(a); \\ \\ \\ g(x) = f(x,b)$$ $$f_y(a,b) = h\u0026rsquo;(b); \\ \\ \\ h(y) = f(a,y)$$ Which can be extrapolated for 3 (or more) variables: $$f_z(a,b,c) = k\u0026rsquo;(c); \\ \\ \\ k(z) = f(a,b,z)$$ Higher Derivatives # Just like regular derivatives, we can do many partial derivatives For example, the following are second partial derivatives of .$z = f(x,y)$: $$(f_x)_x = f_{xx} = \\frac{\\delta }{\\delta x}\\bigg(\\frac{\\delta f}{\\delta x}\\bigg) = \\frac{\\delta^2f}{\\delta x^2}$$ $$(f_x)_y = f_{xy} = \\frac{\\delta }{\\delta y}\\bigg(\\frac{\\delta f}{\\delta x}\\bigg) = \\frac{\\delta^2f}{\\delta y \\delta x}$$ $$(f_y)_x = f_{yx} = \\frac{\\delta }{\\delta x}\\bigg(\\frac{\\delta f}{\\delta y}\\bigg) = \\frac{\\delta^2f}{\\delta x \\delta y}$$ $$(f_y)_y = f_{yy} = \\frac{\\delta }{\\delta y}\\bigg(\\frac{\\delta f}{\\delta y}\\bigg) = \\frac{\\delta^2f}{\\delta y^2}$$ Clairaut\u0026rsquo;s Theorem Suppose .$f$ is defined on a disk .$D$ that contains the point .$(a,b)$. If the functions .$f_{xy}$ and .$f_{yx}$ are both continuous on .$D$, then $$f_{xy}(a,b) = f_{yx}(a,b)$$ Partial Differential Equations # In the sciences, we typically want to find how a system changes with respect to multiple variables Partial derivatives occur in partial differential equations, e.x Laplace\u0026rsquo;s Equation: $$ \\frac{\\delta^2 u}{\\delta x^2} + \\frac{\\delta^2 u}{\\delta y^2} = 0$$ Solutions to Laplace\u0026rsquo;s are always harmonic functions, such as .$u = e^x \\sin(y)$ 14.4 Tangent Planes and Linear Approximations # Tangent Planes # Tangent planes are to surfaces as tangent lines are to curves Tangent planes contain both the partial derivative lines w.r.t .$x$ and .$y$ All we need to know to write a tangent plane is a point .$(a,b)$ and direction of the two partial derivatives .$\\langle 1,0, f_x(a,b)\\rangle, \\langle 0,1, f_y(a,b)\\rangle$ Direction vector can be found with .$\\langle 1,0, f_x(a,b)\\rangle \\times \\langle 0,1, f_y(a,b)\\rangle = \\langle -f_x, -f_y, 1 \\rangle$ which we can dot with .$\\langle x-x_0, y-y_0, z-f(x_0,y_0) \\rangle$ Suppose .$f$ has continuous partial derivatives. An equation of the tangent plane to the surface .$z = f(x, y)$ at the point .$P(x_0,y_0,z_0)$ is $$z-f(x_0, y_0) = f_x(x_0,y_0)(x-x_0) + f_y(x_0, y_0)(y-y_0)$$ Linear Approximations # As we get very close to the surface, then the tangent plane (at point .$(a,b)$) looks more and more like the surface Thus, we can use it to for making approximations when we are near near .$(a,b)$ If .$z = f(x,y)$, then .$f$ is differentiable at .$(a,b)$ if .$\\Delta z$ can be expressed in the form $$\\Delta z = f_x(a,b)\\Delta x + f_y(a,b)\\Delta y + \\varepsilon_1 \\Delta x + \\varepsilon_2 \\Delta y$$ where the error terms, .$\\varepsilon_1$ and .$\\varepsilon_2 \\to 0$ as .$(\\Delta x, \\Delta y) \\to (0,0)$ and the other terms are the linearization of the function. Rewriting this, we can use the given .$f(x,y)$ to write the linear approximation which is an estimate for point/state at .$f(a,b)$ $$f(x,y) \\approx f(a,b) + f_x(a,b) (x-a) + f_y(a,b)(y-b)$$ We can similarly define the 3D linear approximation, increment of .$w$, and differential .$dw$ as: $$f(x,y,z) \\approx f(a,b,c) + f_x(a,b,c) (x-a) + f_y(a,b,c)(y-b) + f_z(a,b,c) (z-c)$$ $$\\Delta w = f(x+\\Delta x, y + \\Delta y, z + \\Delta z) - f(x,y,z)$$ $$dw = \\frac{\\delta w}{\\delta x}dx + \\frac{\\delta w}{\\delta y}dy + \\frac{\\delta w}{\\delta z}dz$$ Differentials # With one variable functions, i.e. .$y = f(x)$, we defined the differential .$dx$ to be independent so we had to write .$dy$ as .$dy = f\u0026rsquo;(x)\\ dx$ Given a differentiable function of two variables, i.e. .$z = f(x,y)$, we know both .$dx$ and .$dy$ are independent so we write: $$dz = f_x(x,y)\\ dx + f_y(x,y)\\ dy = \\frac{\\delta z}{\\delta x}dx + \\frac{\\delta z}{\\delta y}dy$$ If the partial derivatives .$f_x$ and .$f_y$ exist near .$(a,b)$ and are continuous at .$(a,b)$, then .$f$ is differentiable at .$(a,b)$. 14.5 Chain Rule # Chain Rule # Suppose that .$z = f(g_1 (x_1,_{\\dots}, x_m), g_2 (x_1,_{\\dots}, x_m), g_n (x_1,_{\\dots}, x_m))$ is a differentiable function of the .$n$ variables .$g_1,_{\\dots}, g_n$ and is .$g_j$ is a differentiable function of the .$m$ variables .$x_1,_{\\dots}, x_m$. Then .$z$ is a function of .$x_1,_{\\dots}, x_m$ and $$ \\frac{\\delta f}{\\delta x_i} = \\frac{\\delta f}{\\delta g_1} \\frac{\\delta g_1}{\\delta x_i} + \\frac{\\delta f}{\\delta g_2} \\frac{\\delta g_2}{\\delta x_i} + \\dots + \\frac{\\delta f}{\\delta g_m} \\frac{\\delta g_m}{\\delta x_i}$$ for each .$i = 1,2,\\dots,m$ Implicit Differentiation # If .$F(x,y) = 0$ defines .$y$ implicitly as a function of .$x$ (that is, .$y = f(x)$, where .$F(x,f(x))= 0$ for all .$x$ in the domain of .$f$), then $$ \\frac{dy}{dx} = - \\frac{\\frac{\\delta F}{\\delta x}}{ \\frac{\\delta F}{\\delta y}} = - \\frac{F_x}{F_y}$$\nIf .$F(x,y,z) = 0$ defines .$z$ implicitly as a function of .$x,y$, then\n$$ \\frac{dz}{dx} = - \\frac{\\frac{\\delta F}{\\delta x}}{ \\frac{\\delta F}{\\delta z}} = - \\frac{F_x}{F_z};\\ \\ \\frac{dz}{dy} = - \\frac{\\frac{\\delta F}{\\delta y}}{ \\frac{\\delta F}{\\delta z}} = - \\frac{F_y}{F_z}$$\n14.6 Directional Derivatives and the Gradient Vector # Directional Derivatives # Direction Derivative For function .$f$ at .$(x_0, y_0)$ in the direction of unit vector .$\\hat u = \\langle a, b \\rangle$ is $$D_{\\hat u} f(x_0, y_0) = \\lim_{h\\to 0} \\frac{f(x_0 + ha, y_0 + hb) - f(x_0, y_0)}{h} = $$ $$\\dots = \\nabla f \\cdot \\hat u = f_x (x,y) a + f_y (x,y) b$$ if the limit exists (for the former) and if .$f$ is a differentiable function of .$x$ and .$y$ (for the latter) That is, .$\\hat u = \\hat i = \\langle 1, 0 \\rangle$ for .$D_\\hat{i} = f_x$ and .$\\hat u = \\hat j = \\langle 0, 1 \\rangle$ for .$D_\\hat{j} = f_y$ Differentiability is important because it means that as you approach the surface very closely, it looks more and more like the tangent plane. Directional derivatives can be thought of as the slope of the tangent line at a given point This definition can be extrapolated to (three variables/higher dimensions) trivially, shown below with gradient vectors Gradient Vector # Gradient: If .$f$ is a function of variable .$x,y,z$, then the gradient of .$f$ is the vector function .$\\nabla f$ defined by $$\\nabla f(x,y,z) = \\big\\langle f_x(x,y,z), f_y (x,y,z), f_z (x,y,z) \\big\\rangle = \\frac{\\delta f}{\\delta x}\\hat i + \\frac{\\delta f}{\\delta y}\\hat j + \\frac{\\delta f}{\\delta z}\\hat k$$ We can now use the gradient to re-write our directional derivative equation as $$D_{\\hat u} f(x,y,z,\\dots) = \\nabla f(x,y,z,\\dots) \\cdot \\hat u$$ We can re-write this using the definition of the dot product as $$D_{\\hat u} f(x,y,z,\\dots) = \\Vert \\nabla f \\Vert \\Vert \\hat u \\Vert \\cos\\theta =\\Vert \\nabla f \\Vert \\cos\\theta $$ Maximizing the Directional Derivative # Suppose .$f$ is a differentiable function of two or three variables. The maximum value of the directional derivatives .$D_\\hat{u} f(\\vec x)$ is .$\\Vert \\nabla f(\\vec x ) \\Vert$ and it occurs when .$\\hat u$ has the same direction as the gradient vector .$\\nabla f(\\vec x)$ We can see from the definition above that since the max of .$\\cos\\theta$ is .$1$ when .$\\theta = 0$, therefore the max of the directional derivative occurs at the same angle (when .$\\hat u$ has the same direction of .$\\nabla f$ ) TL;DR: .$D_\\hat{u} f(\\vec x)$ is maximized if .$\\vec u = \\frac{\\nabla f}{\\Vert \\nabla f \\Vert}\\bigg\\vert_\\vec{p}$ at point .$\\vec p$ Tangent Planes to Level Surfaces # If .$f(x,y) = k$ is a curve, then .$F(x,y,z) = k$ is a surface Let .$\\vec r(t)$ be a space curve on the surface .$F(x,y,z) = k$ Let .$\\vec r(t_0) = \\langle x_0, y_0, z_0 \\rangle$ for some .$t_0$ (at some time, the space curve passes through some point which is on the surface) We know .$F(\\vec r(t)) = k$, thus, using the chain rule: $$0 = \\frac{\\delta F}{\\delta x} \\frac{\\delta x}{\\delta t} + \\frac{\\delta F}{\\delta y} \\frac{\\delta y}{\\delta t} + \\frac{\\delta F}{\\delta z} \\frac{\\delta z}{\\delta t} = \\nabla F(x_0, y_0, z_0) \\cdot (\\vec r(t_0))\u0026rsquo;$$ The gradient vector at .$P$, .$F(x_0, y_0, z_0)$, is perpendicular to the tangent vector .$\\vec r\u0026rsquo;(t)$ to any curve .$C$ on .$S$ that passes through .$P$ In English: The direction of the gradient is always perpendicular to the level surface at every point We can then define the tangent plane to the level surface .$F(x,y,z) = k$ at .$P(x_0, y_0, z_0)$ as the plane that passes through .$P$ and has normal vector .$\\nabla F(x_0, y_0, z_0)$: $$\\nabla F\\big\\vert_{(x_0, y_0, z_0)} \\cdot \\langle x-x_0, y-y_0, z-z_0 \\rangle = 0$$ We can also write this in the symmetric equation form: $$\\frac{x-x_0}{F_x(x_0, y_0, z_0)} = \\frac{y-y_0}{F_y(x_0, y_0, z_0)} = \\frac{z-z_0}{F_z(x_0, y_0, z_0)}$$ 14.7 Maximum and Minimum Values # Critical Points # A function of two variables has a local maximum at .$(a,b)$ if .$f(x,y) \\leq f(a,b)$ when .$(x,y)$ is near .$(a,b)$ [This means that for .$f(x,y) \\leq f(a,b)$ for all points .$(x,y)$ in some disk with center .$(a,b)$.] The number .$f(a,b)$ is called a local maximum value. If .$f(x,y) \\geq f(a,b)$ when .$(x,y)$ is near .$(a,b)$, then .$f$ has a local minimum at .$(a,b)$ and .$(x,y)$ is a local minimum value. If the inequalities above hold for all points .$(x,y)$ in the domain of .$f$, then .$f$ has an absolute maximum (or absolute minimum) at .$(a,b)$ If .$f$ has a local maximum or minimum at .$(a,b)$ and the first-order partial derivatives of .$f$ exist there, then .$f_x(a,b) = 0$ and .$f_y(a,b) = 0$. If the graph of .$f$ has a tangent plane at a local maximum or minimum, then the tangent plane must be horizontal .$(a,b)$ is a Critical Point of .$f$ if .$f_x(a,b) = 0$ and .$f_y(a,b) = 0$, or if one of these partial derivatives does not exist Thus, if .$f$ has a local maximum or minimum at .$(a,b)$, then .$(a,b)$ is a critical point of .$f$ In other words, at a critical point, a function could have a local maximum or a local minimum or neither (saddle point). Second Derivatives Test: Suppose the second partial derivatives of .$f$ are continuous on a disk with center .$(a, b)$, and suppose that .$f_x(a, b) = 0$ and .$f_y(a, b) = 0$ (that is, .$(a, b)$ is a critical point of .$f$ ). Let $$D = D(a, b) = \\begin{vmatrix}f_{xx} \u0026amp; f_{xy}\\\\ f_{yx} \u0026amp; f_{yy}\\end{vmatrix} = f_{xx}(a, b) f_{yy}(a, b) - (f_{xy}(a, b))^2$$\nIf .$D \u0026gt; 0$ and .$f_{xx}(a, b) \u0026gt; 0$ (or .$f_{yy}(a, b) \u0026gt; 0$), then .$f(a, b)$ is a local minimum. If .$D \u0026gt; 0$ and .$f_{xx}(a, b) \u0026lt; 0$ (or .$f_{yy}(a, b) \u0026lt; 0$), then .$f(a, b)$ is a local maximum. If .$D \u0026lt; 0$, then .$f(a, b)$ is a saddle point If .$D = 0$, the test gives no information: .$f$ could be any of the above Note that in the first two tests, it\u0026rsquo;s implied that .$f_{xx}$ and .$f_{yy}$ have the same sign The determinant is a Hessian matrix Extreme Points # Just as a closed interval contains its endpoints, a closed set in .$\\mathbb{R}^2$ is one that contains all its boundary points. For instance, the disk .$D = \\{(x,y) \\vert x^2 + y^2 \\leq 1 \\}$ is a closed set because it contains all of its boundary points (which are the points on the circle .$r = 1$) But if even one point on the boundary curve were omitted, the set would not be closed A bounded set in .$\\mathbb{R}^2$ is one that is contained within some disk \u0026ndash; it is finite in extent Extreme Value Theorem for Functions of Two Variables: If .$f$ is continuous on a closed, bounded set .$D$ in .$\\mathbb{R}^2$, then .$f$ attains an absolute maximum value .$f(x_1, y_1)$ and an absolute minimum value .$f(x_2, y_2)$ at some points .$(x_1, y_1)$ and .$(x_2, y_2)$ in .$D$. If .$f$ has an extreme value at .$(x_1, y_1)$, then .$(x_1, y_1)$ is either a critical point of .$f$ or a boundary point of .$d$. To find the absolute maximum and minimum values of a continuous function .$f$ on a closed, bounded set .$D$: Find the values of .$f$ at the critical points of .$f$ in .$D$. Find the extreme values of .$f$ on the boundary of .$D$. The largest of the values from steps 1 and 2 is the absolute maximum value; the smallest of these values is the absolute minimum value. 14.8 Lagrange Multipliers # We use Lagrange Multipliers to find critical points of a surface .$f$ given some constraining surface .$g$ Method of Lagrange Multipliers To find the maximum and minimum values of .$f(x,y,z)$ subject to the constraint .$g(x,y,z) = k$ [assuming that these extreme values exist and .$\\nabla \\neq 0$ on the surface .$g(x,y,z) = k$]:\nFind all values of .$x, y, z$, and .$\\lambda$ such that $$\\nabla f(x,y,z) = \\lambda \\nabla g(x,y,z)$$ $$g(x,y,z) = k$$ Evaluate .$f$ at all the points .$(x, y, z)$ that result from the first step. The largest of these values is the maximum value of .$f$; the smallest is the minimum value of .$f$. We can decompose the first equation and use the second equation to get $$f_x = \\lambda g_x;\\ \\ f_y = \\lambda g_y;\\ \\ f_z = \\lambda g_z;\\ \\ g(x,y,z) = k$$ Notice we don\u0026rsquo;t care what .$\\lambda$ is, only that it exists Two Constraints # We can use Lagrange multipliers for two constraints .$f$ and .$g$ too: $$\\nabla f(x_0, y_0, z_0) = \\lambda \\nabla g(x_0, y_0, z_0) + \\mu \\nabla h(x_0, y_0, z_0)$$ Likewise, we can decompose the equation above to get the following five equations: $$f_x = \\lambda g_x + \\mu h_x$$ $$f_y = \\lambda g_y + \\mu h_y$$ $$f_z = \\lambda g_z + \\mu h_z$$ $$g(x,y,z) = k$$ $$h(x,y,z) = c$$ "},{"id":40,"href":"/cogsci-c100/problem/","title":"14: Problem Solving, Creativity, \u0026 Decision Making","section":"CogSci C100","content":" Problem Solving # Steps involved # Studies have found that college education improves people’s problem solving/reasoning ability\nThe ability to reason that’s developed through learning about rules of reasoning, etc. in the natural and social sciences can generalize to other domains (e.g., everyday problems) May be one reason that those with a college education have been found to handle stress better than those who have not gone to college Every problem contains an initial state, goal state, and obstacles Approaches: use of algorithms vs. heuristics # Algorithm: a methodical, logical rule or procedure that guarantees a solution to a particular problem\nOne example is exhaustive search Trying to solve an anagram by testing out all possible letter combinations one by one Problem with this is that it is very time-consumiang Heuristic: a rule-of-thumb strategy that allows one to reduce the number of operations that are tried in solving a problem\nLooking only for pronounceable letter combinations (like \u0026ldquo;le\u0026rdquo; or \u0026ldquo;sa\u0026rdquo; when trying to solve an anagram) Most everyday problems are solved using heuristics Heuristics are speedier but also more error-prone than algorithms Strategies for improving problem solving # Understand the problem # Elevator in high rise building problem Represent problem effectively using symbols # Represent the problem effectively using symbols, matrices, diagrams, and visual images Example: Buddhist Monk Problem: At sunrise one morning, a Buddhist monk began to climb a tall mountain. He sometimes climbed the path quickly, and he sometimes went more slowly. From time to time, he stopped along the way to rest or to eat the fruit he had brought with him. Finally he reached the temple, just a few minutes before sunset. At the temple, he meditated for several days. Then he began his descent back along the same path. He left the temple at sunrise and walked at a varying pace as before. Of course though, he walked down the hill more quickly than when he was walking up the hill. Demonstrate that there must be a spot along the path that the monk will pass on both trips at exactly the same time of day. Represent the problem in a way that is appropriate to problem solver’s background knowledge\nMeans-end heuristic # Means-end heuristic: divide problem into sub-problems and try to reduce difference between initial state and goal state for each of sub-problems\nNewell and Simon developed General Problem Solver program (1957) asa way of solving complex problems using means-end analysis Hobbits-and-Orcs Problem:\nImagine that 3 Hobbits and 3 Orcs all arrive at the right side of a riverbank, and they all want to cross to the left side. Fortunately, there is a boat. However, the boat is small, and it can hold only two creatures at one time. Moreover, whenever there are more Orcs than Hobbits on one side of the river, the Orcs will immediately attack the Hobbits and gobble them up.. Therefore, you must be absolutely certain that you never leave more Orcs than Hobbits on any riverbank. How would you solve this dilemma?\nAnswer Sometimes it may be necessary to move backwards (hobbits-and-orcs problem) \u0026ndash; problem-solvers are often unwilling to do this! Tower of Hanoi Analogy approach # Analogy approach: problem solving is often a matter of finding a useful analogy between the present problem or situation and some other problem or situation with which you are more familiar Candle problem: find a way to attach the candle to a wall so that it burns properly using no other objects than those on the table Answer Can find solution to candle problem through a deliberate search for a useful analogy: Shelf ➜ tack box Represent information efficiently # Represent information efficiently and find shortcuts Stick-configuration problem: remove exactly five sticks from the pattern so that only three squares are left, with no sticks left over Can find solution to stick-configuration problem by using a heuristic that involves thinking in terms of squares rather than sticks: \u0026ldquo;Find three squares, out of the six available, which if preserved, would allow for the removal of exactly five sticks\u0026rdquo; Answer Promote use of parallel processing rather than serial processing # Promote use of parallel processing rather than serial processing Expert anagram solvers say that solutions simply seem to \u0026ldquo;pop out\u0026rdquo; (parallel processing) In contrast, novices generally use serial processing Foster insight # Foster insight: finding a solution is a matter of perceptual reorganization — any conditions that would allow your thought and perceptual processes to run more freely might help\nObstacles to Problem Solving # Confirmation bias: tendency to search for information that confirms one’s preconceptions\nWason card problem Statement: \u0026ldquo;If a card has vowel on one side, then it has an even number on the other side\u0026rdquo; Accuracy is enhanced when task describes a concrete social situation In general, research has found that humans tend to be much better at reasoning with deontic conditional than they are with ordinary, nondeontic conditionals (with the Wason card problem being one example) Deontic conditionals have to do with permissions, prohibitions etc. Statement: \u0026ldquo;If a person is drinking beer, they must be over 21 years of age\u0026rdquo; 73% of students who tried drinking-age problem made correct selections, as opposed to 0% in the standard, abstract form of task Why are people are better at reasoning with deontic conditionals? Theory that when we solve problems with deontic conditionals, we are using a specialized module for monitoring social exchanges and detecting cheaters This is the cheater detection module (Cosmides and Tooby) Why should there be a cheater detection module? Cooperative behavior presumably has a genetic basis However, an individual who takes advantage of cooperators without reciprocating will likely do better than one who cooperates Ex: They gain your trust, then steal all your bananas So how could the genes for cooperative behavior ever have become established? Enter the cheater detection module… Confirmation Bias And Social Judgments # Study in which participants asked to interview other student to determine if interviewee was an introvert or extravert Participants who tested for extraversion tended to find interviewees extraverted and vice versa because of tendency to ask questions that confirmed trait Rosenhan study Confirmation bias may have important implication for medical diagnosis/psychotherapy Therapist may form a less than accurate first impression of a patient and then only ask questions geared toward confirming that view Be willing to entertain the possibility that you may be wrong!\nMental set # Mental set: tendency to approach a problem in a particular way, especially a way that has been successful in the past but may not be helpful in solving a new problem\nNine dot problem: connect dots by drawing four continuous straight lines without lifting your pencil from the paper Match-triangle problem: assemble six matches to form four equilateral triangles, each side of which is equal to the length of one match Horse-and-rider problem: place B on A in such a way that the cowboys are correctly astride their horses Functional fixedness # Functional fixedness: tendency to think of things only in terms of their usual functions; ex. Candle problem\nInsight vs. Non-Insight Problems # Insight: a sudden and often novel realization of the solution to a problem\nContrasts with strategy-based solutions Insight problems are solved when answer appears suddenly Non-insight problems are solved gradually (e.g., using means-end heuristics) Neural Basis of Insight # Some critical components of insight are preferentially associated with the right cerebral hemisphere At the moment when people solve problems by insight, relative to solving identical problems by analytic processing: EEG shows a burst of highfrequency gamma-band EEG activity over the right anterior temporal lobe fMRI shows a corresponding change in blood flow in this area (Kounios and Beeman, 2014) Direct stimulation of right frontal-temporal cortex coupled with inhibition of left frontal- temporal cortex enhances solving of insight problems Insightful individuals show greater right hemisphere activity at rest, relative to analytic individuals Performance is enhanced or unaffected if you verbalize strategies while solving non-insight problem Performance is disrupted if you verbalize strategies while solving insight problem Differences may perhaps be explained by hemispheric specialization – left hemisphere is especially skilled at logical reasoning and language processing; the right is more spatial and holistic\nPriming people to think about the distant future biases them to think abstractly, causing them to perform better on insight and creativity tasks Conversely, priming people to think about the near future biases them to think concretely, causing them to perform better on analytic tasks (Förster, Jens, Friedman et al, 2004) Insightful individuals (Kounios and Beeman, 2014) Show more externally oriented attention (reduced occipital alphaband EEG activity) than do analytical individuals during the resting state Show greater internal focus of attention during the preparation phase prior to the presentation of the insight problem indicated by: Activation of anterior cingulate (detection of weak activation of nondominant solution possibilities) Increase in alpha-band activity over right occipital cortex Probably analogous to common behavior of closing or averting one’s eyes to avoid distractions that would otherwise interfere with intense mental effort Positive mood facilitates insight, probably by increasing attentional scope to include weakly activated solution possibilities (i.e., \u0026ldquo;wide-angle vision\u0026rdquo;) Relation between broad associations/insight and positive mood is bidirectional Inducing faster reading by manipulating paced reading makes participants feel more positively (Bar, M., 2009) Reading causes the activation of concepts, and presumably, faster reading activates more concepts, which could be seen as analogous to a massive increase in broad associative activation In contrast, \u0026ldquo;rumination,\u0026rdquo; which involves dwelling on a narrow theme, is associated with depression Similarity between insight and jokes in that both involve restructuring Creativity # Definition of creativity # Creativity: finding a solution that is novel and useful\nTo foster creativity and insight, promote conditions that allow thought and perceptual processes to run more freely Increase intrinsic motivation: Intrinsic motivation (measured by concentration on task, exploration, and enjoyment of task) promotes high levels of creativity \u0026ldquo;The labor of love aspect is important.\u0026quot;\n\u0026ndash; Nobel Laureate Arthur Schawlow\nReduce anxiety # Extrinsic motivation (being offered a reward for being creative) can reduce creativity (Glucksberg, 1962) Participants who expected no particular reward for solving candle problem solved the problem more quickly than those who were told they might win a $20 reward! Conditions that increase anxiety (knowing that your work will be evaluated, having someone watch while you work, being offered a reward for being creative) tend to reduce creativity\nPromote lightheartedness # Conditions that promote lightheartedness tend to facilitate solution Students who had just watched a comedy film were far more successful in solving candle problem than those who had seen either a serious film or no film(Isen, Daubman, \u0026amp;Nowicki, 1987) Strategies for enhancing creativity # Brainstorming # Brainstorming: where you get people together and just try to get ideas out, to get them up on the board \u0026ndash; and completely suspend criticism (nothing kills creativity like the threat of criticism!)\nIncubation # Incubation: taking a break when you get stuck on a problem may encourage creativity\nHelps you break out of mental set and see problem in new way In one series of experiments, three groups of people read complex information (e.g., about apartments or European football matches) Group 1 stated preference immediately after reading information about four possible options Group 2, given several minutes to analyze the information, made slightly smarter decisions Group 3, whose attention was distracted for a time, enabling their minds to engage in automatic, unconscious processing of the complex information, performed best (Strick, Dijksterhuis, \u0026amp; Baaren, 2010) Does a messy desk promote creativity? # College students were randomly assigned to spend time in adjacent office spaces (Vohs, Redden \u0026amp; Rahinel, 2013) One room was exquisitely neat, the other was wildly cluttered with papers and other work-related detritus Those in messy spaces generated ideas that were significantly more creative, according to two independent judges Participants were also offered an apple or a chocolate bar as they exited Those who sat in the orderly office were twice as likely to choose the apple than those who sat amid the mess) Decision Making # Decision making: assessing and choosing among several alternatives\nRules less clear than in logical reasoning Decision-making biases # Decision-making heuristics are typically useful in our daily lives Errors occur because we use heuristics beyond the range for which they are intended Availability bias # Availability heuristic or bias: estimating the likelihood of events based on their availability in memory: if instances come readily to mind, we presume such events are common\nAre tornados more likely to occur in Kansas or Texas? People’s estimates of violent crimes, insanity pleas Clinical applications When people are encouraged to recall pleasant events form their past, they judge pleasant events to be more likely in their future In contrast, when asked to recall unpleasant events, they judge unpleasant event to be more likely in future Framing # Framing: Drawing different conclusions from the same information, depending on how that information is presented\nImagine two surgeons explaining the risk of an upcoming surgery One explains that during this type of surgery, 10% of people die The other explains that 90% survive In real-life surveys, patients and physicians overwhelmingly say the risk is greater when they hear that 10% die Choosing to save for retirement U.S. companies once required employees who wanted to contribute to a retirement plan to opt in by choosing a lower take-home pay A new law allowed employers to automatically enroll their employees in the plan but with the option of opting out Either way, the decision to contribute is the employee’s, but under the new \u0026ldquo;opt-out\u0026rdquo; arrangement, one analysis of 3.4 million workers found that enrollments soared from 59% to 86% Base-rate fallacy # Base-rate fallacy: tendency to ignore the base rate in evaluating information\nProblem: A patient tests positive for a nasty disease The test has a 99% accuracy rate The disease afflicts 1 in 100 people What are the chances the patient actually has the disease? Answer The intuitive answer is 99%, but the correct answer is 50%\nEven though the test is highly reliable, it will still misdiagnose one person in every 100 That means that, in a population of 10,000, there will be 99 people without the disease who will test positive 99 people with the disease will also test positive Bayes’ Theorem # $P(A)$ = probability of event A or probability of disease in general population $P(B)$ = probability of event B or probability of a positive test result $P(A|B)$ = probability of observing event A if B is true or probability of having disease if test is positive $P(B|A)$ = probability of observing event B if A is true or probability of testing positive if one has disease \\(\\) $$P(A|B) = \\frac{P(B|A)}{P(A)} = \\frac{0.99 \\times 0.01}{198/10,000} = 50\\% $$\nPeople generally do not answer the medical diagnosis question correctly, but there is evidence that biological systems, such as our visual perception, language learning, and reward systems may utilize Bayesian principles Anchoring and adjustment bias # Anchoring and adjustment heuristic: Relying too heavily on the first piece of information seen\nStudy in which real estate agents asked to estimate value of a particular house with different list prices Real estate salesmen may show \u0026ldquo;set-up\u0026rdquo; properties first: \u0026ldquo;The house I got them spotted for looks really great after they’ve first looked at a couple of dumps.\u0026rdquo; (Northcraft \u0026amp; Neal, 1987) Overconfidence # Overconfidence: tendency to overestimate the accuracy of one’s beliefs and judgments\nHas many potentially detrimental effects Ex: In wars, each side tends to overestimate its own chances of success Dunning-Kruger effect: ignorance of one’s own incompetence\nStudents scoring at the low end of grammar and logic tests believed they had scored in the top half (Williams, Dunning \u0026amp; Kruger, 2013) In a set of six studies looking at people’s confidence in their performance on intellectual tasks, participants completed tests involving logical reasoning, intuitive physics, or financial investment Results: the more they approached such tasks in a \u0026ldquo;rational\u0026rdquo; (i.e., consistent, algorithmic) manner \u0026ndash; as opposed to more variable or ad hoc approaches \u0026ndash; the more confident they become, irrespective of whether they are correct (Kruger \u0026amp; Dunning, 1999) However, overconfidence may have adaptive value People who err on the side of overconfidence live more happily and find it easier to make tough decisions (Anderson, Brion, \u0026amp; Moore, 2012) Depressed people actually tend to be more accurate in judging their beliefs and judgments, as well as their degree of control over a situation (Alloy \u0026amp; Abramson, 1979; Gotlib \u0026amp; Meltzer, 1987) \u0026ldquo;The secret of success is to never face facts.\u0026quot;\n\u0026ndash; Gertrude Stein\nBelief perseverance # Belief perseverance: clinging to one’s initial conceptions after the basis on which they were formed has been discredited\nHigh school students were shown either an effective or confusing instructional film attributed their success/failure on the post-test to innate ability \u0026ndash; even when informed by the researchers that the film was responsible for their success/failure Once belief that \u0026ldquo;I’m not very smart\u0026rdquo; forms, it tends to persist This has important implications for early childhood education Given supposedly new research findings on a controversial issue, such as capital punishment, climate change, or politics, that was mixed in its results, people inevitably saw the study as supporting their own beliefs Role of emotions in effective decision making # Feelings are indispensable for rational decisions!\nLack of emotional responses leads to poor decision-making The orbital frontal cortex (OFC) connects three major regions of the brain: The cortex (the thinking brain) The amydala (the emotional brain) The brain stem (the reptilian brain for automatic response) Patients with damage to the prefrontal-amygdala circuit Make disastrous choices in their business and personal lives Can obsess endlessly over a decision as simple as when to make an appointment Case of neuropsych patient \u0026ldquo;Elliott\u0026rdquo; \u0026ldquo;At the Supreme Court level where we work, 90% of any decision is emotional. The rational part of us supplies the reasons for supporting our predilections.\u0026rdquo; \u0026ndash; Justice William Douglas\nIn making purchasing decisions, are you better off weighing all the pros and cons of the various options or going with your gut instinct? Shoppers were asked after leaving different stores how much time they had spent deliberating before they bought what they bought Researcher called shoppers a few weeks later to find out how happy they were with their purchases (Dijksterhuis, Bos, Nordgren et al., 2006) Results: In the case of low-cost items, like oven mitts and shampoo, the longer people had spent deliberating, the more satisfied they were with their purchases The reverse was true of more complicated and expensive purchases, like furniture, cars, or homes \u0026ldquo;When making a decision of minor importance, I have always found it advantageous to consider all the pros and cons. In vital matters, however, such as the choice of a mate or a profession, the decision should come from the unconscious, from somewhere within ourselves. In the important decisions of personal life, we should be governed, I think, by the deep inner needs of our nature.\u0026rdquo; \u0026ndash; Freud\n"},{"id":41,"href":"/cogsci-c100/developement/","title":"15: Cognitive Development","section":"CogSci C100","content":" The malleability of the young brain # Environmental factors play a huge role in cognitive development Rats raised in \u0026ldquo;enriched\u0026rdquo; environments with other rats and lots of toys develop larger brains (greater cortical mass) than those raised in \u0026ldquo;impoverished\u0026rdquo; environments Similarly, there is evidence to suggest that a stimulating environment is more likely to facilitate development of a child’s neural connections Most brain development occurs between the ages of 0 and 2 Less than 3% of general population are intellectually challenged, but 10% to 30% of those in lower socioeconomic groups \u0026ndash; research indicates that this is primarily due to environmental factors Researchers found that pioneering day-care program at the University of North Carolina (Smart Start Initiative) cut incidence of intellectual disability by as much as 80% among kids whose unstimulating home environment put them at high risk for low IQ By age 3, privileged children have heard 30 million more words than those who are underprivileged Postmortem brain analyses reveal that highly educated people die with more synapses (17% more in one study) than those less educated due to greater neural development during younger years Importance of touch # Early experience of neglect can result in cognitive as well as social deficits\nMice that are not licked by their mothers as pups are more prone to developing learning and memory impairments later in life Premature infants that are physically stroked in incubators show superior cognitive as well as physical and emotional development It is estimated that simply stroking premature infants in neonatology wards is saving hospitals across the country approximately a billion dollars a year! Comparison of brain of neglected Romanian infant raised in orphanage with that of a typical American infant\nCognitive development in early childhood # Piagettian stages # Controversies in Cognitive Science # Do children suddenly develop specific cognitive abilities at a certain age? That is, are developmental milestones discrete or continuous?\nEx: Object permanence at age 8 months Ex: Theory of mind at age 4 years Piaget’s theory of cognitive development Theory of how humans acquire, construct, and use knowledge Piaget observed that children of different ages made different kinds of mistakes when solving problems This led him to believe that children are not just \u0026ldquo;little adults\u0026rdquo; who know less; rather, they think and speak differently He proposed that humans progress through four developmental stages Ex: Object permanence: Understanding that object continues to exist even though they cannot see it Infants do not understand object permanence, which is why they respond to the game of peek-a-boo Once they develop object permanence, they quickly lose interest in the game Infants also won’t reach under cloth for toy that is hidden According to Piaget, object permanence is one of most important accomplishments in sensorimotor stage (age 0-2 years) Formal operations stage (over age 11) \u0026ndash; child develops ability to engage in hypothetical and deductive reasoning and to think about abstract concepts\nChild may know that 4 + 1 is odd, that 6 + 1 and 8 + 1 are odd However, before this stage, doesn’t understand that if you add one to any even number, the result will be odd Child is asked to discover what makes a pendulum go fast or slow (length of string, weight of object, or initial force that sets pendulum in motion) Before this stage, doesn’t understand that one needs to vary one factor and hold the others constant Strengths # Provides good overview of children’s thinking at different points Fascinating observations Weaknesses # Stage model depicts children’s thinking as being more consistent than it actually is Draw a line to show how the water line would look. 50% or male undergraduates and 75% of female undergraduates failed this \u0026ldquo;formal operations\u0026rdquo; test! (Sholl \u0026amp; Liben, 1995) Later research found that children are more cognitively competent than Piaget recognized Figure A shows a bottle with some water in it. In Figure B, the bottle has been tilted.\nUnderstates contribution of the social world Does not explain underlying mechanisms As mentioned, research has indicated that children are more cognitively competent than Piaget recognized; Ex: Object permanence: Piaget thought understanding of object permanence developed in infants around the age of 8 months However, Renée Baillargeon argued that Piaget’s finding was rooted in lack of motor ability in infants since experiments required infant to manually search for the hidden object by pulling a cover off to reveal the object More recent studies have indicated that infants as young as 3.5 months of age and perhaps younger understand that objects continue to exist when hidden, that they can’t just disappear Babies show surprise when object seems to just disappear, demonstrating rudimentary understanding of object permanence\nAlternate Theory: Vygotsky and Scaffolding # Rejected Piagettian idea that children’s cognitive development happens in stages Held that children develop independently of stages as the result of social interactions Development ideally happens in the zone of proximal development, that is, what we can do with the help of a \u0026ldquo;more knowledgeable other\u0026rdquo; Scaffolding can be used to support a child in developing skills Development of self-recognition # Rouge test of self-recognition\nSpot of red rouge is surreptitiously placed on child’s nose, then child is placed in front of mirror 15-month-olds respond by touching own nose to feel or rub off rouge A younger child touches mirror or tries to look behind it to find red-nosed child The only other animals capable of passing rouge test are other apes – chimpanzees, bonobos, orangutans, gorillas \u0026ndash; dolphins, orcas, elephants, and magpies Mindreading # Mindreading: the ability to understand other people’s mental state\nAllows us to make sense of other people Allows us to coordinate our behavior with theirs Key to human social interaction Roots of mindreading in early childhood lie in pretend play Pretend play # Pretend play, which typically emerges around 14 months, is considered a major milestone in cognitive and social development\nTends to follow a standard trajectory: Self-directed: Pretending to carry out familiar activity, e.g., drinking from empty cup Other-directed: Pretending that some object has properties it doesn’t have, e.g., pretending that a doll is saying something Object substitution: Pretending that some object is a different object, e.g., that banana is a telephone In pretend play, some of infant’s primary representations of the world and other people become \u0026ldquo;decoupled\u0026rdquo; from their usual functions while preserving their ordinary meaning Both pretend play and mindreading involve metarepresentation \u0026ndash; use of a representation to represent another representation, rather than referring directly to the world Children with autism spectrum disorder show impoverished pretend play, as well as impairments in mindreading False Belief Task # Displacement Task One of the best-known tests for mindreading ability Tests whether children are able to abstract away from their own knowledge to understand that someone else can have different (and mistaken) beliefs about the world Container test Child is shown a familiar kind of container (M\u0026amp;Ms bag) that contains an unexpected object (marble) Asked to predict what other person will think is inside False belief task tests children’s theory of mind mechanism (TOMM) \u0026ndash; their ability to identify and reason about other people’s complex mental states, such as beliefs, desires, hopes, and fears Pretend play emerges during the second year of life, but children do not typically pass the false belief test until they are nearly 4 Indicates that the BELIEVES operation is much harder to acquire than the PRETENDS operation However, research by Kristine Onishi and Renée Baillargeon demonstrated that children may develop an implicit understanding of false belief well before age 4 Experiment similar to false belief displacement task measured looking time in 15- month old infants Results indicated that children looked significantly longer \u0026ndash; indicating surprise – when actor’s behavior violated expectations that someone with an understanding of false belief would have Suggests that children may develop an implicit understanding of false belief by 15 months, but that explicit understanding, involving explicit conceptual abilities manifested in verbal responses and explicit reflection, develops later The Mindreading System # TOMM (theory of mind mechanism) is the end point of the development of mindreading, but there are several stepping stones on the way\nHigh-level mindreading is a complex phenomenon that depends upon a complex system of lower-level mechanisms that emerge at different stages of cognitive development The intentionality detector: is responsible for perceptual sensitivity to purposeful movements The eye direction detector: makes it easier to identify where other people’s attention is focused The emotion detector: gives a basic sensitivity to emotions and moods, as revealed in facial expressions, tone of voice, etc. These first three basic components of mindreading are typically in place by the time the infant is 9 months old In addition, the shared attention mechanism (SAM): occurs when infants look at objects (and take pleasure in looking at objects) because They see that another person is looking at that object I see (Mother sees the cup) OR They see that the other person sees that they are looking at the object Mother sees (I see the cup) This requires infant to be able to embed representations \u0026ndash; to represent that an agent is representing someone else’s representation Makes possible a range of coordinated social behaviors and collaborative activities Children with autism spectrum disorder have difficulties with this type of joint attention There is a strong correlation between severity of social impairments and inability to engage in joint attention Attunement between caregiver and child \u0026ndash; child’s understanding that caregiver knows how he feels \u0026ndash; is critical for normal development From SAM emerges the empathizing system (TESS), which is responsible for affective responses to other people’s mood and emotions (as opposed to simply identifying them) In Baron-Cohen’s model, TESS is a component of TOMM, but he also acknowledges that TESS and TOMM are distinct and can come apart, e.g., in psychopathy Psychopaths can be very good at working out what is going on in other people’s minds According to criminal psychologist, Robert Hare (Baibak \u0026amp; Hare, 2007): About 1% of general population meets clinical criteria for psychopathy Around 3-4% of CEOs meet criteria More recent study by forensic psychologist Nathan Brooks indicated that around 21% of CEOs meet criteria for psychopathy, the same percentage as for prison inmates Ability of psychopaths to do well in business is due in part to their ability to read others accurately, as well as their charm, ruthlessness, and ability to thrive on chaos Two Models of Mindreading # Dedicated theory of mind system # Many cognitive scientists think there is a dedicated theory of mind system responsible for identifying and reasoning about other people’s beliefs, desires, and other propositional attitudes Evidence for this view is provided by neuroimaging studies that have identified a number of brain areas that show increased activation during mindreading tasks, including Medial prefrontal cortex Superior temporal sulcus Inferior parietal lobule (Saxe \u0026amp; Kanwisher) Simulation # In contrast, simulationists think that mindreading is carried out by \u0026ldquo;ordinary\u0026rdquo; information-processing systems that are co-opted for mindreading According to standard simulationism, we understand the psychological states of others by analogy with our own psychological states Research indicates that medial prefrontal cortex (MPFC) plays an important role in self-reflection, lending support to this theory Radical simulationism holds that mindreading does not involve representing other people’s psychological states but, rather, representing the world from their perspective Neurocognitive research has found that the same mechanism that mediates the experience of a particular emotion is recruited when a participant recognizes that emotion in someone else For instance, brain-injured patients show paired deficits \u0026ndash; in problems experiencing relevant emotions and in identifying emotions in others Fear S.M., a patient with damage to amygdala on both sides of the brain does not experience fear and is also significantly impaired in her ability to identify expression of fear in others (Adolphs \u0026amp; Trannel, 2000) Anger Neurotransmitter dopamine plays an important role in experience of anger In rats, level of aggression can be directly manipulated by raising/lowering rat’s dopamine level In humans, temporary blockage of dopamine production using sulpiride causes selective impairment in recognition of expression of anger (Lawrence, Goerendt \u0026amp; Brooks, 2007) Disgust Neuroimaging studies have shown that the insula is area of brain most associated with experience and recognition of disgust N.K., a patient with damage to the insula and basal ganglia, has severe difficulty both in experiencing and recognizing disgust in other (Calder, Keane, Manes et al., 2000) Damage to the somatosensory cortex severely compromises people’s ability to recognize and identify facial expressions (Kragel \u0026amp; LaBar, 2016) Theory that when we see a facial expression of an emotion, we unconsciously imagine ourselves making that expression and that is what helps us to identify the expression Injections of Botox impaired people’s ability to read facial emotions, as well as to experience emotions (Lewis, M.B., 2018; Neal \u0026amp; Chartrand, 2011; Havas, Glenberg, Gutowski et al., 2010) Mirror neurons: neurons located in premotor area of frontal lobe that provide a neural basis for observational learning\nSeeing a loved one’s pain triggers activity in many of the same brain regions as those activated in the person actually experiencing the pain (Iacoboni) Researchers have identified a set of neurons in the premotor cortex that lights up when participants hear someone munching on potato chips or ripping paper Same neurons flash when participants perform similar actions themselves People who display particularly strong activity in response to sound cues alone score higher on a questionnaire gauging their ability to put themselves in another person’s shoes (Gazzola, Aziz-Zadeh, \u0026amp; Keysers, 2006) Autism Spectrum Disorder (ASD) # Early research indicated that ASD is associated with impairments in emotion perception and empathy\nEmpathizing-systemizing theory (Baren-Cohen, 2009) People may be classified on the basis of their scores along two dimensions: Empathizing: reading facial expressions and gestures Systemizing: understanding things according to rules or laws, as in mathematical and mechanical systems Individuals with ASD are more likely to to score low on empathy and high on systemizing Also, parents and close relatives of those with ASD score higher on systemizing Other research indicating impairments in emotion perception and empathy in ASD Research has found atypical function in the occipital gyrus, fusiform gyrus, and amygdala, as well as in mirror networks, during face perception in those with ASD (Spezio, Adolphs, Hurley et al., 2007) Individuals with ASD primarily show deficient performance on face perception when those faces display emotional expressions (rather than a neutral expression) Also, eye-tracking studies have found that, whereas control participants fixate on major features of the face that convey emotions, such as the eyes, a majority of the time, those with ASD tend to fixate on portions of the face that do not contain core facial features A new theory, however, proposes that the fundamental problem in ASD is not social deficiency or lack of empathy, but on the contrary, a hypersensitivity to experience, which induces an overwhelming fear response(Patil, Melsbach, Hennig-Fast et al., 2016; Markram, Rinaldi, \u0026amp; Markram, 2007) \u0026ldquo;I can walk into a room and feel what everyone is feeling. The problem is that it all comes in faster than I can process it.\u0026rdquo;\nFollow-up neuroimaging research has indicated that in children with ASD may show impairments in performance on face perception tests and reduced activity in fusiform gyrus primarily because they are avoiding looking at people’s eyes due to the discomfort and anxiety they feel when they do that In the original studies, children with autism \u0026ndash; who are highly sensitive to environmental stimuli \u0026ndash; were placed in a deafening, claustrophobia-inducing MRI tube and instructed to perform tasks involving perception of faces It’s likely they either simply stared unfocused into space to try to calm themselves or just shut their eyes until the whole ordeal is over (Davidson) Follow-up study was done in which children were outfitted with eye-tracking goggles while in fMRI; Asked to classify faces as emotional or neutral\nChildren with autism classified about 85% of faces correctly Non-autistic controls had a 98% accuracy rate Children with autism also showed diminished activation in the fusiform gyrus However, children with autism spent an average of 20% less time looking into the eyes of the faces in the pictures compared with controls This explained virtually all of the variation in how activated the fusiform region was Children with autism also evidenced greater activity in the amygdala during the face perception task This was similarly correlated (negatively) with gaze fixation Looking at faces made these children profoundly uncomfortable, even fearful Only by looking away could they stop this onslaught \u0026ndash; which is what they did (Dalton, Nacewicz, Johnstone et al., 2005) Factors affecting cognitive development # Early studies found that breast-fed infants later score higher on IQ tests than formula-fed infants Mental stimulation # The earlier children start reading books with their families, the better their test scores later 6-year-olds given 6 weeks of music lessons (keyboard or voice) or drama lessons Music group showed greater increases in full scale IQ (about 4 points) than those in drama and control groups Drama group showed substantial improvements in adaptive social behavior that were not evident in the music group (Mol \u0026amp; Bus, 2011) (Schellenberg, 2004) Retroviruses # Research has found that exposure to infectious diseases around time of birth is a better predictor of IQ than education\nEveryone has retroviruses in their bodies, but the body normally works hard to keep them under tight control However, infections by agents like toxoplasma, herpes, or the Epstein-Barr virus around the time of birth destabilizes the defense system The retrovirsuses pour into the newborn’s blood and brain fluid, triggering a huge immune response that causes inflammatory cytokines to flood the system… Active forms of retroviruses found in 49% of those with schizophrenia, compared with just 4% of normal controls (Perron, Mekaoui, Bernard et al., 2008)\nAttempts to develop intelligence # John Stuart Mill: Child prodigy; took walks with father every morning during which he summarized what he had learned the previous day Age 3: reading Greek philosophy, Plato and Herodotus, in original Age 8: reading Cicero, Virgil in Latin; responsible for teaching younger sibs Latin Age 12: mastered calculus Age 20: nervous breakdown Virginia Axline’s Dibs: In Search of Self Age 3: psychologists thought he was intellectually disabled/autistic Given intensive play therapy Turned out that he was very, very bright \u0026ndash; scored at genius level on IQ tests Parents were overachievers who had pushed him so hard to learn so much at such a young age that he basically just shut down Pushing a child too hard can backfire!\nOn the other hand, there are kids who push their parents i.e. Arjun Ayyangar: By the time he was 2 years old, he could… Name all the US presidents; identify all the states and their capitals, as well as countries around the world and their flags Name 80 symbols of elements from the periodic table Calculate squares and square roots In addition, he was learning German from his father, Spanish from his mother, French and ASL from his cousin; and four Indian dialects from his grandmother His parents didn’t think there was anything unusual about any of this until they started talking with the neighbors… \u0026ldquo;Arjun wants to learn something new everyday. He would get bored and cry if we didn\u0026rsquo;t teach him things.\u0026rdquo; He’s now a 22-year-old musical prodigy who regularly performs for charity fundraisers as well as in piano competitions \u0026ndash; http://www.thekidshalloffame.com Role of motivation in learning # Studies of expert tutors found that these tutors will do anything to avoid telling a child that he is wrong \u0026ndash; even to the point of lying! Tutoring \u0026ndash; and learning \u0026ndash; is 90% motivational To teach child to read, need to find subject that child is interested in (e.g., sports magazines) Growth mindset # \u0026ldquo;In a growth mindset, people believe that their most basic abilities can be developed through dedication and hard work… This view creates a love of learning and a resilience that is essential for great accomplishment\u0026rdquo;\n\u0026ndash; Carol Dweck\nResearch by Blackwell and Dweck (2007) on students who were predominantly minority and low achieving Control group was taught study skills Experimental group got study skills and a special module on how intelligence can be improved that was taught in two lessons totaling 50 minutes Those who got the special module showed dramatic improvement in study skills and grades They pushed themselves harder They tried new things This very brief intervention basically reversed the students’ longtime trend of decreasing academic scores The kids who had cultural reasons to be anxious about their skills were the ones most affected by the message, e.g., girls and math Recent research has found that Programs to develop growth mindset are most beneficial in students with low socioeconomic status or who are academically at risk (Sisk, Burgoyne, Sun et al., 2018) Low income students are less likely to hold a growth mindset than their wealthier peers However, low income students who exhibit a growth mindset show academic performance as high as that of fixed mindset students from higher income brackets (Claro, Paunescu, \u0026amp; Dweck, 2016) Parents and teachers with growth mindset do not necessarily pass that on A sustained focus on the process of learning is critical for developing growth mindset (Haimovitz \u0026amp; Dweck, 2017) Praise and Academic Success # Does praising kids boost their confidence and increase their likelihood of success?\n85% of American parents think it’s important to tell their kids that they’re smart But a growing body of research strong suggests that giving kids the label of \u0026ldquo;smart\u0026rdquo; does not prevent them from underperforming It might actually be causing it… Study by Carol Dweck and Lisa Blackwell\nFifth graders were given a nonverbal IQ test consisting of a series of fairly easy puzzles Researchers told each student their score, then gave them a single line of praise Half the children were praised for their intelligence: \u0026ldquo;You must be smart at this\u0026rdquo; Half were praised for their effort: \u0026ldquo;You must have worked really hard\u0026rdquo; Researchers used just a single line of praise to see how sensitive children were Students were given a choice of test for the second round, either… A test that would be more difficult than the first \u0026ndash; researchers told the kids that they’d learn a lot from attempting these puzzles; OR An easy test, just like the first Results: Of those praised for their effort, 90% chose the harder set of puzzles Of those praised for their intelligence, a majority chose the easy test The \u0026ldquo;smart\u0026rdquo; kids took the cop-out Final round: easy test like the first Those who had been praised for their effort significantly improved on their first score—by about 30% Those who’d been told they were smart did worse than they had at the very beginning—by about 20% What happened? If children who had been labeled \u0026ldquo;smart\u0026rdquo; tried hard and failed, they would lose their \u0026ldquo;smart\u0026rdquo; label, so better not to try hard Another problem with praising ability: image maintenance becomes the primary concern Over-praised kids are more competitive and more interested in tearing others down Students were given two puzzle tests (Dweck) Between the first and the second, they were offered a choice: Learning a new puzzle strategy for the second test OR Finding out how they did compared with other students on the first test Students praised for their intelligence chose to find out their class rank, rather than use the time to prepare for the second test When ego-boosting parents exclaim \u0026ldquo;Great job!\u0026rdquo; not just the first time a young child puts on his shoes but every single morning he does this, the child learns to feel that everything he does is special… (Lori Gottlieb) What starts off as healthy self-esteem can quickly morph into a self-absorption and sense of entitlement that looks a lot like narcissism As adults, they don’t like being told by a boss that their work might need improvement They feel insecure if they don’t get a constant stream of praise In general, it’s best to give specific, rather than general, praise \u0026ldquo;Really impressive how much effort you put into getting that ball\u0026rdquo; \u0026ldquo;Looking to pass that ball was a great move\u0026rdquo; Not just \u0026ldquo;You played great!\u0026rdquo; Aging and Cognitive Change # Cross-sectional evidence for intellectual decline Longitudinal evidence for intellectual stability Reason for difference: older cohort from era in which people generally lesseducated, less affluent, and raised in larger families Variability spread of scores in intellectual function greater in older adults than in younger adults Decline greatest in people with low verbal ability Age is less a predictor of intelligence than is proximity to death The good news: Studies have consistently found that there is a small group of people who do not show stress-induced psychological/physical deterioration as they age! "},{"id":42,"href":"/math-53/15/","title":"15: Multiple Integrals","section":"Math 53","content":" 15.1 Double Integrals over Rectangles # We describe closed rectangles in the form .$R = [a,b] \\times [c,b] = \\{(x,y) \\in \\mathbb{R}^2 \\vert a \\leq x \\leq b, c \\leq y \\leq d\\}$ Then we can write the solid .$S$ that lies above .$R$ as .$S = \\{(x,y,z) \\in \\mathbb{R}^3 \\vert 0 \\leq z \\leq f(x,y), (x,y) \\in R\\}$ To find the volume of this surface, we take a double integral $$V = \\iint_R f(x,y)\\ dA$$ Fubini\u0026rsquo;s Theorem: If .$f$ is continuous on the rectangle .$R = \\{(x,y) \\vert a \\leq x \\leq b, c \\leq y \\leq d\\}$, then $$\\iint_R f(x,y)\\ dA = \\int_a^b \\int_c^d f(x,y)\\ dy\\ dx = \\int_c^d \\int_a^b f(x,y)\\ dx\\ dy$$ More generally, this is true if we assume that .$f$ is bounded on .$R$, .$f$ is discontinuous only on a finite number of smooth curves, and the iterated integrals exist. Average Value # In the 2D case we could write the average as $$f_\\text{avg} = \\frac{1}{b-a} \\int_a^b f(x) dx$$ For 3D, instead of dividing by the change in just .$y$ (which was .$b-a$), we divide over the total area: $$f_\\text{avg} = \\frac{1}{A(R)}\\iint_R f(x,y)\\ dA$$ 15.2 Double Integrals over General Regions # Type I # If .$f$ is continuous on a type I region .$D$ such that $$D = \\{(x,y)\\ \\vert\\ a \\leq x \\leq b,\\ g_1(x) \\leq y \\leq g_2(x)\\ \\}$$ then $$\\int_D f(x,y)\\ dA = \\int_a^b \\int_{g_1(x)}^{g_2(x)} f(x,y)\\ dy\\ dx$$ Type II # If .$f$ is continuous on a type II region .$D$ such that $$D = \\{(x,y)\\ \\vert\\ c \\leq y \\leq d,\\ h_1(y) \\leq x \\leq h_2(y)\\ \\}$$ then $$\\int_D f(x,y)\\ dA = \\int_c^d \\int_{h_1(y)}^{h_2(y)} f(x,y)\\ dx\\ dy$$ Properties of Double Integrals # $$\\iint_D \\Big[ f(x,y) + g(x,y)\\Big]\\ dA = \\iint_{D} f(x,y)\\ dA + \\iint_{D} g(x,y)\\ dA$$ $$\\iint_D c\\cdot f(x,y)\\ dA = c \\iint_D f(x,y)\\ dA\\ \\text{ where $c$ is a constant}$$ $$\\iint_D f(x,y)\\ dA \\geq \\iint_D g(x,y)\\ dA \\ \\text{ if $f(x,y) \\geq g(x,y)$ for all $(x,y) \\in D$}$$ $$\\iint_D f(x,y)\\ dA = \\iint_{D_1} f(x,y)\\ dA + \\iint_{D_2} f(x,y)\\ dA\\ \\text{ for $D = D_1 \\cup D_2$}$$ $$mA(D) \\leq \\iint_D f(x,y)\\ dA \\leq MA(D)$$ If .$m \\leq f(x,y) \\leq M$ for all .$(x,y) \\in D$ 15.3 Double Integrals in Polar Coordinates # Recall that we can convert cartesian to polar with the following equations: $$r^2 = x^2 +y^2$$ $$x = r\\cos\\theta$$ $$y = r\\sin\\theta$$ We multiply .$r$ because an \u0026ldquo;infinitesimal\u0026rdquo; polar rectangle as an ordinary rectangle with dimensions .$r\\ d\\theta$ and .$dr$ and therefore has area .$dA = r\\ dr\\ d\\theta$ That is, the further out the polar rectangle is (the larger the .$r$), the larger the area of that rectangle is (this scale is .$\\propto r$) If .$f$ is continuous on a polar region of the form $$D = \\{(r,\\theta)\\ \\vert\\ \\alpha \\leq \\theta \\leq \\beta, h_1(\\theta) \\leq r \\leq h_2(\\theta) \\}$$ then $$\\iint_D f(x,y)\\ dA = \\int_\\alpha^\\beta \\int_{h_1(\\theta}^{h_2(\\theta)} r \\cdot f(r\\cos\\theta, r\\sin\\theta)\\ dr\\ d\\theta$$ 15.5 Surface Area # The area of the surface with equation .$z = f(x,y), (x,y) \\in D$, where .$f_x$ and .$f_y$ are continuous, is $$A(S) = \\iint_D \\sqrt{\\big[f_x(x,y)\\big]^2 + \\big[f_y(x,y)\\big]^2 + 1}\\ dA$$ Notice the similarities between the SA function and the line integral function, .$L = \\int_a^b \\sqrt{1 + \\big( \\frac{dy}{dx} \\big)^2}\\ dx$ 15.6 Triple Integrals # Fubini\u0026rsquo;s Theorem for Triple Integrals If f is continuous on the rectangular box B − fa, bg 3 fc, dg 3 fr, sg, then $$\\iiint_B f(x,y,z)\\ dV = \\int_r^s\\int_c^d\\int_a^b f(x,y,z)\\ dx\\ dy\\ dz$$ The iterated integral on the right side of Fubini\u0026rsquo;s Theorem means that we integrate first with respect to .$x$ (keeping .$y$ and .$z$ fixed), then we integrate with respect to .$y$ (keeping .$z$ fixed), and finally we integrate with respect to .$z$. Note that if .$f$ is separable this just becomes the product of three single-dimensional integrals, or one two-dimensional integral and two one-dimensional integrals. Fubini\u0026rsquo;s theorem still applies. Type 1 # $$\\iiint_E f(x,y,z)\\ dV$ = \\iint_D \\bigg[\\int_{u_1(x,y)}^{u_2(x,y)}f(x,y,z)\\ dz\\bigg]\\ dA$$\nType I # $$\\dots = \\int_a^b \\int_{g_1(x)}^{g_2(x)} \\int_{u_1(x,y)}^{u_2(x,y)}f(x,y,z)\\ dz\\ dx\\ dy$$ Type II # $$\\dots = \\int_c^d \\int_{h_1(y)}^{h_2(y)} \\int_{u_1(x,y)}^{u_2(x,y)}f(x,y,z)\\ dz\\ dy\\ dx$$ Type 2 # $$\\iiint_E f(x,y,z)\\ dV$ = \\iint_D \\bigg[\\int_{u_1(y,z)}^{u_2(y,z)}f(x,y,z)\\ dx\\bigg]\\ dA$$\nType 3 # $$\\iiint_E f(x,y,z)\\ dV$ = \\iint_D \\bigg[\\int_{u_1(x,z)}^{u_2(x,z)}f(x,y,z)\\ dy\\bigg]\\ dA$$\n15.7 Triple Integrals in Cylindrical Coordinates # Cylindrical to Cartesian # $$x = r\\cos\\theta$$ $$y = r\\sin\\theta$$ $$z = z$$ Cartesian to Cylindrical # $$r^2 = x^2 + y^2$$ $$\\tan\\theta = \\frac{y}{x}$$ $$z = z$$ Now we deal with integration in cylindrical coordinates. We have .$dV = dx\\ dy\\ dz$ Since .$z$ is the same, .$dz$ is the same. However, we can convert .$dx, dy$ to .$r\\ dr, dθ.$ since it is the same transformation as in polar coordinates. Thus the volume element in cylindrical coordinates is .$dV = r\\ dr\\ dθ\\ dz$. $$\\iiint_D f(x,y,z) dx\\ dy\\ dz = \\iiint_D f(r\\cos\\theta, r\\sin\\theta, z)\\cdot r\\ dr\\ d\\theta\\ dz$$ $$\\dots = \\int_\\alpha^\\beta \\int_{h_1(\\theta)}^{h_2(\\theta)} \\int_{u_1(r\\cos\\theta, r\\sin\\theta)}^{u_2(r\\cos\\theta, r\\sin\\theta)} f(r\\cos\\theta, r\\sin\\theta, z)\\cdot r\\ dz\\ dr\\ d\\theta$$ 15.8 Triple Integrals in Spherical Coordinates # Spherical Coordinates map .$(x,y,z) \\Longrightarrow (\\rho, \\theta, \\phi)$ .$\\rho$ is the distance between the origin and point; .$\\rho \\geq 0$ .$\\theta$ is the angle on the .$xy$ plane; .$\\theta \\in [0, 2\\pi]$ .$\\phi$ is the angle between the .$z$ axis and the .$xy$ plane; .$\\phi \\in [0, \\pi]$ We only need to consider half the sphere; the other half is already counted by the varying .$\\theta$. Spherical to Cartesian # $$x =\\rho \\sin \\phi \\cos \\theta$$ $$y =\\rho \\sin \\phi \\sin \\theta$$ $$z =\\rho \\cos \\phi$$ Cylindrical to Spherical # $$x =\\rho \\sin \\phi$$ $$\\theta = \\theta$$ $$z =\\rho \\cos \\phi$$ Cartesian to Spherical # $$\\rho = \\sqrt{x^2 + y^2 + z^2}$$ $$\\tan \\theta = \\frac{y}{x}$$ $$\\tan \\phi = \\frac{z}{\\sqrt{x^2 + y^2}}$$ $$\\iiint_E f(x,y,z)\\ dV = \\dots$$ $$\\dots = \\int_c^d \\int_\\alpha^\\beta \\int_a^b f(\\rho\\sin\\phi\\cos\\theta, \\rho\\sin\\phi\\sin\\theta, \\rho\\cos\\phi) \\cdot \\rho^2 \\sin\\phi\\ d\\rho\\ d\\theta\\ d\\phi$$ We have the volume element .$dV = dx\\ dy\\ dz$. By converting to cylindrical coordinates and doing some algebraic conversions we have .$dV = \\rho^2 \\sin\\phi d\\rho\\ d\\theta\\ d\\phi$: 15.9 Change of Variable in Multiple Integrals # We\u0026rsquo;ve done .$u$-sub before in single variable, as well as cartesian .$\\iff$ polar .$\\iff$ spherical in multivariable More generally, we can consider a change of variables that is given by a transformation .$T$ from the .$uv$-plane to the .$xy$-plane: .$T(u,v) = (x,y)$ where .$x = g(u,v), y = h(u,v)$ .$x,y$ must be differentiable in .$S$ We usually assume that .$T$ is a .$C^1$ transformation: .$g$ and .$h$ have continuous first-order partial derivatives A transformation .$T$ is really just a function whose domain and range are both subsets of .$\\mathbb{R}^2$ If .$T(u_1, v_1 = (x_1, y_1)$, then the point .$(x_1,y_1)$ is called the image of the point .$(u_1, v_1)$ That is, .$T$ transforms .$S$ into a region .$R$ in the .$xy$-plane called the image of .$S$, consisting of the images of all points in .$S$ If no two points have the same image, .$T$ is called one-to-one ( wiki) If .$T$ is a one-to-one transformation, then it has an inverse transformation .$T^{-1}$ from the .$xy$-plane to the .$uv$-plane + it may be possible to solve for .$u$ and .$v$ in terms of .$x$ and .$y$: .$u = G(x,y), v = H(x,y)$ If not one-to-one, then the transformation would \u0026ldquo;fold\u0026rdquo; over itself \u0026ndash; we would double-count some amount of area This change in variable affects the size of the region (the area/integral) The vector .$\\vec r (u, v) = g(u, v) \\hat i + h(u, v) \\hat j$ is the position vector of the image of the point .$(u, v)$. The tangent vector at .$(x_0, y_0)$ to the image curve of the lower side (.$v = v_0$) of .$S$: $$\\vec r_v = \\frac{\\delta x}{\\delta v} \\hat i + \\frac{\\delta y}{\\delta v} \\hat j$$ Similarly, the tangent vector at .$(x_0, y_0)$ to the image curve of the left side (.$u = u_0$) of .$S$: $$\\vec r_u = \\frac{\\delta x}{\\delta u} \\hat i + \\frac{\\delta y}{\\delta u} \\hat j$$ We can then find the area by calculating the cross product: $$\\vec r_u \\times \\vec r_v = \\begin{vmatrix} \\frac{\\delta x}{\\delta u} \u0026amp; \\frac{\\delta x}{\\delta v} \\\\ \\frac{\\delta y}{\\delta u} \u0026amp; \\frac{\\delta y}{\\delta v} \\end{vmatrix}$$ $$\\dots = \\frac{\\delta (x,y)}{\\delta (u,v)} = \\frac{\\delta x}{\\delta u} \\frac{\\delta y}{\\delta v} - \\frac{\\delta x}{\\delta v}\\frac{\\delta y}{\\delta u}$$ This is the Jacobian of the transformation .$T$ given by .$x = g(u,v)$ and .$y = h(u,v)$ Formally, Suppose that .$T$ is a .$C^1$ transformation whose Jacobian is nonzero and that .$T$ maps a region .$S$ in the .$uv$-plane onto a region .$R$ in the .$xy$-plane. Suppose that .$f$ is continuous on .$R$ and that .$R$ and .$S$ are type I or type II plane regions. Suppose also that .$T$ is one-to-one, except perhaps on the boundary of .$S$. Then, $$\\iint_R f(x,y) dA = \\iint_S f(x(u,v), y(u,v)) \\Bigg\\vert \\frac{\\delta (x,y)}{\\delta (u,v)} \\Bigg\\vert\\ du\\ dv$$ That is, .$dA = \\Bigg\\vert \\frac{\\delta (x,y)}{\\delta (u,v)} \\Bigg\\vert\\ du\\ dv$ General Solving Steps # Write down transformations (.$x$ and .$y$ in terms of .$u$ and .$v$) Find the Jacobian Rewrite the equations with .$u$ and .$v$ Sketch the new region + find new bounds Integrate on the new field with Jacobian Triple Integrals # We can also let .$T$ be a transformation that maps a region .$S$ in .$uvw$-space onto a region .$R$ in .$xyz$-space by means of the equations: $$x = g(u,v,w)$$ $$y = h(u,v,w)$$ $$z = k(u,v,w)$$ Then, the Jacobian is a .$3 \\times 3$ determinant: $$ \\frac{\\delta (x,y,z)}{\\delta (u,v,w)} = \\begin{vmatrix} \\frac{\\delta x}{\\delta u} \u0026amp; \\frac{\\delta x}{\\delta v} \u0026amp; \\frac{\\delta x}{\\delta w} \\\\ \\frac{\\delta y}{\\delta u} \u0026amp; \\frac{\\delta y}{\\delta v} \u0026amp; \\frac{\\delta y}{\\delta w} \\\\ \\frac{\\delta z}{\\delta u} \u0026amp; \\frac{\\delta z}{\\delta v} \u0026amp; \\frac{\\delta z}{\\delta w} \\end{vmatrix} = dA$$ "},{"id":43,"href":"/cogsci-c100/therapy/","title":"16: Cognition \u0026 Therapy","section":"CogSci C100","content":" Use of AI in diagnosis and treatment of psychological disorders # Expert Systems # Expert systems programs are typically applied in narrowly defined domains to solve very determinate problems, such as diagnosing specific medical disorders\nMYCIN Early expert system developed in 1970s to diagnose infectious diseases Used knowledge base of about 600 heuristic rules about infectious diseases derived from clinical experts and textbooks Accuracy rate of 69%, which turned out to be significantly higher than diagnoses by infectious disease experts Are expert systems now able to accurately diagnose psychological disorders? Deep learning (a subtype of machine learning that we discussed earlier, remember?) has been used to integrate data obtained from multiple imaging modalities, such as fMRI, MRI, and PET, to effectively classify some psychological disorders Most studies have focused on diagnosis of neurocognitive disorders and ADHD, probably due to accessibility of large publicly available neuroimaging data sets Accuracy rates above 90% have been achieved in some studies A few of these studies were also able to accurately predict disease trajectory Studies classifying other mental disorders, including schizophrenia, autism, Parkinson’s, depression, substance abuse, and epilepsy, are also accumulating (Durstewitz, Koppe, and Meyer-Lindenberg, 2019) In addition, we’re nearing the point where we may be able to tailor treatment for psychological disorders based on neuroimaging data People whose depression improved most after behavioral activation therapy had greater brain network connectivity between the anterior insular cortex (involved in assigning importance to events) and the middle temporal gyrus (involved in subjective experience of emotion) Differences in brain structure and neural connectivity among different regions predicted how well CBT reduced symptoms of those with social anxiety disorder Estimates of treatment outcome were five times more accurate than estimates using a behavioral assessment tool alone (Whitfield-Gabrieli, Ghosh, Nieto-Castanon et al., 2015) In one study, participants with social anxiety disorder were asked to identify letters behind which occasionally lurked pictures of angry faces Those who struggled most to avoid being distracted by the threatening stimuli—indicated by more activity in dorsal anterior cingulate cortex— showed the most symptom improvement when treated with CBT (Crowther, Smoski, Minkel et al., 2016) (Klumpp, Fitzgerald, Piejko et al, 2015) Network Neuroscience # There has been a shift in recent years to studying networks or functional connectivity, that is, how different brain regions work together, rather than just brain regions themselves Traditional localizationist research almost always involves watching how brain activity changes while a person is engaged in a particular task Network research, in contrast, can be done when people are doing nothing at all This gets closer to a person’s natural state Ex: Someone with a psychological disorder will have the disorder even when they are not doing a working memory task The network approach has proven to be particularly well suited to understanding schizophrenia Healthy human brains are generally small-world networks Most nodes make only short-range connections to one another and tend to be clustered into densely connected modules At least one node in each module, however, is a hub, which means that it makes long-range connections all over the network Researchers found that the brain of a person with schizophrenia tends to be measurably less of a small-world network It still tends to be organized into modules, but those modules aren’t as densely connected (Liu, Lian, Zhou et al., 2008; Lynall, Bassett, Kerwin et al., 2010) Frontoparietal network # People with major depressive disorder tend to show Reduced connectivity between regions in the frontoparietal network (FN), which is involved in cognitive control of attention and emotional regulation Reduced connectivity between frontoparietal systems and parietal regions of dorsal attention network involved in attending to the external environment Increased connectivity in the default network (DN), which is believed to support internally-oriented and self-referential thought, such as rumination Above pattern may reflect deficits in emotional regulation and depressive biases toward ruminative thoughts at cost of attending to external world (Kaiser, Andrews-Hann, Wager et al., 2015) Frontoparietal network (left), Default network (right) Patients experiencing hallucinations often have lesions in different parts of the brain, but in each case, the lesion was tightly associated with the extrastriate visual cortex Symptoms often correlate with damage to a specific circuit, not a specific location (Boes, Prasad, Liu, 2015) Other important applications of network neuroscience: # Networks can be used not just for diagnosis, but to identify PTSD patients who are unlikely to respond to psychotherapy If scientists can determine the circuits that a highly invasive technique like deep brain stimulation (DBS) is acting upon, they might be able to achieve similar results with a nonsurgical approach like TMS Clinicians can access regions buried in the brain, like those targeted in DBS treatments for Parkinson’s, through areas closer to the surface (Etkin, Maron-Katz, Wu et al., 2019) (Michael Fox, neurologist at Harvard Medical School) Also, it might be that the best way to help a symptom that maps to a circuit is actually multiple electrodes, or multiple stimulation sites Tumor problem Effects of Trauma on Cognition and Emotion # The Neurobiology of Trauma # Some psychologist have claimed that most, if not all, psychological disturbances are the result of trauma High rates of co-morbidity of PTSD with other psychological disorders 58% of patients with depression; 54% with borderline personality disorder; 40% with bipolar disorder; 28% with schizophrenia 58% of people with migraines experienced abuse as children Neurologically, the experience of trauma is characterized by: State of hyperarousal Dissociation Hyperarousal # PTSD is associated with hypersensitization of amygdala circuitry/HPA-axis PTSD is associated with oversecretion by hypothalamus of corticotropin-releasing hormone (CRH), the main stress hormone used to mobilize emergency fight-or-flight response The greater the degree of arousal during and immediately after a traumatic incident, the more likely it is that person will have PTSD or other neuropsychiatric symptoms following trauma Symptoms less severe if patient is treated quickly after trauma (e.g., with propranolol) Neural changes of PTSD make a person more susceptible to further traumatizing: exposure to even mild stress when young makes person more vulnerable to trauma-induced brain changes later in life The role of glucocorticoids # A 20% reduction in volume of hippocampal formation has been found in combat veterans and those subjected to severe abuse in childhood The hippocampus contains large numbers of glucocorticoid receptors and plays a key role in regulating production of glucocorticoids through negative feedback People who were abused as children have more methyl groups capping the genes for glucocorticoid receptors \u0026ndash; this prevents the cell from reading its DNA and is correlated with suicide risk PTSD is associated with low levels of glucocorticoids Excessively low, as well as excessively high, levels of glucocorticoids can cause dysfunction In general, glucocorticoids increase the stress response, but a certain amount of glucocorticoid is also necessary to turn off the stress response Sensitivity to Context # How good you are at taking into account the context in which you find yourself in regulating your emotional responses Measures how attuned you are to the social environment, e.g., when a behavior might be socially acceptable vs. offensive Associated with Differences in hippocampal volume, particularly anterior hippocampus \u0026ndash; the part closest to the amygdala \u0026ndash; as well as strength of connections between hippocampus and other brain regions, especially the prefrontal As mentioned, PTSD is associated with loss of hippocampal volume In particular, the anterior hippocampus is involved in regulating behavioral inhibition in response to different contexts The anxiety and even terror that people with PTSD feel is quite appropriate in certain contexts, such as a battleground The problem is that they experience these feelings in nontraumatic contexts, such as if they hear pounding from a construction site Both human toddlers and monkeys tend to freeze when they encounter an unfamiliar situation, a form of anxiety called behavioral inhibition Dissociative aspect of trauma # Little or no connections forms between the neocortex and memory storage and emotional centers (e.g., amygdala) In addition, PTSD is associated with overactivation of brain’s opioid system: this may cause a numbing of feelings, a sense of being cut off from life or from concern about others’ feelings Psychotherapy # Help patient understand that jumpiness and nightmares, hypervigilance and panics, are part of symptoms of PTSD \u0026ndash; this makes the symptoms themselves less frightening Help patient regain some sense of control over what is happening to them (security measures) Aid patient in mourning any loss that may have been brought on by the trauma Have patient retell and reconstruct the story of the trauma in an environment of safety (play therapy may be used with children) Patient should be encouraged to retell the traumatic events as vividly as possible, retrieving every detail \u0026ndash; their emotional reactions as well as the events themselves The goal is to put the entire memory into words to capture parts that may have been dissociated from conscious recall By putting sensory details and feelings into words, memories are brought more under control of the neocortex, where the reactions they kindle can be rendered more manageable (Siegel, D., 2001) Also, retelling the story in a surrounding of safety and security, in the company of a trusted therapist, enables a sense of security, rather than terror, to be experienced in connection with the trauma memories May need to focus more on Cognitive Behavior Therapy, relaxation techniques, or physical exercise initially, especially in cases of severe trauma Treatment of PTSD # Recent Research suggests that it is the element of helplessness that makes a given event subjectively overwhelming It’s the feeling that your life is in danger and there is nothing you can do to escape it \u0026ndash; that’s the moment the brain change begins Theory that trauma experience entails a tremendous urge to take action (fight or flight), at the same time that one is paralyzed by sense off helplessness A number of newly developed therapies (bioenergetics, sensorimotor psychotherapy, etc.) thus focus on physical selfexpression as way to foster emotional release and heal trauma Therapeutic effects of emotional release through journaling \u0026ndash; Pennebacker study: undergraduates were told to write for 20 minutes each day for five days either about the most traumatic and stressful event of their life or about a trivial topic Students who wrote about the traumatic event were more upset immediately following writing the essays and showed elevated blood pressure compared to those who wrote about trivial topics However, over the following six months, they were significantly less likely to visit the student health service for illness and showed improved immune function Greatest improvement shown by those who expressed both positive and negative emotions and who were able to weave a narrative and find meaning in the experience Cognition and emotion in therapy # Psychodynamic therapy: Focuses on helping patient gain cognitive insight into unconscious roots of problem (e.g., early childhood experiences)\nTracing roots of negative thoughts and behaviors to allow you to see that they are based on beliefs that you simply picked up and not necessarily Truth with a capital T\nBased on view that the reason negative emotional patterns are so difficult to change is that they are not just established through conditioning, but are deeply interwoven into the way we try to gain love\n\u0026ldquo;See, Mommy/Daddy, I’m not any better than you are. Now will you love me?\u0026rdquo; \u0026ldquo;See, Mommy/Daddy, I’m a worthless guttersnipe just like you said I was. Now will you love me?\nEven when we rebel, we stay trapped in the same negative patterns\nEx: Woman with sexually repressive parents So how to release negative emotional patterns?\nBecome more aware of patterns of parental figures that you might have adopted In addition, transference can be used to gain greater awareness of those negative patterns Transference has to do with the way in which we internalize our parents and project them onto other people Ex: over-reacting to criticism/authority because that was the way our parent acted, and it made us feel worthless, powerless, not good enough, etc. Psychodynamic-Based Writing Exercise # Writing exercise to help clients become aware of negative emotional patterns they have picked up from parents and to \u0026ldquo;reclaim their power\u0026rdquo; What were your parents like during your childhood (personality, behavior)? What forms of punishment did your parents use? Describe a childhood scene in which you were angry with your parents Describe a childhood scene in which you were hurt, rejected, and/or sad due to your Mother or Father in the first person present tense Follow-up visualization exercise: What was it like for each of your parents growing up in their family? How did they feel? How were they punished? If we could read the secret history of our enemies, we should find in each man\u0026rsquo;s life sorrow and suffering enough to disarm all hostility. \u0026ndash; Longfellow\nClient-centered Therapy # Client-centered therapy: therapist provides unconditional positive regard so client can feel free to express their inner thoughts and emotions and be themselves This involves validating their emotional experience to strengthen their sense of identity \u0026ndash; not parroting their beliefs! \u0026ldquo;Just remember, son, it doesn’t matter whether you win or lose \u0026ndash; unless you want Daddy’s love.\u0026rdquo; Mirroring: therapist’s reflecting back to the client an understanding of his inner state, leading patient to feel acknowledged and understood This is not merely a matter of listening sympathetically and parroting the client! Emotionally Focused Therapy (EFT) # It emphasizes \u0026ldquo;listening with the heart\u0026rdquo;: meaning listening not for the literal meaning of a partner\u0026rsquo;s words, but for the feelings that lie beneath their words EFT is grounded in attachment theory: it focuses on one’s need for security and one’s sense of dependency on one’s partner; it legitimates vulnerability Based on the premise that there is almost always a soft emotion behind a hard emotion, and that if that soft emotion can be accessed and then accepted by the partner, it almost always dissolves the conflict Cognitive Behavior Therapy # Cognitive Behavioral Therapy (CBT): changing negative emotions by identifying and challenging associated negative thoughts\nAims to help one develop cognitive awareness \u0026ndash; the ability to see thoughts as simply thoughts, rather than the \u0026ldquo;Truth\u0026rdquo; Ex: Woman with severe depression who thinks she will die if her husband leaves her Therapist challenges this belief by reminding her that she was able to function very well before she married Ex: Children with depression learn to tune into their thoughts when facing tough situations and to imagine alternatives to negative thought Ex: Client with Obsessive-Compulsive Disorder (OCD) is taught to: Relabel: identify what’s real and what isn’t and refuse to be misled by obsessive thoughts \u0026ldquo;That compulsion is bothering me again,\u0026rdquo; rather than \u0026ldquo;I feel like I need to wash my hands again\u0026rdquo;\nReattribute: you understand that those thoughts and urges are merely false messages being sent from your brain \u0026ldquo;That compulsion is bothering me because I have a medical condition called OCD that is related to a biochemical imbalance in my brain\u0026rdquo;\nRefocus: turn your attention to more constructive behavior, knowing that by doing so, you are actually changing the way your brain works in an extremely healthy and wholesome way \u0026ldquo;Passing mental states become lasting neural traits\u0026rdquo;\nRevalue: you come to see compulsions and obsessive thoughts as the useless garbage they really are as soon as they arise Variant: mindfulness: bringing emotions to cognitive behavior therapy\nThoughts and emotions are inextricably linked, but in mindfulness, you use emotions to guide your thoughts, rather than using thoughts to guide your emotions as in standard CBT View that emotions are a foolproof guidance system telling you whether you are thinking a thought that is in line with your own best interests How does it feel when you criticize someone? How does it feel when you are praising someone or something out of a genuine sense of appreciation? Becoming more aware of the \u0026ldquo;feeling tone\u0026rdquo; behind thoughts makes it much easier to choose the positive thoughts and let go of the negative One way to become more aware of the feeling tone behind thoughts is to reach for the \u0026ldquo;best feeling thought\u0026rdquo; that one can access at any given moment (on a deep, not superficial level, of course!) It’s about becoming more aware of your emotions and giving yourself permission to feel good/prioritizing feeling good:\n\u0026ldquo;Nothing is more important than that I feel good.\u0026rdquo;\nThis also means focusing more on how you feel and less on what other people may think \u0026ldquo;Mental health is getting to the point where you don’t give a rip what anyone else thinks or says or does.\u0026rdquo;\nMindfulness # Dialectical behavioral therapy (DBT): Variant of mindfulness therapy originally developed by Marsha Linehan to treat people with borderline personality disorder\nCombines standard cognitive-behavioral techniques for emotion regulation and reality-testing with concepts of distress tolerance, acceptance, and mindful awareness derived from Buddhist mediation practice DBT involves several different \u0026ldquo;dialectics\u0026rdquo; or oppositions, including: Acceptance and openness to change Therapist aims to accept and validate the client’s feelings at any given time while also informing the client that some feelings and behaviors are maladaptive, and showing them better alternatives \u0026ldquo;Each of you is perfect the way you are \u0026hellip; and you can use a little improvement.\u0026rdquo; \u0026ndash; S. Suzuki\nYou don’t have to like it \u0026ndash; you may not want to change, but you need to change in order to get what you want First therapy that has been empirically demonstrated to be effective in treating borderline personality disorder; also used to treat spectrum mood disorders, including self-injury Clients are taught to practice mindfulness to enhance distress tolerance DBT focuses on helping clients to recognize and accept in a non-judgmental way negative situations and emotions, rather than becoming overwhelmed or hiding from them Includes many innovative techniques Once clients become aware of negative emotional state, they can then effect a repair by engaging in some activity they enjoy, forcing themselves to think about something else, or doing something that has an intense feeling, e.g., taking a hot shower, snapping a rubber band against their wrist, or holding ice in their hand when they feel the urge to cut Therapy also incorporates boundary setting, e.g., client has option of calling therapist but only before they cut Affirmations # In CBT, visualization is often used in conjunction with affirmations Examples: \u0026ldquo;I am full of radiant health and energy.\u0026rdquo; \u0026ldquo;Infinite riches are freely flowing into my life.\u0026rdquo; \u0026ldquo;The world is a safe and friendly place.\u0026rdquo; \u0026ldquo;I always communicate easily and effectively.\u0026rdquo; \u0026ldquo;Everything good is coming to me easily and effortlessly.\u0026rdquo;\nBasic rules: Phrase affirmations in the present tense, not the future Phrase affirmations in the most positive way you can e.g., don’t say, \u0026ldquo;I no longer oversleep in the morning,\u0026rdquo; but rather \u0026ldquo;I now wake up on time and full of energy every morning.\u0026rdquo; Keep it short and simple! Focus of attention # How to \u0026ldquo;win friends and influence people\u0026rdquo;: shifting one’s focus of attention Research on CBT has shown that one of the most effective way to change another person’s behavior is to shift our focus of attention More specifically, the trick is to accentuate the positive and ignore the negative This is the basis of behavioral therapy, which is empirically validated to work on children and animals, as well as adults What Shamu Taught Me about a Happy Marriage Applying Principles of CBT # Use what trainers call \u0026ldquo;approximations,\u0026rdquo; rewarding small steps toward learning a new behavior Behavior: husband leaves dirty clothes all over floor Response: Thank him if he throws one dirty shirt into the hamper; if he throws in two, kiss him; step over soiled clothes on the floor without any sharp word Devise incompatible behaviors Behavior: Husband has habit of hovering over you when you cook Response: Pile up parsley for him to chop or cheese to grate at other end of kitchen island or set out bowl of chips and dip across the room Use least reinforcing syndrome (L.R.S.): any response, positive or negative, will fuel a behavior; if a behavior provokes no response, it typically dies away Behavior: husband habitually loses keys and tears around in panic searching Response: Ignore behavior, keep doing what you are doing Adopt animal trainers’ motto: \u0026ldquo;It’s never the animal’s fault\u0026rdquo; You just need to brainstorm new strategies when your \u0026ldquo;puppy\u0026rdquo; misbehaves Positive aspects # Listing positive aspects: practicing \u0026ldquo;seeing others as if the good in them were all of them\u0026rdquo; Sister Mrosla story of \u0026ldquo;listing positive attributes\u0026rdquo; exercise in her ninth grade classroom CBT and mindfulness are methods of engaging in self-directed neuroplasticity: changing our mind to change our brain "},{"id":44,"href":"/math-53/16/","title":"16: Vector Calculus","section":"Math 53","content":" 16.1 Vector Fields # A vector field in .$\\mathbb{R}^3$ is a function .$\\vec F$ on region .$E \\in \\mathbb{R}^3$ that assigns each point .$(x,y,z)$ a vector .$F(x,y,z)$ .$\\vec F$ is made up of component function: .$\\vec F(x,y,z) = \\langle P(x,y,z)\\hat i, Q(x,y,z) \\hat j, R(x,y,z) \\hat k\\rangle$ .$\\vec F$ is continuos iff its component vectors are continuos .$\\vec F$ is conservative (path taken doesn\u0026rsquo;t change work) iff potential function .$f(x,y,z)$ is a partial of .$\\vec F$ $$\\vec F = \\nabla f$$ Notice that the gradient lines are always perpendicular to the level sets If the function .$f$ is differentiable, .$\\nabla f$ at a point is either zero or perpendicular to the level set of .$f$ at that point. That is, that the gradient of a function is called a gradient field which is always conservative (the fundamental theorem of calculus for line integrals) Conversely, a (continuous) conservative vector field is always the gradient of a function 16.2 Line Integrals # We know that the distance (length) normally is .$L = \\int_a^b \\sqrt{(dx/dt)^2 + (dy/dt)^2}\\ dt$ Over a vector field, we can think of the function being the density of the line (or height of particle). Therefore, we say .$ds = \\int_a^b \\sqrt{(dx/dt)^2 + (dy/dt)^2}\\ dt$ and can write $$\\int_C f(x,y) ds = \\int_a^bf(x(t), y(t)) \\cdot \\sqrt{\\bigg(\\frac{dx}{dt}\\bigg)^2 + \\bigg(\\frac{dy}{dt}\\bigg)^2} dt$$ and for 3D in a slightly different form: $$\\int_a^b f (\\vec r (t) ) \\vert \\vec r\u0026rsquo;(t) \\vert \\Longrightarrow \\int_a^b f(x(t), y(t), z(t)) \\cdot \\sqrt{x\u0026rsquo;(t)^2 + y\u0026rsquo;(t)^2 + z\u0026rsquo;(t)^2} dt$$ We can write .$\\vec a \\to \\vec b$ as .$(1-t)\\vec a + t\\vec b$ with .$t\\in[0,1]$ Just like usual, we can break up un-integrable smooth curves, i.e $$\\int_a^z f (x,y) = \\int_a^b f_1(x,y) + \\int_b^c f_2(x,y) + \\dots \\int_{\\dots}^z f_n(x,y)$$ wrt variable # Opposed to the line integrals on .$f$ along .$C$ with respect to .$x$ both and .$y$, we can write line integral with respect to arc length as follows:\n$$\\int_C f(x,y) dx = \\int_a^b f(x(t), y(t)) \\cdot y\u0026rsquo;(t) dt$$ $$\\int_C f(x,y) dy = \\int_a^b f(x(t), y(t)) \\cdot x\u0026rsquo;(t) dt$$\nIt frequently happens that line integrals with respect to .$x$ and .$y$ occur together which we abbr as\n$$\\int_C P(x,y)\\ dx + \\int_C Q(x,y)\\ dy = \\int_C P(x,y)\\ dx + Q(x,y)\\ dy$$\nOrientation # When we parameterize a curve, we give it a direction Positive: Enclosed region .$D$ is always on the left as we traverse curve .$C$ (counter-clockwise) Negative: Enclosed region .$D$ is always on the right as we traverse curve .$C$ (clockwise) The orientation represents the direction of the line The positive direction corresponding to increasing values of the parameter .$t$ Doesn\u0026rsquo;t matter for regular line integrals: .$\\int_C f(x,y) ds = \\int_C f(x,y) ds$ Deals with distance, .$ds$, which doesn\u0026rsquo;t depend on direction Does matter for field line integrals: .$\\int_C f(x,y) dx \\neq \\int_C f(x,y) dy$ Deals with displacement, .$dx/dy$, which depends on direction Let .$\\vec F$ be a continuous vector field defined on a smooth curve .$C$ given by a vector function .$\\vec r(t), t\\in[a,b]$. Then the line integral of .$\\vec F$ along .$C$ is $$W = \\int_C \\vec F \\cdot d\\vec r = \\int_a^b \\vec F ( \\vec r (t) ) \\cdot (\\vec r (t))\u0026rsquo;\\ dt = \\int_C \\vec F \\cdot \\vec T\\ ds$$\n.$\\vec T(x,y,z)$ is the unit tangent vector at the point .$(x,y,z)$ on .$C$ .$\\vec F \\cdot \\vec T = \\vec F(x,y,z) \\cdot \\vec T(x,y,z)$ This equation says that work is the line integral with respect to arc length of the tangential component of the force. Then, for a non-conservative force i.e .$F = \\langle P(x,y,z), Q(x,y,z), R(x,y,z) \\rangle$ $$W = \\int_a^b P(\\vec r(t)) \\cdot x\u0026rsquo;(t) + Q(\\vec r(t)) \\cdot y\u0026rsquo;(t) + R(\\vec r(t)) \\cdot z\u0026rsquo;(t)$$ $$ \\Longrightarrow \\int_C P\\ dx + Q\\ dy + R\\ dz$$ If we flip the curve\u0026hellip; \u0026hellip;and integrate with respect to just .$x$ or .$y$ then the value flips: $$\\int_{-C}f(x,y)\\ dx = - \\int_C f(x,y)\\ dx$$ Since .$\\Delta x$ and .$\\Delta y$ change sign when we reverse the orientation of .$C$. \u0026hellip;and integrate with respect to arc length, the value of the line integral does not change when we reverse the orientation of the curve: $$\\int_{-C}f(x,y)\\ ds = \\int_C f(x,y)\\ ds$$ This is because .$\\Delta s$ is always positive. 16.3 Fundamental Thm for Line Integrals # Let .$C$ be a smooth curve given by the vector function .$\\vec r (t), t\\in[a,b]$. Let .$f$ be a differentiable function of two or three variables whose gradient vector .$\\nabla f$ is continuous on .$C$. Then $$\\int_C \\nabla f \\cdot d\\vec r =\\int_C \\vec F \\cdot d\\vec r = f(\\vec r(b) ) f(\\vec r(a))$$ Independence of Path # Suppose .$C_1$ and .$C_2$ are two piecewise-smooth curves (which are called paths) that have the same initial point .$A$ and terminal point .$B$. Therefore, .$\\int_{C_1} \\nabla f \\cdot d\\vec r = \\int_{C_2} \\nabla f \\cdot d\\vec r$ whenever .$\\nabla f$ is continuous In other words, line integrals of conservative vector fields are independent of path (they only depend on the start and end points) Plane Curves # .$\\int_C \\vec F \\cdot d\\vec r$ is independent of path in .$D$ iff .$\\int_C \\vec F \\cdot d\\vec r = 0$ for every closed path .$C$ in .$D$ Closed: A curve with the same end and start points: .$\\vec r(b) = \\vec r(a)$ That is, only vector fields that are independent of path are conservative. Space Curves # Suppose .$\\vec F$ is a vector field that is continuous on an open connected region .$D$. If .$\\int_C \\vec F \\cdot d\\vec r$ is independent of path in .$D$, then .$\\vec F$ is a conservative vector field on .$D$; that is, there exists a function .$f$ such that .$\\nabla f = \\vec F$. Open: For every point .$P$ in .$D$ there is a disk with center .$P$ that lies entirely in .$D$. (So .$D$ doesn\u0026rsquo;t contain any of its boundary points.) Connected: Any two points in .$D$ can be joined by a path that lies in .$D$. $$$$ If .$\\vec F (x,y) = P(x,y) \\hat i + Q(x,y) \\hat j$ is a conservative vector field, where .$P$ and .$Q$ have continuous first-order partial derivatives on a domain .$D$, then throughout .$D$ we have $$ \\frac{\\delta P}{\\delta y} = \\frac{\\delta Q}{\\delta x}$$ The converse of the theorem above is true on only simple curves: curves that don\u0026rsquo;t intersect itself anywhere between its endpoints A simply-connected region in the plane is a connected region .$D$ such that every simple closed curve in .$D$ encloses only points that are in .$D$ Intuitively speaking, a simply-connected region contains no hole and can\u0026rsquo;t consist of two separate pieces. .$\\vec F = \\langle P, Q \\rangle$ is a conservative vector field on an open simply-connected region .$D$ iff both .$P$ and .$Q$ have continuous first-order partial derivatives and $$ \\frac{\\delta P}{\\delta y} = \\frac{\\delta Q}{\\delta x} \\text{ throughout } D$$ 16.4 Green\u0026rsquo;s Theorem # Green\u0026rsquo;s Theorem: Let .$C$ be a positively oriented, piecewise-smooth, simple closed curve in the plane and let .$D$ be the region bounded by .$C$. If .$P$ and .$Q$ have continuous partial derivatives on an open region that contains .$D$, then $$\\int_C \\vec F \\cdot d\\vec r = \\oint_C P\\ dx + Q\\ dy = \\iint_D \\bigg(\\frac{\\delta Q}{\\delta x} - \\frac{\\delta P}{\\delta y}\\bigg) dA$$\n.$dA = dx\\ dy = r \\cdot dr\\ d\\theta = \\dots$ .$\\vec F(x,y) = \\langle P(x,y), Q(x,y)\\rangle$ .$\\oint$ implies an integral over a closed curve The proof (for Green\u0026rsquo;s thm + remaining sections) is (are) too much for me to write out here, the book does a good job One important takeaway is the the shape doesn\u0026rsquo;t have to be \u0026ldquo;nice\u0026rdquo; \u0026ndash; we can break up any shape into parts that are either Type I or II. Even though we will have overlapping lines, they cancel one another out (leaving only the boundaries) because of orientation Green\u0026rsquo;s Theorem should be regarded as the counterpart of the Fundamental Theorem of Calculus for double integrals Recall the Fundamental Theorem of Calculus is .$\\int_a^b F\u0026rsquo;(x)\\ dx = F(b) - F(a)$ In both cases there is an integral involving derivatives (.$F\u0026rsquo;, \\delta Q/\\delta x, \\delta P/\\delta y$) on the left side of the equation. And in both cases the right side involves the values of the original functions (.$F, Q, P$) only on the boundary of the domain. (In the one-dimensional case, the domain is an interval .$[a,b]$ whose boundary consists of just two points, .$a$ and .$b$.) Application: Finding Area # Since the area of .$D$ is .$\\iint_D 1 dA$, we wish to choose .$P$ and .$Q$ so that $$ \\frac{\\delta Q}{\\delta x} - \\frac{\\delta P}{\\delta y} = 1$$ Some examples of .$P/Q$ combos are $$P(x,y)= 0$$ $$Q(x,y)= x$$ $$A = \\oint_C x\\ dy$$ $$P(x,y)=-y$$ $$Q(x,y)= 0$$ $$A = -\\oint_C y\\ dx$$ $$P(x,y)=-y/2$$ $$Q(x,y)= x/2$$ $$A = \\frac{1}{2} \\oint_C x\\ dy - y\\ dx$$ Planimeters (a measuring instrument used to determine the area of an arbitrary two-dimensional shape) is an example of an application of Green Theorem We can also use Green\u0026rsquo;s to prove our last equation found in 16.3: $$\\oint_C \\vec F \\cdot d \\vec r = \\oint_C P\\ dx + Q\\ dy = \\iint_R \\bigg( \\frac{\\delta Q}{\\delta x} - \\frac{\\delta P}{\\delta y}\\bigg)\\ dA = \\iint_R 0\\ dA = 0$$ A curve that is not simple crosses itself at one or more points and can be broken up into a number of simple curves. We have shown that the line integrals of .$\\vec F$ around these simple curves are all .$0$ and, adding these integrals, we see that .$\\int_C \\vec F \\cdot d\\vec r = 0$ for any closed curve .$C$. Therefore .$\\int_C \\vec F \\cdot d\\vec r$ is independent of path in .$D$. It follows that .$F$ is a conservative vector field.\n16.5 Curl and Divergence # Recall that .$\\nabla = \\langle \\frac{\\delta}{\\delta x} \\frac{\\delta}{\\delta y} \\frac{\\delta}{\\delta z} \\rangle$ 3b1b video going over this section Curl # Vector of the rotation caused by the field at a given point $$\\nabla \\times \\vec F(x,y,z) = \\begin{bmatrix}\\hat i \u0026amp; \\hat j \u0026amp; \\hat k\\\\ \\frac{\\delta}{\\delta x} \u0026amp; \\frac{\\delta}{\\delta y} \u0026amp; \\frac{\\delta}{\\delta z}\\\\ P \u0026amp; Q \u0026amp; R\\end{bmatrix} = \\dots $$ If .$\\vec F$ is conservative then .$\\text{curl($\\vec F$) = 0}$ We know if .$\\vec F$ is conservative then .$\\vec F = \\nabla f$ for some .$f(x,y,z)$ Crossing .$\\nabla F$ with .$\\nabla$ we get .$\\langle \\frac{\\delta^2 f}{\\delta y \\delta z} - \\frac{\\delta^2 f}{\\delta z \\delta y}, \\dots \\rangle = \\langle 0, 0,0 \\rangle$ Divergence # If .$\\vec F (x,y,z)$ is the velocity of a fluid (or gas), then .$\\text{div}(\\vec F (x,y,z))$ represents the net rate of change (wrt time) of the mass of fluid (or gas) flowing from the point .$(x,y,z)$ per unit volume. In other words, .$\\text{div}(\\vec F (x,y,z))$ measures the tendency of the fluid to diverge from the point .$(x,y,z)$. If .$\\text{div}(\\vec F (x,y,z)) = 0$, then .$F$ is said to be incompressible. Scalar of the amount of \u0026ldquo;flow\u0026rdquo; at a given point \u0026ndash; how much does the field expand/contract at a given point? $$\\nabla \\cdot \\vec F(x,y,z) = \\langle \\frac{\\delta}{\\delta x} \\frac{\\delta}{\\delta y} \\frac{\\delta}{\\delta z} \\rangle \\cdot \\langle P, Q, R \\rangle$$ Fun fact: .$\\text{(div(curl($\\vec F$)))} = \\nabla \\cdot (\\nabla \\times \\vec F)= 0$ We can use this fact to find if there exists a vector field .$\\vec G$ such that .$\\text{curl($\\vec G$)} = \\vec H$ because .$\\text{div(curl($\\vec G$))} = \\text{div($\\vec H$)} = 0$ Vector Form of Green\u0026rsquo;s # $$\\oint_C \\vec F \\cdot d \\vec r = \\iint_D \\text{(curl ($\\vec F$))} \\cdot \\hat k\\ dA = \\bigg( \\frac{\\delta Q}{\\delta x} - \\frac{\\delta P}{\\delta y} \\bigg) \\hat k$$\nThis equation expresses the line integral of the tangential component of .$\\vec F$ along .$C$ as the double integral of the vertical component of .$\\text{curl($\\vec F$)}$ over the region .$D$ enclosed by .$C$. We can write this using the normal component of .$\\vec F$ too: With .$\\vec r(t) = \\langle x(t), y(t) \\rangle; t \\in [a,b]$ Recall the unit tangent vector: .$\\vec T(t) = \\frac{1}{\\vert \\vec r \u0026rsquo; (t) \\vert} \\langle x\u0026rsquo;(t), y\u0026rsquo;(t) \\rangle$ The outward unit normal vector to .$C$ is given by .$\\vec n (t) = \\frac{1}{\\vert \\vec r \u0026rsquo; (t) \\vert} \\langle y\u0026rsquo;(t), -x\u0026rsquo;(t) \\rangle$ We can then evaluate $$\\oint_C \\vec F \\cdot \\vec n\\ ds = \\int_a^b (\\vec F \\cdot \\vec n)(t) \\vert \\vec r\u0026rsquo;(t) \\vert\\ dt = \\iint_D \\bigg( \\frac{\\delta P}{\\delta x} - \\frac{\\delta Q}{\\delta y} \\bigg)\\ dA = \\iint_D \\text{div $\\vec F (x,y)$}\\ dA$$ This says that the line integral of the normal component of .$\\vec F$ along .$C$ is equal to the double integral of the divergence of .$\\vec F$ over the region .$D$ enclosed by .$C$. 16.6 Parametric Surfaces and Their Area # Parametric Surfaces # Just like how we can describe curves with single parameter (variable) function .$\\vec r(t)$, we can describe surfaces with a vector function .$\\vec r(u,v) = \\langle x(u,v), y(u,v), z(u,v) \\rangle$ .$\\vec r(u,v)$ is called a vector-valued function defined on a region .$D$ in the .$uv$-plane .$x,y,z$ are the component functions of .$\\vec r$, each of which have domain .$D$ In general, a surface given as the graph of a function of .$x$ and .$y$, that is, with an equation of the form .$z = f(x,y)$, can always be regarded as a parametric surface by taking .$x$ and .$y$ as parameters and writing the parametric equations as .$x = x; y = y; z = f(x,y)$ E.x. the plane with point .$(x_0, y_0, z_0)$ and vectors .$\\langle a,b,c \\rangle$ .$\\langle \\alpha,\\beta,\\gamma \\rangle$ is .$\\vec r(u,v) = \\langle x_0, y_0, z_0 \\rangle + u\\langle a,b,c \\rangle + v\\langle \\alpha,\\beta,\\gamma \\rangle$ or .$0 = (\\langle x - x_0, y - y_0, z - z_0 \\rangle) \\cdot (\\langle a,b,c \\rangle \\times \\langle \\alpha,\\beta,\\gamma \\rangle)$ Surfaces of Revolution # We can write surfaces of revolution as parametric equations too Consider surface .$S$ obtained by rotating the curve .$y = f(x)$ about the .$x$-axis (with .$f(x) \\geq 0$) Therefore, we get the following parameterization: $$x = x$$ $$y = f(x)\\cos\\theta$$ $$z = f(x)\\sin\\theta$$ Tangent Planes # Employing the same method as before, we can find the tangent plane to a param surface .$S$ by finding the initial point and the normal vector Given some parameterization .$\\vec r(u,v) = \\langle x(u,v), y(u,v), x(u,v) \\rangle$ and initial point .$P_0 = (x_0, y_0)$\u0026hellip; Find point .$P_0$ by setting .$x(u,v) = x_0, y(u,v) = y_0, \\dots$ and solving for .$u_0,v_0$ Find normal vector .$\\vec n$ by deriving to get, then cross, the parameterization: .$\\vec n = \\vec r_u \\times \\vec r_v$ If the normal vector isn\u0026rsquo;t zero, the tangent plane is at .$\\vec n (u_0, v_0)$ If it is, then the surface .$S$ isn\u0026rsquo;t smooth (it is at a \u0026ldquo;corner\u0026rdquo;) The tangent plane can then be expressed as $$(\\vec r_u \\times \\vec r_v)_{(u_0, v_0)} \\cdot (\\langle x,y,z \\rangle - \\langle x_0, y_0, z_0 \\rangle)$$ Surface Area # The image of the subrectangle .$R_{ij}$ is the patch .$S_{ij}$.\nRecall that the magnitude of the cross product is the area of a parallelogram formed by two vectors We can think of this as the jacobian of the transformation If a smooth parametric surface .$S$ is given by the equation $$\\vec r(u,v) = \\langle x(u,v), y(u,v), z(u,v) \\rangle; (u,v) \\in D$$ and .$S$ is covered just once as .$(u,v)$ ranges throughout the parameter domain .$D$, then the surface area of .$S$ is $$A(S) = \\iint_D \\vert \\vec r_u \\times \\vec r_v \\vert\\ dA$$ where $$\\vec r_u = \\langle \\frac{\\delta x}{\\delta u}, \\frac{\\delta y}{\\delta u}, \\frac{\\delta z}{\\delta u} \\rangle; \\vec r_v = \\langle \\frac{\\delta x}{\\delta v}, \\frac{\\delta y}{\\delta v}, \\frac{\\delta z}{\\delta v} \\rangle$$ Surface Area of the Graph of a Function # For the special case of a surface .$S$ with equation .$z = f(x,y)$, where .$(x,y)$ lies in .$D$ and .$f$ has continuous partial derivatives, we take .$x$ and .$y$ as parameters. That is, the parametric equations are .$x = x; y = y; z = f(x,y)$ Therefore .$\\vec r_x = \\langle 1, 0, \\frac{\\delta f}{\\delta x} \\rangle; \\vec r_y = \\langle 0, 1, \\frac{\\delta f}{\\delta y};$ and .$\\vec n = \\sqrt{1 + \\frac{\\delta f}{\\delta x}^2 + \\frac{\\delta f}{\\delta y}^2}$ Thus, the surface area becomes $$A(S) = \\iint_D \\sqrt{1 + \\frac{\\delta f}{\\delta x}^2 + \\frac{\\delta f}{\\delta y}^2}\\ dA$$ Notice the similarity between the surface area formula above and the arc length formula 16.7 Surface Integral # Surface Integral # The relationship between surface integrals and surface area is much the same as the relationship between line integrals and arc length. The arc length is a line integral where the density (or weight) function .$f(x,y,z) = 1$ That is, .$\\int_C f(x,y,z)\\ ds = \\int_a^b f(\\vec r(t)) \\vert \\vec r\u0026rsquo;(t) \\vert$ Similarly, the surface area is found taking the surface integral with density function .$f(x,y,z) = 1$ That is, .$\\iint_S 1\\ dS = \\iint_D \\vert \\vec r_u \\times \\vec r_v \\vert\\ dA = A(S)$ If we define .$\\vec r(u,v) = \\langle x(u,v), y(u,v), z(u,v) \\rangle; (u,v) \\in D$ $$\\iint_S f(x,y,z)\\ dS = \\iint_D f(\\vec r(u,v)) \\vert \\vec r_u \\times \\vec r_v \\vert\\ dA$$ Graphs of Functions # Any surface .$S$ with equation .$z = g(x,y)$ can be regarded as a parametric surface with parametric equations $$x = x$$ $$y = y$$ $$z = g(x,y)$$ Thus, $$\\vec r_x = \\bigg\\langle 1, 0, \\frac{\\delta g}{\\delta x}\\bigg\\rangle$$ $$\\vec r_y = \\bigg\\langle 0, 1, \\frac{\\delta g}{\\delta y}\\bigg\\rangle$$ $$\\vec r_x \\times \\vec r_y = \\bigg\\langle -\\frac{\\delta g}{\\delta x}, -\\frac{\\delta g}{\\delta y}, 1\\bigg\\rangle$$ $$\\vec n = \\sqrt{\\bigg(\\frac{\\delta z}{\\delta x}\\bigg)^2 + \\bigg(\\frac{\\delta z}{\\delta y}\\bigg)^2 + 1}$$ Therefore, $$\\iint_S f(x,y,z)\\ dS = \\iint_D f(x,y,g(x,y)) \\sqrt{\\bigg(\\frac{\\delta z}{\\delta x}\\bigg)^2 + \\bigg(\\frac{\\delta z}{\\delta y}\\bigg)^2 + 1}\\ dA$$ Similar formulas apply when it is more convenient to project .$S$ onto the .$yz$-plane or .$xz$-plane. Oriented Surfaces # With the exception of the Möbius strip, most surfaces are two-sided, meaning they\u0026rsquo;re Orientable surfaces We start with a surface .$S$ that has a tangent plane at every point .$(x,y,z)$ on .$S$ (except at any boundary point). There are two unit normal vectors .$\\hat n_1$ and .$\\hat n_2 = -\\vec n_1$ at .$(x,y,z)$ If it is possible to choose a unit normal vector .$\\hat n$ at every such point .$(x,y,z)$ so that .$\\hat n$ varies continuously over .$S$, then .$S$ is called an oriented surface The choice of .$\\hat n$ provides .$S$ with an orientation. There are two possible orientations for any orientable surface For a closed surface, the positive orientation has the normal vectors pointing outward and vis-versa If .$S$ is a smooth orientable surface given in parametric form by a vector function .$\\vec r (u,v)$ then it is automatically supplied with the orientation of the unit normal vector $$\\hat n = \\frac{\\vec r_u \\times \\vec r_v}{\\vert \\vec r_u \\times \\vec r_v \\vert}$$ E.x., going back to a surface .$z = g(x,y)$ given as the graph of .$g$, the normal unit vector is $$\\hat n = \\frac{\\big\\langle -\\frac{\\delta g}{\\delta x}, -\\frac{\\delta g}{\\delta y}, 1\\big\\rangle}{\\sqrt{\\big(\\frac{\\delta z}{\\delta x}\\big)^2 + \\big(\\frac{\\delta z}{\\delta y}\\big)^2 + 1}}$$ Since the .$\\hat k$-component is positive, this gives the upward orientation of the surface. Surface Integrals of Vector Fields (Flux) # If .$\\vec F$ is a continuous vector field defined on an oriented surface .$S$ with unit normal vector .$\\hat n$, then the surface integral of .$\\vec F$ over .$S$ is $$\\iint_S \\vec F \\cdot d\\vec S = \\iint_S \\vec F \\cdot \\hat n \\ dS = \\iint_D \\vec F \\cdot (\\vec r_u \\times \\vec r_v)\\ dA$$ This integral is also called the flux of .$\\vec F$ across .$S$. In words, the surface integral of a vector field .$\\vec F$ over .$S$ is equal to the surface integral of its normal component over .$S$ (as previously defined). We can apply this to fluids: Imagine a fluid with density .$\\rho (x,y,z)$ and velocity field .$\\vec v (x,y,z)$ flowing through .$S$. Think of .$S$ as an imaginary surface that doesn\u0026rsquo;t impede the fluid flow, like a fishing net across a stream. Then the rate of flow (mass per unit time) per unit area is .$\\vec F = \\vec v \\rho$ The flux can be interpreted physically as the rate of flow through .$S$. 16.8 Stokes\u0026rsquo; Theorem # Just as Green\u0026rsquo;s Theorem relates a double integral over a plane region .$D$ to a line integral around its plane boundary curve, Stokes\u0026rsquo; Theorem relates a surface integral over surface .$S$ to a line integral around the boundary curve of .$S$ (which is a space curve) Stokes\u0026rsquo; Theorem: Let .$S$ be an oriented piecewise-smooth surface that is bounded by a simple, closed, piecewise-smooth boundary curve .$C$ with positive orientation. Let .$\\vec F$ be a vector field whose components have continuous partial derivatives on an open region in .$\\mathbb{R}^3$ that contains .$S$. Then $$\\int_C \\vec F \\cdot d \\vec r = \\iint_S \\text{curl } \\vec F \\cdot d\\vec S$$ In words, Stokes\u0026rsquo; Theorem says that the line integral around the boundary curve of .$S$ (some curve.$C$) of the tangential component of .$\\vec F$ is equal to the surface integral over .$S$ of the normal component of the curl of .$F$ This is since $$\\int_C \\vec F \\cdot d \\vec r = \\int_C \\vec F \\cdot \\vec T\\ ds \\ \\ \\ \\text{ and }\\ \\ \\iint_S \\text{curl } \\vec F \\cdot d\\vec S = \\iint_S \\text{curl } \\vec F \\cdot \\hat n\\ dS$$ 16.9 Divergence Theorem # Divergence Theorem: Let .$E$ be a simple solid region and .$S$ be the boundary surface of .$E$, given with positive (outward) orientation. Let .$\\vec F$ be a vector field whose component functions have continuous partial derivatives on an open region that contains .$E$. Then $$\\iint_S \\vec F \\cdot d\\vec S = \\iiint_E \\text{div } \\vec F\\ dV$$ In words, we can say that the Divergence Theorem says that (under the given conditions) the flux of .$\\vec F$ across the boundary surface of .$E$ is equal to the triple integral of the divergence of .$\\vec F$ over .$E$. "},{"id":45,"href":"/cogsci-c100/ai-ml/","title":"17: Artificial Intelligence \u0026 Machine Learning","section":"CogSci C100","content":" Basic definitions # Artificial Intelligence (AI): tries to design computer models that accomplish the same cognitive tasks that humans do Machine Learning: a subset of AI that allows computers to \u0026ldquo;learn\u0026rdquo; (i.e., progressively improve performance on a specific task) by creating new algorithms to produce a desired output based on structured (or unstructured) data that is provided Deep Learning: a subset of Machine Learning involving numerous layers of algorithms Computer does not need to be provided with structured data Neural Networks: Networks of algorithms that are similar to the neural networks present in the human brain Artificial Intelligence Formulations # There are a number of different artificial intelligence formulations\nStrong AI: General purpose AI Machines that possess artificial general intelligence (AGI) Would be just as smart as humans across the board with the ability to understand and learn any task that a human can Applied AI: AGI isn\u0026rsquo;t going to be created any time soon, but machine learning has made it possible for machines to learn how to master complex tasks (expert systems), including Playing the ancient Chinese board game Go Identifying human faces Translating text into practically every language Driving cars Computer replication: Understand how the mind works and replicate its functions in machine or organic form Machine Learning # Problem: How can a computer learn to distinguish between pictures of dogs and cats? Approach: You label pictures of dogs and cats with specific defining characteristics (e.g., length of ear, color of nose), then feed this structured data through the computer Deep Learning # You feed the computer pictures labeled as dogs or cats without additional structured data Data is sent through different layers of the artificial neural network corresponding to different layers of abstraction, from low level (e.g., does this part of picture contain a brown spot?) to more complex, e.g., (is there a nose in this part of the picture?) Supervised, Unsupervised, and Deep Reinforcement Learning # Supervised learning: network receives explicit feedback on how successful it is Unsupervised learning: network does not receive explicit feedback; instead it learns to detect patterns in data Reinforcement learning: distinct from both of the above in that\u0026hellip; It does depend upon a feedback signal However, the feedback signal does not tell the network exactly what it has done wrong; instead, the network is driven by a reward signal The job of the network is to maximize the reward, but it is not told how to do that It has to work out for itself which outputs are most profitable Ex: Atari breakout AlphaGo # \\(\\) In 2016 and 2017, AlphaGo program created by Google’s Deep Mind research group beat the world’s leading human experts at the game of Go\nThese victories were widely recognized as historic achievements for AI Go is an abstract strategy board game for two players, in which the aim is to surround more territory than the opponent It is one of world’s most complex games Chess has $10^{123}$ possible moves; Go has $10^{360}$ AlphaGo used a mixture of supervised learning and reinforcement learning Was initially trained on a database of 30 million moves from an online server using supervised learning \u0026ndash; received explicit feedback on how successful it was Once AlphaGo had achieved a relatively high level of playing strength, training shifted to reinforcement learning In reinforcement learning, network will not increase its reward simply by repeating what has worked in the past It needs to engage in trial and error to discover new reward-generating strategies Reinforcement learning was achieved by getting the network to play games of Go against former versions of itself New version, AlphaGo Zero, incorporated zero supervised learning After three days (and 4.9 million games played against itself), it was able to beat the version of AlphaGo that had defeated the leading human expert in 2016 \u0026ndash; 100 games to 0! Within 40 days, it was able to beat all existing versions of AlphaGo Applications of AI # Healthcare # Diagnosis of disorders, e.g., IBM Watson Computers found to be as good or better than doctors at detecting tiny lung cancers on CT scans Potential hazard: A radiologist who misreads a scan may harm one patient, but a flawed A.I. system in widespread use could injure many Determining optimal treatment, including Type and dosage of drugs Best diet for individual, e.g., to avoid glucose spikes after eating This may vary, depending on patient’s unique gut microbiome Precision surgery without human artifacts like handshaking Study comparing computer-controlled robots with human surgeons in performing intestinal surgery on a pig found that the robot sutures were much better—more precise and uniform with fewer chances for breakage, leakage, and infection (Shademan, Decker, Opfermann et al., 2016) Brain Computer Interface # Brain Computer Interface (BCI) or Brain Machine Interface (BMI): \u0026ldquo;neural prosthetics\u0026rdquo;\nComputer chip is implanted in motor cortex and communicates directly with external device Allows animal (or person) to directly control a robotic arm with their thoughts A cap with electrodes can now be used instead of implants, but an extensive calibration process is required Direct brain-to-brain communication in humans is also now possible Two research participants are are positioned in two different buildings on campus The sender, left, thinks about firing a cannon at various points throughout a computer game That signal is sent over the Web directly to the brain of the receiver, right, whose hand hits a touchpad to fire the cannon (Rao, Stucco, Ryan et al., 2014) Psychotherapy # Diagnosis/identification of psychological disorders\nAI system that analyzed Facebook posts of consenting patients in an emergency department was able to generate predictions of depression risk that were as accurate as standard depression screening tests (Reece \u0026amp; Danforth, 2017) Indicators included references to sadness, loneliness, hostility, rumination, and increased self-reference, e.g., words like \u0026ldquo;alone,\u0026rdquo; \u0026ldquo;ugh,\u0026rdquo; \u0026ldquo;tears,\u0026rdquo; and higher frequency of use of \u0026ldquo;I\u0026rdquo; and \u0026ldquo;me\u0026rdquo; Length and timing of posts were also considered (Eichstaedt, Smith, Merchant et al., 2018) AI has also been used to analyze Instagram photos to successfully screen for depression Photos posted by depressed individuals tended to be bluer, darker, and grayer The more comments Instagram posts received, the more likely they were posted by depressed participants, but the opposite was true for likes received Depressed participants were more likely to post photos with faces, but had a lower average face count per photo than healthy participants The screening models created from the data were able to outperform general practitioners in correctly diagnosing depression without the assistance of assessment instruments\nA.I.-driven voice analysis\nResearchers are currently working on developing voice analysis programs that can help identify psychological disorders In depression, speech is generally more monotone, with reduced pitch range and lower volume; there may be more pauses In anxiety, speech tends to be faster, and there may be evidence of difficulty breathing Programs may also be used to predict other mental illnesses like schizophrenia and PTSD Use of deep-learning algorithms can uncover patterns that might not be evident even to trained experts \u0026ldquo;The technology that we’re using now can extract features that that even the human ear can’t pick up on.\u0026rdquo; Potential problems: It can be difficult to know why your vocal levels fluctuate, e.g., you may just be trying to speak quietly Issue of bias: need to ensure that programs work for all patients, regardless of age, gender, ethnicity, etc. Deep learning algorithms work in ways that even the developers themselves can’t fully explain, that is, knowing which features are being used to make the predictions Treatment of psychological disorders # Apps that can administer Cognitive Behavioral Therapy for disorders like depression or social anxiety, e.g., Woebot Virtual therapists, e.g., Ellie These programs may analyze tone of voice, breathing pattern, smartphone keystrokes and communication, and/or physical movements in making diagnoses and generating responses Pros: Easy accessibility and affordability Research has indicated that people would rather share their innermost secrets with an avatar than a human being Cons: Can’t really replace human empathy Adherence to treatment may be poor Transportation # Self-driving cars\n2018 was the target date proposed in 2015 both by Elon Musk of Tesla and by Google for introduction of self-driving cars But a series of widely publicized crashes, some fatal, have delayed release Are self-driving cars really a viable option?\nThe great successes of deep learning have all been in relatively circumscribed domains, including chess and Go, and even things like image recognition, which primarily involves identifying patterns in a data set, then projecting those patterns onto new exemplars Some believe that self-driving cars (and other autonomous vehicles, like drones) need more than sensitivity to patterns and the ability to learn from experience They need to be able to deal with the unexpected \u0026ndash; completely unpredictable behavior from other drivers, pedestrians, cyclist, and even wild animals Also, human drivers are constantly exploiting their knowledge of how physical objects move and behave (folk physics), as well as their knowledge of other drivers and road-users (mindreading) Perhaps a key challenge for designers of self-driving cars is how to equip their vehicles with this kind of general knowledge Pros: Less human error = more lives saved Accessible to those who cannot drive Can engage in other activities during commute Cons: Criminal hacking or system glitches Loss of jobs High initial cost Fewer people using public transportation Moral issues # Cars will need to make \u0026ldquo;moral\u0026rdquo; decisions in unavoidable accidents Which person to sacrifice, the pedestrian or the driver? Robots # Social robots Home robots: cooking, cleaning, fetching… Moley learns to cook by using data from the motion-capturing gloves and wristbands of a master chef Other applications # Language processing: Use of natural language processing of speech to synthesize notes in professional settings Advertising: Tracking customer behavior to target them with personalized promotions Customer service: Help lines; providing information to consumers Finance and economics: Record keeping Fraud detection Optimizing profits in online trading Predicting market supply and demand Risks and Dangers of AI # Deliberate programming of AI to be hostile, e.g., by terrorist group Sci-fi scenario: If human behavior contradicts one of AI’s preprogrammed goals, AI could turn malicious In 2017, two Facebook computers started communicating with each other in a language they had developed on their own Simulation of government leader’s image and voice issuing unauthorized orders, e.g., military action Generation of fake emails, phone calls, video chats Loss of privacy Loss of jobs Some AI experts predict that AI will replace or eliminate 40% of jobs within 15 years Greatest impact will be on jobs involving tasks that are repetitive and can be automated Other AI-related risk: Development of computer technology has meant an increase in the amount of time we spend looking at computer screens, and that may be harmful Recent study published in JAMA Pediatrics found that after controlling for age, gender and income, 3-5 year old children with higher use of screen-based media Had lower measures of structural integrity and myelination Scored lower on cognitive tests (Hutton, Dudley, Horowitz-Kraus et al., 2020) Similarly, in older adults, increased television viewing was found to be correlated with cognitive impairment and poor verbal memory (Fancourt and Steptoe, 2019) In fruit flies, daily blue-light exposure (such as is used in computer screens) causes brain neurodegeneration, as well as shortening of lifespan (Nash, Chow, Law et al., 2019) American Academy of Pediatrics recommendations:\nChildren under 18 months should avert their eyes from TV and screen media at all times For children 2 to 5, screen time should be limited to no more than 1 hour per day Issue of biases # Back in 2015, software engineer Jacky Alciné pointed out that the image recognition algorithms in Google Photos were classifying his African American friends as \u0026ldquo;gorillas\u0026rdquo; Google said it was \u0026ldquo;appalled\u0026rdquo; at the mistake, apologized to Alciné, and promised to fix the problem However, three years later, Google still had not really fixed anything – the company simply blocked its image recognition algorithms from identifying gorillas altogether Baboons, gibbons, and marmosets were all correctly identified, but gorillas and chimpanzees were not Try typing things like \u0026ldquo;why Black/Latinx/Asian women/men/boys/girls \u0026hellip;\u0026rdquo; on Yahoo or Bing You’ll get really biased, racist and discriminatory auto-suggestions These are now blocked on Google, but not on other search engines AI and jobs # On the other hand, AI has also created new jobs, e.g., AI designer, software engineer, cybersecurity developer, machine relations manager Unfortunately though, people whose jobs are taken away may not necessarily be those who get new jobs created by AI development Jobs that Are Hard to Replace # Emotionally demanding jobs Therapist Depends on individual (some veterans like talking to AI) Taking care of babies/children Human Resources Politician Creative jobs Writer Software/graphic designer Things We Can Do that AI Cannot Do Well # Feel or show empathy Have insights As Anthony Goldbloom puts it, machines cannot \u0026ldquo;connect seemingly disparate threads to solve problems they have never seen before\u0026rdquo; Machines can’t tackle novel situations Need to learn from large volumes of past data Percy Spencer was working on radar during World War II when he noticed a magnetron (used to generate radio signals) melting his chocolate bar This led to the discovery of the Microwave oven! Make plans for the distant future Humans can plan their lives years in advance Robots tend to focus only on completing the immediate task at hand Be conscious (?) Whether machines can have consciousness depends on the definition of consciousness No machine today meets all the criteria that we may give to consciousness and that humans have "},{"id":46,"href":"/physics-7b/17/","title":"17: Temperature, Thermal Expansion, \u0026 Ideal Gas Law","section":"Physics 7B","content":" 17.1: Atomic Theory # Atoms are the smallest unit of matter Atomic unit: .$\\text{u} = 1.66\\cdot 10^{-27}$ kg E.x. Hydrogen weighs .$1.0078 \\text{u}$ Molecular mass of a compound is the sum of the particles (atoms) in the compound Terms # Element: Substance that cannot be broken down into smaller substances (gold) Molecule: Group of atoms held together by covalent bonds Compound: Substance made from atoms combined in specific ratios Brownian Motion # Random movement seen in pollen/dust, as well as atoms Using Brownian motion, Einstein found the size of an atom to be .$10^{-10}$ meters Forces # Atoms and molecules exert an (electric) attractive force on one another by default If an atom/molecule gets too close to another, they exert a repelling force on one another Matter states: Solid: Atoms held in matrix formation by strong attractive forces. Atoms vibrate around their mean position Liquid: Force between atoms is weaker so atoms move more rapidly within Gas: Atom attractive forces are so weak compared to their kinetic energy that they move randomly If two atoms collide, the attractive force is so weak that they may just bounce off one another 17.2: Temperature and Thermometers # Matter property changes under different temperatures Sidewalks expand under the sun Electric resistance increases with heat Lightbulb filament glows Thermometers Types # Originally used alcohol which expands linearly with heat (water doesn\u0026rsquo;t) Bimetalic strips bend at slightly different rates under heat Electronic thermometers measure resistance change and often have digital screens Scales # Fahrenheit: Water freezes at 32 and boils at 212 deg Celsius: Water freezes at 0 and boils at 100 deg Kelvin: Celsius + 273.15K. Written without degree sign. Absolute = 0K Conversions: $$T(^\\circ C) = \\frac{5}{9}(T(^\\circ F)-32)$$ $$T(^\\circ F) = \\frac{9}{5}(T(^\\circ C)) + 32$$ Different materials expand at different rates ro we use constant-volume thermos because it\u0026rsquo;s pressure linearly relates to the temperature 17.3 0th Law of Thermodynamics # If objects .$A$ and .$B$ are at equilibrium with object .$C$ , then .$A$ and .$B$ are also at equilibrium with one another Systems naturally reach equilibrium over time Thermal Expansion # Most materials expand when heated Expansion amount depends on the material Equations (assuming a constant volume .$V$ ) Linear Expansion: .$\\alpha$ is the coefficient of linear expansion and depends on the material with units .$(^\\circ C)^{-1}$ $$\\Delta l \\approxeq \\alpha l_0 \\cdot \\Delta T$$ $$l_i + \\Delta l = l_f = l_i ( 1 + \\alpha\\Delta T)$$ $$\\frac{dl}{dT} = \\alpha(T)\\cdot l$$ If .$\\Delta T$ is too large such that the temperature dependence of .$\\alpha$ is too large, we can do the following: $$\\int_{l_i}^{l_f} \\frac{1}{l}dl = \\int_{T_i}^{T_f} \\alpha(T) dT$$ Volume Expansion: $$\\beta = \\frac{1}{V} \\frac{dV}{dT}$$ $$V_f \\approxeq V_0 ( 1 + \\beta\\Delta T)$$ .$\\beta \\approx 3\\cdot\\alpha$ = coefficient of volume expansion. Coefficient of expansion varies at extremely high heats so it only works with small .$\\Delta T$ \u0026rsquo;s Materials must be isotropic (have same expansion properties in all directions) for us to say .$\\alpha \\approx 3\\cdot\\beta$ (Linear) expansion doesn\u0026rsquo;t exist for gas or liquids because they have no fixed space like solids. Weird water property .$0 - 4 ^\\circ C$ : Water increases in density .$\\rho^+\\Longrightarrow$ decreases in volume .$V^-$ .$4^\\circ C +$ : Water acts \u0026ldquo;normally\u0026rdquo;: increase in volume .$V$ proportional to temperature .$T$ This explains why pipes burst when frozen and why ice cubes float 17.5 Thermal Stresses # When the ends a solid (rod) are fixed (such as in beams), temperature changes induce thermal stress due to the clamp limiting expansion/contraction Process Steps: Beam tries to expand/contract by .$\\Delta l$ Mount reacts with an opposite reactive force, keeping it at it\u0026rsquo;s original length: $$\\Delta l = \\frac{1}{E} \\cdot \\frac{F}{A} \\cdot l_0$$ where .$E$ is Young\u0026rsquo;s modulus for the material. We can also re-write for stress: $$\\frac{F}{A} = \\Delta l \\cdot E \\cdot \\frac{1}{l_0} = (\\alpha l_0 \\Delta T) E \\cdot \\frac{1}{l_0} = \\alpha E \\Delta T$$ 17.6 Gas Laws and Absolute Temperature # Equation at State describes how pressure varies with Temperature, Number of Particles (Molecules), and Volume State is the physical condition of a system Equilibrium State: .$T, N, \\\u0026amp;\\ V = \\text{Constant}$ Laws # Assume that gasses aren\u0026rsquo;t too dense (so .$P \\sim$ atmospheric pressure) and that they aren\u0026rsquo;t close to liquefaction (boiling) point either (for oxygen, this is .$~183^\\circ \\text{C}$.) Boyle\u0026rsquo;s Law .$V \\propto P^{-1}$ [Constant Temperature] .$P$ is absolute, not gauge, pressure Alternatively, .$PV =$ const or .$P_1V_1 = P_2V_2$ Charles\u0026rsquo;s Law .$V \\propto T$ [Constant Pressure] Alternatively, .$\\frac{V_1}{T_1} = \\frac{V_2}{T_2}$ Gay Lussac\u0026rsquo;s Law .$P \\propto T$ [Constant Volume] Alternatively, .$\\frac{P_1}{T_1} = \\frac{P_2}{T_2}$ 17.7 Ideal Gas Law # $$PV = nRT = n k_B N_a T = N k_B T$$ .$P$ is the pressure of the gas [Pascals] .$V$ is the volume of the gas [Cubic Meters] .$T$ is the absolute temperature of the gas [Kelvins] .$N$ is the number of molecules of gas .$n$ is the amount of substance of gas (number of moles) [Moles] .$R$ is the ideal, or universal, gas constant, equal to .$k_B \\cdot N_a = 8.314 \\frac{J}{K\\cdot \\text{mol}}$ Using mass of a gas, different gasses have different proportionality constants So we used number of moles, in which case .$R$ becomes the constant for all gasses .$k_B $ is the Boltzmann constant Relates the average relative kinetic energy of particles in a gas with the thermodynamic temperature of the gas .$N_a$ is the Avogadro constant The number of particles that are contained in one mole of gas .$n = N/N_A$ This equation is Ideal in that the equation only works for gasses around atmospheric pressure and not excessive temperatures Moles # Mole is the SI unit for amount of substance 1 mole = Number of particles in .$\\text{12g}$ of Carbon 1 mole = Number of grams of a substance numerically equal to the molar mass $$n \\text{(moles)} = \\frac{\\text{mass (grams)}}{\\text{molecular mass (g/mol)}}$$ 17.8 Problem Solving with .$PV = nRT$ # STP: Standard Temperature and Pressure # .$T = 273 \\text{K}$ .$P = 1.00 \\text{atm} = 1.013\\cdot10^5 \\text{N/m}^2 = 101.3 \\text{kPA}$ .$1 \\text{mol of ideal gas} = 22.4\\text{L}$ in volume If P is in liters and V is in atm, then we can use .$R = 0.0821 \\frac{\\text{L} \\cdot \\text{atm}}{\\text{mol} \\cdot \\text{K}}$ Since .$n$ and .$R$ are constants, we can say: $$\\frac{P_1 V_1}{T_1} = \\frac{P_2 V_2}{T_2}$$ 17.9 Ideal Gas with Avogadro\u0026rsquo;s Number # Avogadro\u0026rsquo;s hypothesis: Equal volume of gas with the same .$P$ and .$T$ have an equal .$n$ umber of particles (molecules) .$N_a$ is avogadro\u0026rsquo;s number: the number of particles that are contained in one mole of gas (or one gram of hydrogen). .$N_a = 6.022 \\cdot 10^{23}\\ \\text{particles/mole}$ Therefore, if .$N$ is the number of molecules of a gas sample and .$n$ is the number of moles, then $$N = n\\cdot N_A \\Longrightarrow n = \\frac{N}{N_A} \\Longrightarrow PV = \\frac{N}{N_A}RT = Nk_B T$$ where .$k_B $ is Boltzmann\u0026rsquo;s constant .$\\frac{R}{N_A} = 1.38 \\cdot 10^{-23} \\frac{\\text{J}}{\\text{K}}$ 17.10 Ideal Gas Temperature # Triple point: A precise temperature and pressure where the three phases (gas, liquid, and solid) of a substance can coexist in thermodynamic equilibrium. .$P_3 = 4.88\\ \\text{torr};\\ T_3 = 0.01^\\circ C$ for water Ideal Gas, constant volume: $$T = (273.16 K)\\bigg(\\frac{P}{P_3}\\bigg)$$ Constant volume: $$T = (273.16 K)\\lim_{P_3 \\to 0}\\bigg(\\frac{P}{P_3}\\bigg)$$ A typical phase diagram. The solid green line applies to most substances; the dashed green line gives the anomalous behavior of water. For more see 18.4\n"},{"id":47,"href":"/physics-7b/18/","title":"18: Kinetic Theory of Gases","section":"Physics 7B","content":" 18.1 Ideal Gas Laws and Molecular Interpolation # Ideal Gas Law Assumptions # There are a large number of molecules, .$N$, each of mass .$m$ that move in random directions at random speeds Molecules are, on average, sufficiently far away from one another (separation .$\\gg$ diameter) Molecules obey classical mechanics so .$KE \\gg PE$ when colliding Collisions are perfectly elastic Micro and Macroscopic views related through Energy # In a system with .$N$ molecules each of mass .$m$ and average speed .$\\bar{v}^2$ (also denoted as .$\\langle v^2 \\rangle$), we can combine the ideal gas law with the .$\\overline{KE}$ equation: $$\\overline{K} = \\frac{1}{2} m \\bar{v}^2 = \\frac{3}{2}k_B T$$ This shows .$\\overline{KE} \\propto T$ which makes sense intuitively; cold = slow particle motion E.x: A container is filled with a light and heavy molecule. Which has a greater speed? The lighter molecules do because they are less massive. And since the system\u0026rsquo;s internal energy .$E_{\\text{int}} = N \\cdot \\overline{K}$ then we can write an important .$PV$ relation + find the gas.$E_{\\text{int}}$: $$T = \\frac{\\frac{1}{2} m\\bar{v}^2}{\\frac{3}{2}k_B} = \\frac{2}{3} \\cdot \\frac{E_{\\text{int}}}{N k_B} \\Longrightarrow PV = \\frac{2}{3}E_{\\text{int}}$$ $$E_{\\text{int}} = N\\cdot \\frac{1}{2}m\\bar{v}^2 \\Longrightarrow PV = \\frac{Nm}{3}\\bar{v}^2 $$ Which shows us that .$(P,V)$ is a representation of (kinetic) energy! We can think of any point on a .$PV$ diagram in terms of energy Absolute 0 # Before, we said .$T = 0K$ exists when .$P = 0$\nNow we can also see that .$KE = 0$ when .$T = 0$ as well. This would mean that at absolute 0, there is no particle movement\nWe can then write an equation for the root-mean-square (or RMS): $$\\overline{K} = \\frac{3}{2}k_B T \\Longrightarrow \\frac{1}{2}m\\bar{v}^2 = \\frac{3}{2}k_B T;\\ v_{\\text{rms}} = \\sqrt{\\bar{v}^2} = \\sqrt{3k_B\\frac{T}{m}} = \\sqrt{3 R\\frac{T}{M}} $$ $$\\bar v ^2 = \\frac{1}{N}\\sum_i^N n_i v_i^2 \\Longrightarrow v_{\\text{rms}} = \\frac{1}{\\sqrt N} \\sqrt{\\int_0^\\infty n(v)\\cdot v^2 dv} $$\nThis is the typical velocity of particles that make up the gas/liquid .$M$ is the molar mass of the gas [kilograms per mole] .$v_{\\text{rms}}$ is also called the thermal velocity, .$v_{\\text{th}}$ Fun fact: Less than 1% of particles of particles exceed .$v_{\\text{rms}}$ Example problems: If a sample is quasistatically shrunk to half it\u0026rsquo;s original volume with no change in pressure, the new root-mean-square speed is .$1/\\sqrt{2}$ times the original rms speed If we double the root-mean-square speed (thermal speed) of the molecules of a gas, then its temperature must increase by a factor of 4 18.2 Distribution of Molecular Speeds # $$f(v) = 4\\pi N \\bigg( \\frac{m}{2 \\pi k_B T}\\bigg)^{3/2} v^2 e^{ -\\frac{1}{2} \\cdot \\frac{mv^2}{k_B T}}$$ Recognize that .$f(v) \\propto T^{-3/2}, v^2$ and exponentially .$\\propto KE/T$ If .$T$ increases, so does .$KE$ and .$v$ thus variance .$\\sigma^2$ increases and the distribution becomes \u0026ldquo;stretched\u0026rdquo; (lower max, thicker tail) Spread of important values (.$v_p, v_{\\text{avg}}, \\text{etc.}$) are spread out further from one another Area stays constant (always equal to .$N$) .$f(v)\\ dv$ represents the number of molecules with .$v \\in [v, v+dv]$ That is, .$\\int_0^\\infty f(v) dv = N$ .$\\sigma^2$ is the variance, or standard deviation squared, which can be found from the equation .$\\langle v^2 \\rangle - \\langle v \\rangle ^2$ Chemical Reactions Some reactions only occur at a certain energy levels (activation energy) Warmer conditions lead to faster moving particles which have energy That\u0026rsquo;s why reaction speed .$\\propto$ temperature Important Values # $$v_p = 1.41 \\sqrt{k_B\\frac{T}{m}}$$ $$v_{\\text{avg}} = 1.60 \\sqrt{k_B\\frac{T}{m}}$$ $$v_{\\text{rms}} = \\sqrt{3 k_B \\frac{T}{m}}$$ Notice how .$v \\propto m^{-1/2}$, which explains why it\u0026rsquo;s easier for lighter particles to escape earth\u0026rsquo;s atmosphere! 18.3 Real Gases and Phase Changes # (a) Each curve represents the relationship between .$P$ and .$V$ at a fixed temperature; the upper curves are at higher temperatures. The lower curves are not hyperbolas, because the gas is no longer an ideal gas. (b) An expanded portion of the diagram for low temperatures, where the phase can change from a gas to a liquid.\nPhase changes can only be explained if we\u0026rsquo;re considering the behavior of a real \u0026ndash; not ideal \u0026ndash; gas This is because phase changes involve intermolecular bonds which we only factor in when considering real gases At high enough pressures, gases take up less volume than expected. This effect is magnified with lower temperatures At lower temperatures, the .$PE$ attractive forces between particles aren\u0026rsquo;t negligible with respect to .$KE$ At the critical points (when .$PV$ curve is horizontal), gases may no longer change to liquid under any pressure This point varies by substance Gas below the critical point is vapor Gas above the critical point is just gas Sublimation: When substance changes from solid to gas, skipping liquefaction step Ideal gases would have a straight line along a .$PV/nRT \\text{ vs } P$ graph Real gases vary don\u0026rsquo;t follow this line and deviate from it proportional to their molecule size and weight (resulting in higher .$a$ and .$b$ values respectively \u0026ndash; see 18.5) 18.4 Vapor, Pressure, and Humidity (not covered) # Evaporation # Molecules in liquid are held tightly together with intermolecular attractive forces (covalent hydrogen bonds) Some molecules may momentarily leave the liquid if their velocity is fast enough If velocity isn\u0026rsquo;t too large, then the particle will be pulled back to the liquid surface If velocity is large enough, then the particle will break the intermolecular bonds and leave the liquid to enter their gas form Low probability of occurring Because .$v_{\\text{particle}} \\propto T$, the .$\\text{evaporation rate} \\propto T$ As fast moving (thus hot) particles leave the liquid, the liquid\u0026rsquo;s temperature decreases That is, evaporation is a cooling process Vapor Pressure # A typical phase diagram. The solid green line applies to most substances; the dashed green line gives the anomalous behavior of water\nGreen line = SL Line; transition between solid .$\\iff$ liquid (melting/freezing) Red line = SV; transition between solid .$\\iff$ vapor (sublimation/deposition) Blue line = LV; transition between liquid .$\\iff$ vapor (vaporizing/condensing) When evaporation particles go from gas to liquid, it\u0026rsquo;s called condensation The number of particles in vapor increases until the rate of particles condensing is equal to the number of particles becoming vapor (equilibrium!) When this state is reached, the space above the water is considered saturated Pressure of vapor saturation is called (saturated) vapor pressure Saturated Vapor Pressure varies with the volume of container If volume above the liquid was reduced, then the density would increase so particles would condense back to liquid Assuming .$T$ is constant, vapor pressure would stay constant too Since at high temperatures there are more particles (entering/already in) the vapor phase, higher pressure is required for equilibrium When the volume is large, it\u0026rsquo;s likely that all the liquid evaporates before equilibrium Boiling # Boiling occurs when saturated vapor pressure equals external pressure Bubbling forms as temperature approaches boiling temperature If the pressure in the bubbles are less than the external pressure, the bubbles are crushed Otherwise, bubbles are able to rise to surface Boiling point is proportional to pressure Lower pressure = lower temperature required for boiling point Partial Pressure and Humidity # In gases composed of multiple other gases, the total pressure is the sum of all partial pressures for each of other sub gas Partial Pressure is the pressure a single gas would exert by itself The partial pressure of water in the air can be as low as zero and vary up to a maximum equal to the saturated vapor pressure (of water at the given temperature) Relative Humidity: ratio of partial pressure of water vapor to the saturated vapor pressure at a given temperature $$\\text{Relative Humidity} = \\frac{\\text{partial pressure of }H_2O}{\\text{saturated vapor pressure of }H_2O} \\cdot 100\\%$$ Super Saturation: .$P_{\\text{partial}} \u0026gt; P_{\\text{saturated vapor pressure}}$ Happens when temperature decreases Excess water condenses as dew / mist 18.5 Van der Waals Equation of State # Microscopic (molecular) view accounts for\u0026hellip;\nFinite size of molecules (before we assumed separation .$\\gg$ diameter, ignoring density) Since gas particles aren\u0026rsquo;t negligible in size, we can\u0026rsquo;t use all of our volume Particles are solid spheres that can\u0026rsquo;t get closer than .$2r$ to one another That means .$V$ is over-estimated: .$V_{\\text{real}} \u0026lt; V_{\\text{ideal}}$ Lower volume mean more collisions, leading to pressure being higher than estimated with the ideal gas law: .$P_{\\text{real}} \u0026gt; P_{\\text{ideal}}$ The unavailable volume due to particles, .$b$, depends on the .$n$umber of moles $$ P(V-nb) = nRT \\Longrightarrow P\\bigg(\\frac{V}{n} - b\\bigg) = RT\\ \\ \\ \\big[\\text{Clausius Equation of State}\\big]$$ Where .$b$ is the volume consumed by 1 mol of gas with the units .$\\text{V/mols}$ Forces between molecules (before we assumed that forces only played an effect in collisions) At low .$T$, electric attractive forces aren\u0026rsquo;t 0 Particles towards the edge are slowed down by the other particles attractive forces For that reason, our pressure is lower than estimated with the ideal gas law: .$P_{\\text{real}} \u0026lt; P_{\\text{ideal}}$ On the contrary, with higher temperatures gases appear more ideal because .$KE$ is greater than the intermolecular .$PE$ \u0026ldquo;Slow down\u0026rdquo; is proportional to the gas density M ore dense means more molecules means more intermolecular forces Pressure reduced by the following equation where .$n/V$ is the gas density and .$a$ is a constant unique to the gas that measures the attractive forces between particles $$ a\\bigg( \\frac{n}{V}\\bigg)^2$$ .$a \\propto m$ and boiling point because the lower the boiling point, the less energy is required to break the internal bonds Thus, we can rewrite the ideal gas law with the last two equations as $$ \\bigg(P+ \\frac{a}{(V/n)^2}\\bigg)\\bigg(\\frac{V}{n} - b\\bigg) = RT \\ \\ \\big[\\text{Van der Waals Eq of State}\\big]$$ Note that these equations aren\u0026rsquo;t accurate in all cases, but they\u0026rsquo;re the best generalization we can do and they show the relation With low densities, .$a\\big/(V/n)^2 \\ll P$ and .$b \\ll V/n$ so Van der Waals equation reduces to the ideal gas law 18.6 Mean Free Path # Molecules bump into each other a lot which slow them down Mean free path: Average distance between collisions is proportional to .$\\rho ^{-1}, r^{-1}$ $$l_m = \\frac{1}{4\\pi \\sqrt{2}r^2 (N/V)}$$ 18.7 Diffusion (not covered) # Particles diffuse from high to low concentrations until equilibrium is reached (when .$\\rho$ is constant throughout) Given a tube with a cross section area .$A$, two concentrations, .$C_1$ and .$C_2$, separated by .$\\Delta x$, we can write the rate of diffusion, .$J$, as $$J = DA \\frac{C_1 - C_2}{\\Delta x} = DA \\frac{dC}{dx} \\ \\ \\text{[Fick\u0026rsquo;s Law]}$$ .$D$ is the diffusion constant Varies with temperature, viscosity, and particle size "},{"id":48,"href":"/cogsci-c100/vr/","title":"18: Virtual Reality","section":"CogSci C100","content":" Virtual Reality # Equipment # Headset Made of plastic, rubber, and a few billion transistors Blocks out all ambient light from wearer’s eyes Sensors on headset track your movements and shift the scene accordingly, e.g., gazing up, you see stars Haptic gloves or finger tracking Uses magnets and accelerometers to detect movement Maps onto hands in VR Allows you to manipulate objects, such as picking things up or dancing with someone New developments Multisensory masks that simulate heat, wind, vibration, and rain, as well as 250 scents Can also use VR headset while submerged in pool Vatican # VR allows for a fully immersive experience, where you can 360 VR experience creation By stitching together videos, i.e., doing multi-takes, then merging videos using computer or With 360º camera These can now be placed in helmet to capture snowboarding or biking footage Mixing and mingling in VR # You can also enjoy concerts, sporting events, dance parties, writing workshops or just hang out with other people, represented by avatars If an avatar mingles in a way that bothers you, you can Mute it, making it invisible to you or Install a bubble around your avatar that insulates you from the person Risks: # Eyestrain and headaches; nausea Real world injuries: Journalist testing VR for an article lost her balance when she reached to press button in the virtual world, falling off stool she was sitting on in real life and breaking her toe A Russian man reportedly crashed into a glass table and bled to death while wearing VR goggles Oculus, one of most popular types of VR headsets, now has feature that allows you to map out a safe zone and warns you when you’ve stepped past the perimeter Applications of Virtual Reality # In business # Business meetings Can save on rental costs for office space and hold meetings on a tropical island Supervisor can walk around and hear what people are talking about and join in conversations Furniture shopping Macy’s and IKEA have equipped furniture departments in stores around the country with VR headsets so customers can visualize what furniture will look in their home No need to lug furniture home only to find it doesn’t fit or match Details of room are entered on tablet, e.g., dimensions, window size, wall color Then selected furniture is placed in position and you put on the headset to step into the room Found that people bought more and returned less as a result For education # VR is a full sensory, immersive experience that conveys sense of \u0026ldquo;presence\u0026rdquo;\nResearch by Eric Krokos at University of Maryland and by Strivr in Menlo Park has indicated that presence improves learning and memory Virtual science laboratories are supplementing, and even replacing, real world laboratories in some universities, like Arizona State\nCan be used by students who are unable to afford or access the real thing Also permits students to do things that cannot be done in a real laboratory, like shrinking themselves to the size of a molecule In near future, lab instructor will even be able to \u0026ldquo;teleport\u0026rdquo; into the lab to assist students (Michael Bodekaer) For training purposes # Companies are using VR for training employees Walmart uses VR to train employees in all of its 4700 US stores JetBlue trains technicians virtually \u0026ndash; rather than having to rent a plane or do the training at 3am because that’s the only time the plane is available Verizon has VR instruction module in which employee plays a store manager held up at gunpoint 30 Fortune 500 companies now use VR for training Military uses VR for flight and battlefield simulation Sports: many teams, e.g., football, now train players using 360˚ enactment of plays Health care # Use of VR in preoperative mapping and use of virtual cadavers for medical students Can simulate and test multiple different approaches to a particular surgery to see which one might be best for an individual patient Laparoscopic surgery study: Performance level of novices using VR simulation was increased to that of intermediately experienced laparoscopists Operation time was halved (Larsen, Soerensen, Grantcharov et al., 2009) Loren Sacks at Stanford has developed program for training doctors to deliver bad news to parent of young child or to deal with a difficult patient Gold standard training would be to hire professional actors to run lines with you, but that’s not doable from a financial standpoint For development of empathy # \u0026ldquo;Embodiment through VR can induce can potentially induce a level of understanding and empathy that’s greater than any other form of communication we have today\u0026rdquo; (Thong Nguyen)\nVirtual Interaction Lab at Stanford found that even a brief embodiment inside the avatar of an elderly person has a significant impact on person’s attitude toward the elderly Columbia University’s \u0026ldquo;1000 Cut Journey\u0026rdquo; lets you experience racism firsthand from perspective of a black boy VR body transfer technique: you watch your body morph into someone else’s while you are looking at your reflection in a full-length mirror You experience being teased by other kids about your skin color, being stopped by the police as a teen, being denied a job you deserve… Produces much greater impact than just reading about these things when they happen to \u0026ldquo;you\u0026rdquo; Simulation in which you play the role of a lumberjack, using a chainsaw to cut down a sequoia after learning that similar trees could be saved if you stopped using fluffy toilet paper Stanford researchers found that someone who performs this 2- minute exercise is 20% more likely to use less toilet paper compared with those who merely watched a video about deforestation (Ahn \u0026amp; Bailenson, 2011) Therapeutic Applications # For the elderly # Seniors with memory impairments were given auditory stimulation and VR experiences in path finding Showed improvements in memory tests over controls who received music therapy (Optale, Urgesi, Busato et al., 2009) VR can increase balance and reduce falls (Rendon, Lohman, Thorpe et al., 2012) Virtual reality may also be effective in reducing depression and anxiety in the elderly – allows people with mobility problems to Take trip to French countryside, soar through Yosemite National Park, and explore depths of ocean Attend a grand-daughter’s wedding in real time Revisit childhood home Treatment of pain # Snow World: Participants fly through snowy canyon, shooting snowballs at snowmen, polar bears, and igloos Immersive experience is able to capture attention to distract from pain Used for burn wound treatment, one of most painful types of medical procedures (Keefe, Huling, Coggins et al., 2012; Tashijan, Mosadeghi, Howard et al., 2017) Psychotherapeutic applications # Self-counseling # Effects of self-counseling in VR\nPeople participated in a virtual reality counseling session in which they alternately occupied a body representing themselves and one representing Freud Could give themselves advice from the embodied perspective of Freud Experimental participants experienced greater perception of change than controls who simply spoke to a scripted Freud character (Slater, Neyret, Johnston et al., 2019) Virtual reality can change self perceptions in those with eating disorders Researchers (Keizer, van Elburg, Helms et al, 2016) placed participants in a healthy sized avatar Results: Decrease in overestimation of one’s own body size and improved self body image Altered perception persisted after VR experience Treatment of psychological disorders # VR has been found to be effective in treating PTSD VR can also be effectively used to treat phobias (Goncalves, Pedrozo, Coutinho et al., 2012) (Bohil, Alicea, \u0026amp; Biocca, 2011; Botella, Fernandez-Alverz, Guillen et al., 2017) VR and brain mapping # VR can be used on conjunction with imaging technologies to provide new insight into brain function\nResearchers can present multimodal stimuli with a high degree of ecological validity and control while recording changes in brain activity Results can provide new insights into activity of brain regions involved in Spatial cognition and navigation Multisensory integration of perceptual stimulation Social interaction (Bohil, Alicea, \u0026amp; Biocca, 2011) Augmented Reality (AR) # Superimposes computer-generated visuals and audio over physical landscape Person still sees normal surrounding, but the digital content is inserted into the room as if it’s actually there Can be viewed on handheld device or head-mounted display Allows you to Superimpose directions over the road while you’re driving See someone walking into room who is not really there Imagine if your mother who lives thousands of miles away could walk around in your room… Test out how pink-flowered wallpaper would look in your powder room Applications: # Construction: Inspector can walk around job site and accurately align and compare what is being built against the building information model which is projected through glasses to catch errors easily Order picking in factories: Rather than using paper and hand-held scanner, worker can see instantly what needs to go where Face recognition: Police in China are using smart glasses to identify suspects in crowds Collaborative meetings: allows people from different parts of the world to work together as if they’re standing face-to-face Medical education: can walk around 3-D objects and can literally see what is happening in the body, e.g., how the heart moves "},{"id":49,"href":"/physics-7b/19/","title":"19: Heat \u0026 First Law of Thermo","section":"Physics 7B","content":" \\(\\) 19.1 Heat as Energy Transfer # Units # Heat unit is calorie (cal) The amount of heat needed to raise the temperature of 1 gram of water by 1 celsius $$4.186 \\text{ J} = 1 \\text{ cal}$$ Kilocalorie (kcal, Calorie) is more common Amount of heat needed to raise 1 kg of water by 1 celsius $$4.186 \\text{ kJ} = 1 \\text{ kcal}$$ British system of units has British thermal units (Btu) One Btu is the heat needed to raise the temperature of 1 lb of water by 1 Fahrenheit $$1 \\text{ Btu} = 0.252 \\text{ kcal} = 1056 \\text{ J}$$ Gas companies use the unit therm: .$10^5 \\text{ Btu}$ Heat # Heat is energy transferred from one object to another because of a difference in temperature. Energy transfers from hot to cold object until equilibrium The SI units for heat is the joule: this is because heat is a form of energy! 19.2 Internal Energy # Internal Energy: The sum of all the energy of all the molecules in an object Sometimes called thermal energy Difference between Temp, Heat, and Internal Energy # Temperature is the average kinetic energy of all of the molecules Internal energy is the sum of the energy of all of the molecules E.x. Two equal-mass iron ingots could the same temperature as a single ingot, but the two would have double the internal energy Heat refers to the transfer of energy from one object to another due to a difference in temperatures Direction of transfer depends on temperature, not internal energy E.x. .$50\\text{ g}$ of .$30^\\circ\\text{ C}$ water mixed with .$200 \\text{ g}$ of .$25^\\circ \\text{ C}$ water results with heat transferring from the smaller sample with less internal energy to the larger sample with more internal energy. Calculating Internal Energy # Internal energy is the sum of all the translational kinetic energy of the molecules in a monatomic gas Monatomic: Gas with one atom per molecule We can re-write this as the average KE per molecule times the total number of molecules, .$N$ $$E_{\\text{int}} = N \\bigg(\\frac{1}{2}m\\bar{v}^2\\bigg) = \\frac{3}{2}Nk_B T = \\frac{3}{2}nRT$$ We can see that internal energy for a monatomic gas depends only on the temperature and number of moles If a gas isn\u0026rsquo;t monatomic, then we need to consider the rotational and vibrational energy of the molecules Non-monatomic gasses result in a internal energy at a given temperature compared to a monatomic The internal energy of real gases depends mainly on temperature There are some exceptions of gases depending on pressure and volume as well Internal energy of liquids and solids is more complex It includes electric potential energy of the chemical bonds 19.3 Specific Heat # Amount of heat required to change the temperature of a material is found with the following: $$\\Delta Q = mc \\Delta T$$ .$c$ specific heat capacity that depends on the material .$[\\text{J}/(\\text{C}^\\circ\\text{ kg})]$ For water at .$15 ^\\circ \\text{ C}$ and constant pressure .$1 \\text{atm}$, .$c = 4168 \\text{ J}/(\\text{C}^\\circ\\text{ kg}) = 1.00 \\text{ kcal}/(\\text{C}^\\circ \\text{ kg})$ .$c$ does vary to some extent with temperature (and slightly pressure), but for small .$\\Delta T$ we can say .$c$ is a constant Relative to other materials/substances, water has a high specific heat capacity 19.4 Calorimetry # Types of Systems # System: Any (set of) object(s) we choose to consider Closed System: Mass is constant, but energy may be exchanged within environment Isolated: If no energy in any form passes across its boundaries We idealize systems to be closed systems, which is rare in the real world Heat will flow from hot to cold region of system until equilibrium We can assume that no energy is lost; heat lost in one part = heat gained in another part or .$\\Sigma Q = 0$ Open System: Mass and energy may enter/leave Calorimeter # Calorimetry: Quantitative measure of heat exchange Calorimeter tend to have insulation so that no heat is exchanged with the surrounding air Often use thermometer to measure change the temperature E.x. a substance sample will be heated up, measured, then quickly placed inside cool water of calorimeter The heat lost from the sample will be gained by the water and the calorimeter cup Measuring final temperature of the mixture lets us calculate the specific heat Assume that small masses like the thermometer/stirrer are negligible 19.5 Latent Heat # Change of Phase: When a material changes from solid to liquid or liquid to gas. A certain energy is required for a phase change During phase changes, temperature stops increasing and all energy goes into the phase change Latent heat is lost during phase change (often in the form of heat) Heat of fusion .$L_F$: Heat required to change .$1.0 \\text{ kg}$ of a substance from solid to liquid state Heat fusion of water is .$79.7 \\text{kcal/kg} = 333 \\text{kJ/kg}$ Heat of Vaporization .$L_V$: Heat required to change a substance from liquid to vapor phase Heat vaporization is .$539 \\text{kcal/kg} = 2260 \\text{kJ/kg}$ Heat involved in the phase change depends on the mass and latent heat: $$\\Delta Q = mL$$ Therefore, when considering the change in a system involving heating a substance to a phase change (e.g. boiling at temperature .$T$), we can write: $$\\Delta Q_{\\text{total}} = m_L c \\Delta T + m_S L$$ .$m_L$ is the total mass of the substance before the phase change (e.x. initial mass of substance, don\u0026rsquo;t subtract amount that vaporized) .$m_S$ is the mass of the substance that underwent a phase change (e.x. mass that vaporized) Evaporation # Heat of Vaporization of water increases slightly with a decrease in Temperature At .$20^\\circ \\text{ C}$, it\u0026rsquo;s .$585 \\text{ kcal/kg}$ When liquid evaporates, the remaining liquid cools because the heat/energy comes from the water itself Therefore, internal energy decreases with evaporation Kinetic Theory of Latent Heats # At melting point, the latent heat of fusion doesn\u0026rsquo;t increase the average KE / temperature Rather, the energy goes into overcoming the PE associated with the forces between the molecules Once the molecules in a solid are broken from there lattice formation, they can freely roll over one another as a liquid More energy is required for liquid to gas phase because the average distance between the molecules is greatly increased The larger the distance that the molecules have to be separated, the more work has to be done to pull them apart 19.6 First Law of Thermo # Heat and work are different Heat is the transfer of energy due to a difference in temperature \u0026ndash; hot/cold bath around gas chamber Work is the transfer of energy not due to a temperature difference \u0026ndash; piston applying force to a gas Internal energy and temperature are both proportional to heat and work though with the First law equation: $$\\Delta E_{\\text{int}} = Q - W = E_{\\text{int, 2}} - E_{\\text{int, 1}} \\ \\ \\ \\text{[First Law of Thermo.]}$$ .$W$ is net work done by the system Work done by system is .$\\texttt{+}$ Work done on the system is .$\\texttt{-}$ Gas expands .$\\Longrightarrow$ sys looses energy .$Q$ is net heat added to the system Heat added is .$\\texttt{+}$ Heat lost is .$\\texttt{-}$ Gas is heated .$\\Longrightarrow$ sys gains energy .$Q$ and .$W$ are not state variables in that a static state doesn\u0026rsquo;t have \u0026ldquo;heat\u0026rdquo; or \u0026ldquo;work\u0026rdquo; \u0026ndash; only when the system changes through thermodynamic process can we measure heat/work. This is unlike .$P, V, T$ and .$E_{\\text{int}}$ which are state variables (can be measured at all states) We can also extend the first law to include systems that have KE and PE: $$\\Delta K + \\Delta U + \\Delta E_{\\text{int}} = Q - W = E_{\\text{int, 2}} - E_{\\text{int, 1}}$$ 19.7 Thermodynamic Process and the 1st Law # Isothermal Process (.$\\Delta T = 0$) # When temperature is constant, .$PV$ is constant too Each label of points in the graph above represent the systems states (it\u0026rsquo;s pressure and temperatures) Isotherms: curves in PV diagram At a lower temperature, an isothermal process would be represented by the isotherm .$A\u0026rsquo;B'$ We also assume that the container is a heat reservoir: a body whose mass is so big that the temperature doesn\u0026rsquo;t change when heat is exchanged We increase internal energy by doing work, such as by decreasing the volume of the container with by applying a force to a piston over some distance We assume that expansion/compression is quasistatic: we decrease the volume slow enough that we can consider it a series of equilibrium states all at the same temperature E.x. if we started with state .$A$ and added heat .$Q$ to the system, the system would reach point .$B$ If .$T$ remain constant, the volume will expand, both doing work .$W$ on the environment and decreasing the .$P$ We know .$E_\\text{int} = \\frac{3}{2}nR\\Delta T$, and since .$\\Delta T = 0 \\Longrightarrow E_\\text{int} = 0$ Thus, .$E_\\text{int} = Q - W \\Longrightarrow W = Q$ $$$$ $$$$ $$W = \\int_{V_A}^{V_B} P \\ dV$$ $$\u0026hellip; = nRT \\int_{V_A}^{V_B} \\frac{dV}{V}$$ $$\u0026hellip; = nRT \\ln{\\frac{V_B}{V_A}}$$ Adiabatic Process (.$\\Delta Q = 0$) # No heat allowed to flow in our out of system. This can happen if\u0026hellip; Process happens so quickly that heat, a slow process, has no time to flow in/out E.x. a combustion engine happens quickly it\u0026rsquo;s nearly adiabatic System is well insulated If a system experiences an adiabatic process slowly, it will look similar to curve .$AC$ Since .$Q = 0 \\Longrightarrow \\Delta E_\\text{int} = -W$ In a reverse processes represented by .$CA$ (adiabatic compression), work is done on the gas so .$E_\\text{int}$ and .$T$ rise $$$$ $$$$ $$W = \\int_{V_A}^{V_B} P \\ dV$$ $$\u0026hellip; = P_A V_A ^\\gamma \\int_{V_A}^{V_B} \\frac{1}{V^\\gamma}\\ dV$$ $$\u0026hellip; = \\frac{P_A V_A - P_B V_B}{1-\\gamma}$$ 19.9 Adiabatic Expansions # The .$PV$ curve for adiabatic expansion (.$Q = 0$) is slightly less steep than isothermal processes (.$\\Delta T = 0$) This means that for the same change in volume, the pressure will be greater in adiabatic processes Therefore, the temperature of a gas must drop in adiabatic expansion and rise in adiabatic compression Likewise, if during an adiabatic process the volume increases then the internal energy must decrease We can relate .$P$ and .$V$ for a quasistatic expansion / compression with $$PV^\\gamma = \\text{[constant] for } \\gamma = \\frac{C_P}{C_V} = 1 + \\frac{R}{C_V}$$ \u0026hellip;which can also be written as the following (with .$d$ = degrees of freedom) $$T_A^{C_V/R} V_A = T_B^{C_V/R} V_B$$ $$C_V = \\frac{d}{2}R$$ $$C_P = \\frac{d+2}{2}R$$ $$\\gamma = \\frac{d+2}{d}$$ Free Expansion # A type of adiabatic process where gas is allowed to expand in a volume without doing any work Must be done with insulated containers so that no heat is able to flow in/out; .$Q = 0$ No work is done either because no object is moved; .$W = 0$ Thus, .$\\Delta E_\\text{int} = 0$ and .$\\Delta T = 0$ In reality, we see temperature slightly drops meaning internal energy does depend on pressure or volume as well as temperature. Isobaric and Isovolumetric .$(\\Delta P = 0, \\Delta V = 0)$ # Isobaric: .$\\Delta P = 0 \\Longrightarrow Q = \\Delta E_\\text{int} + W = \\Delta E_\\text{int} + P\\Delta V$. The heat transferred to the system does work, but also changes the internal energy of the system Isovolumetric: .$\\Delta V = 0 \\Longrightarrow W = 0 \\Longrightarrow Q = \\Delta E_\\text{int}$. The thermodynamic process is the addition or removal of heat. First law of thermo holds for both of these processes $$$$ $$$$ $$W_{\\text{Isovol.}} = 0$$ $$W_{\\text{Isobaric}} = \\int_{V_A}^{V_B} P \\ dV$$ $$\u0026hellip; = P \\Delta V$$ $$\u0026hellip; = P_B(V_B - V_A)$$ $$\u0026hellip; = nRT_B(1 - \\frac{V_A}{V_B})$$ Work done in volume changes .$(\\Delta V \\neq 0)$ # For quasistatic processes: $$dW = \\vec{F} \\cdot d\\vec{l} = PA d\\vec{l} = P\\ dV \\ \\ \\ \\text{(1)}$$ $$W = \\int dW = \\int_{V_A}^{V_B} P\\ dV \\ \\ \\ \\text{(2)}$$ .$\\text{(1)}$ Where .$F = PA$ is the force the gas exerts on the piston and .$d\\vec{l}$ is the (small) distance the piston moves .$\\text{(2)}$ This shows that the work done is the area under the .$PV$ curve This equations are valid for work done in any volume change (solid, liquids, gas) .$W$ (and even .$Q$) depends on the initial and final states and also on the process (or path) 19.8 Molar Specific Heats for Gases and Equipartition of Energy # Molar Specific Heat # Specific heat for gases depends heavily on the process and how it\u0026rsquo;s carried out Specific heat for constant pressure and constant volume vary We use molar specific heat for gases: .$C_V$ and .$C_P$ which are defined as the heat required to raise .$1 \\text{ mol}$ of gas by .$1^\\circ \\text{ C}$ at a constant volume or pressure respectively. We then use .$n$ instead of .$m$ in our heat equations: $$\\Delta Q = nC_V \\Delta T = mc_V \\Delta T\\ \\ \\ \\text{[Constant Volume]}$$ $$\\Delta Q = nC_P \\Delta T = mc_P \\Delta T\\ \\ \\ \\text{[Constant Pressure]}$$ which we can then relate to the specific heat with .$M$ as the molecular mass of the gas, .$m/n$ in grams/mol: $$C_V = Mc_V$$ $$C_p = Mc_p$$ In a heating process, when .$\\Delta V = 0$ then the heat added, .$Q_V$ goes entirely into internal energy: .$Q_V = \\Delta E_\\text{int}$ However, when pressure is constant work is done. Thus, heat added, .$Q_P$, goes towards increasing internal energy and work: .$W = P\\Delta V$ Therefore, more heat is needed for a constant pressure system: .$Q_P = \\Delta E_\\text{int} + P\\Delta V$ Since .$\\Delta E_\\text{int}$ is the same for both processes, we can write .$Q_P - Q_V = P \\Delta V$ With an ideal gas, we know .$V = nRT/P$ so .$\\Delta V = nR\\Delta T/P$ which we can combine with the prior equations to get: $$nC_P\\Delta T - nC_V \\Delta T = P\\bigg(\\frac{nR\\Delta T}{P}\\bigg) \\Longrightarrow C_P - C_V = R$$ We can also relate internal energy to molar specific heat for gases at constant volumes: $$\\Delta E_\\text{int} = Q_V \\Longrightarrow \\frac{\\text{[Deg. of Freedom]}}{2}nRT = nC_V\\Delta T \\Longrightarrow C_V = \\frac{3}{2}R$$ We can then plug in our new value for .$C_V$ into the second to last equation .$C_P - C_V = R$ to get .$C_P = \\frac{5}{2}R$ for a monatomic gas. We can also combine our equations to write a relation between internal energy and temperature again: $$\\Delta E_\\text{int} = nC_V \\Delta T$$ Equipartition of Energy # Degrees of Freedom: The number of independent ways a molecule can posses energy Degrees of freedom depend on the temperature At low temperatures, the only degree of freedom is from translational .$KE$ Starting after .$0K$ Diatomic gas: .$C_V = \\frac{3}{2}R$ (3 for each axis) Sum of .$\\frac{1}{2}m \\langle v_x, v_y, v_z \\rangle$ At \u0026ldquo;regular\u0026rdquo; temperatures, the molecules posses rotation energy Around .$50K$ Diatomic gas: .$C_V = \\frac{5}{2}R$ Sum of .$\\frac{1}{2}I \\langle 0, \\omega_y, \\omega_z \\rangle$ (since it\u0026rsquo;s rotating about .$\\hat x$ meaning .$E_{\\text{rotational, }x}) = 0$ At higher temperatures, the molecules gain energy associated with their vibrations: Around .$1000K$ One from KE of the molecules vibrating back and forth: .$\\frac{1}{2}mv_{\\text{COM}}^2$ The second from PE of the vibrational motion (think of this as a spring\u0026rsquo;s PE): .$\\frac{1}{2}kx^2$ Solids: The molar temperature of solids at high temperatures is close to .$3R$. At high temperatures, there are six degrees of freedom: three from vibrational KE in the .$x, y,$ and .$z$ axis and three more from spring PE in the same axis Some of these degrees of freedom aren\u0026rsquo;t active at lower temperatures Principle of Equipartition of Energy: Energy is shared equally among degrees of freedom and each degrees has energy .$\\frac{1}{2}k_B T$ Thus, for a particle with three degrees of freedom (such as a monatomic gas) .$C_V = \\frac{3}{2}R$ Diatomic gases have five degrees so they have .$C_V = \\frac{5}{2}R = 4.97 \\text{ cal/(mol K)}$ and have .$E_\\text{int} = N(\\frac{5}{2}k_B T) = n C_V \\Delta T = \\frac{5}{2}nRT$ where .$n$ is the number of moles and .$N$ is the number of molecules 19.10 Heat Transfer # Conduction # Heat transfer by contact Conduction can be visualized thinking of molecular collisions The hot end of an object has fast moving molecules These molecules bump into other molecules, transferring them some of their own KE This keeps repeating down the object Free electrons are the primary source of these collisions Heat conduction only occurs when there is a difference in temperatures Heat conduction rate is proportional to the difference in temperatures: $$\\frac{\\Delta Q}{\\Delta t} = - kA\\frac{T_1 - T_2}{l}$$ Where .$A$ is the cross section area, .$l$ is the distance between the two ends, and .$k$ is a constant called thermal conductivity that depends on the material Good insulator / poor thermal conductors have a low .$k$ Metals have .$k\u0026gt;1$ Wood, plastics have small .$k$s Building materials sometimes list the thermal resistance, .$R$, which is equal to .$R = \\frac{l}{k}$ where .$l$ is the material\u0026rsquo;s thickness Larger .$R$ means better insulation If .$k$ or .$A$ isn\u0026rsquo;t constant, we consider a small thickness: $$ \\frac{dQ}{dt} = -kA \\frac{dT}{dx}$$ .$\\frac{T_1 - T_2}{l} \\text{ and } \\frac{dT}{dx}$ are called the temperature gradients We have a negative sign in the equation above because the direction of heat flow is opposite to the temperature gradient A steady system state is reached when heat flow through each layer of an object is equal Convection # Heat flow by movement of mass Convection involves heat flowing by the bulk movement of molecules from one place to another Whereas conduction involved molecules/electrons moving over small distances, convection involves the movement of a large number of molecules over a long distance Natural Convection occurs in systems where a cold substance (air, water) is warmed and subsequently expands, decreasing density and thus rising Warm fluid/gases are less dense, thus they rise compared to colder fluid/gas Radiation # Whereas conduction and convection require a medium, radiation doesn\u0026rsquo;t The sun\u0026rsquo;s rays are a form of heat and travel through (nearly empty) space Radiation of the sun\u0026rsquo;s rays arrive on a clear day at a rate around .$1000 \\text{W/m}^2$ Most of the time radiation consists of electromagnetic waves, but infrared (IR) wavelengths are responsible for heating Earth The rate at which energy leaves a radiation object, .$Q/t$, is $$ \\frac{\\Delta Q}{\\Delta t} \\varepsilon \\sigma A T^4$$ .$\\varepsilon$ is called emissivity. Between 0 and 1 Characteristic of the surface of the radiating material Black surfaces close to one, shiny metal surfaces close to zero Depends slightly on the temperature of the material A good absorber is also a good emitter A black tee shirt gets very hot because it absorbs nearly all the radiation that hits it .$\\sigma$ is the Stefan-Boltzmann constant: .$\\sigma = 5.67 \\cdot 10^{-8} \\text{ W/(m}^2 \\text{K}^4\\text{)}$ Objects also absorb heat of surrounding objects. This net heat flow can be found by $$ \\frac{\\Delta Q}{\\Delta t} \\varepsilon \\sigma A (T_1^4 - T_2^4)$$ Where .$T_1$ is the object\u0026rsquo;s temperature and .$T_2$ is the surrounding environment\u0026rsquo;s temperature .$T_1 \u0026gt; T_2$: net flow of heat is from object to the surroundings .$T_1 \u0026lt; T_2$: net flow of heat is from surroundings into object, raising the object temperature "},{"id":50,"href":"/physics-7b/20/","title":"20: Second Law of Thermo","section":"Physics 7B","content":" 20.1 Intro # Second law states that systems only increase in entropy over time That is, most systems are on directional E.x. mixing salt and pepper together result in an mixture. No matter how much you keep mixing it, they won\u0026rsquo;t naturally separate and return to the initial state even though it follows first law of thermo (conserving energy) (Specific) Second Law of Thermo Heat can flow spontaneously from a hot object to a cold object; heat will not flow spontaneously from a cold object to a hot object.\n20.2 Heat Engines # Heat Engine: Any device that changes thermal energy into mechanical work, such as steam or car engine Show importance in developing the second law of thermo Mechanical energy can only be obtained from thermal energy when heat is allowed to flow from high temp to low temp During this process, some of the heat can be transformed to mechanical work Heat engines run in a repeating cycle: the system returns repeatedly to its starting point and thus can run continuously In each cycle .$\\Delta E_{\\text{int}} = 0$ because it returns to the initial state Thus, heat input .$Q_H$ at a .$T_H$ is partly transformed into work .$W$ and partly exhausted as heat .$Q_L$ at .$T_L$ By conservation of energy, .$Q_H = W+Q_L$. Operating Temperatures: The high and low temperatures, .$T_H, T_L$ .$Q_H, Q_L, W \u0026gt; 0$ Change in temperature is required for a change in pressure Gas exhaust is cooled to a lower temperature and condensed so that the exhaust pressure is less than intake pressure Thus, the work the piston must do on the gas to expel it is less than the work done by the gas on the piston during the intake 20.3 (Ir)reversible Processes; Carnot Engine # Carnot engine is ideal: doesn\u0026rsquo;t take into account turbulence in gas, friction, etc. Consist of four processes done in a cycle Isothermal expansion (.$\\Delta T = 0$) with the addition of heat .$Q_H$ along path .$ab$ at temperature .$T_H$ Adiabatic expansion (.$Q = 0$) lowering temperature to .$T_L$ along path .$bc$ Isothermal compression (.$\\Delta T = 0$) leads to heat .$Q_L$ flowing out along path .$cd$ Adiabatic compression (.$Q = 0$) occurs path .$da$, returning to temperature .$T_H$ Each process is reversible; that is, each occurs infinitely slowly so that the process could be considered a series of equilibrium states Real processes are irreversible Work done in a cycle is proportional to area enclosed by the curve representing the cycle on a .$PV$ diagram (.$abcd$) Efficiency is given by .$e = 1-\\frac{Q_L}{Q_H} \\Longrightarrow e_{\\text{ideal}} = 1 - \\frac{T_L}{T_H}$ Carnot\u0026rsquo;s Theorem: All reversible engines operating between the same two constant temperatures .$T_H$ and .$T_L$ have the same efficiency. Any irreversible engine operating between the same two fixed temperatures will have an efficiency less than this.\nOnly at absolute zero would 100% efficiency be reachable. But getting to absolute zero is a practical (as well as theoretical) impossibility Kelvin-Planck statement of the second law of thermodynamics: no device is possible whose sole effect is to transform a given amount of heat completely into work.\n20.4 Refrigerators, AC, Heat Pumps # Refrigerators, air conditioners, and heat pumps are just the reverse of heat engines Each transfer heat ouf of a cool environments into a warm environment A perfect fridge (no work required to take heat from low temp to high temp) is impossible No device is possible whose sole effect is to transfer heat from one region at a temperature .$T_L$ into a second region at a higher temperature .$T_H$ (Clausius statement)\nCoefficient of Performance (COP): .$\\text{COP} = \\frac{Q_L}{W}$\nThe more heat .$Q_L$ removed from a fridge for a given amount of work, the more efficient it is Energy is conserved, so we can write .$Q_L + W = Q_H$ or .$W = Q_H-Q_L$ We can then write .$\\text{COP} = \\frac{Q_L}{W} = \\frac{Q_L}{Q_H-Q_L} \\Longrightarrow \\text{COP}_{\\text{ideal}} = \\frac{T_L}{T_H-T_L}$ Heat pump\nElectric motor does work .$W$ to take heat .$Q_L$ from outside at low temperature and delivers heat .$Q_H$ to inside at a hot temperature Whereas fridges cool (remove .$Q_L$), heat pumps heat (deliver .$Q_H$) Thus, COP uses .$Q_H$ instead of .$Q_L$: .$\\text{COP} = \\frac{Q_H}{W}$ COP is greater than 1 because .$W+Q_L = Q_H$ 20.5 Entropy # Entropy, unlike heat, is a state variable and measures the (dis)order of a system When heat is added to a system by a reversible process then change in entropy is $$\\Delta S = \\frac{Q}{T} \\ \\ \\text{[Constant T]} \\Longrightarrow dS = \\frac{dQ}{T} \\ \\ \\text{[Non-const T]}$$ The change of entropy between two states doesn\u0026rsquo;t depend on the process. Thus, $$\\Delta S = S_b - S_a = \\int_a^b dS = \\int_a^b \\frac{dQ}{T}$$ 20.6 Entropy and Second Law # In an isolated system with two objects that eventually reach equilibrium, we can write the change (increase) in entropy as $$\\Delta S = \\Delta S_H + \\Delta S_L = - \\frac{Q}{T_{HM}} + \\frac{Q}{T_{LM}}$$ .$T_{HM}$ is the average temperature between .$T_H$ and .$T_M$ where .$T_M$ is the average between .$T_H$ and .$T_L$ E.x. if .$T_H = 0^\\circ C, T_L = 0^\\circ C$, then .$T_M = 4^\\circ C$ so .$T_{HM} = 6^\\circ C$ and .$T_{LM} = 2^\\circ C$. Also, we use .$Q = mc \\Delta T$ to find heat and use half .$T_{M}$ (in this case .$4^\\circ C$) for .$\\Delta T$ Since .$T_{HM} \u0026gt; T_{LM}, \\Delta S \u0026gt; 0$ is always true While one system may decrease in entropy, the other one always increases more so net always increases For adiabatic processes, we know .$dQ = dW = P dV$, thus $$\\Delta S_\\text{gas} = \\int \\frac{dQ}{T} = \\frac{1}{T} \\int_{V_1}^{V_2} P\\ dV$$ and since we know through the idea gas law that .$P = nRT/V$ so $$\u0026hellip;= \\frac{1}{T} \\int_{V_1}^{V_2} \\frac{nRT}{V}\\ dV = nR \\ \\ln \\bigg(\\frac{V_2}{V_1}\\bigg)$$ 20.7 Order to Disorder (not covered) # If we say that entropy is a measure of (dis)order in a system, we can write the second law as Natural processes tend to move toward a state of greater disorder\nWhen ice melts to water at 0°C, the entropy of the water increases. Intuitively, we can think of solid water, ice, as being more ordered than the less orderly fluid state which can flow all over the place. This change from order to disorder can be seen more clearly from the molecular point of view: the orderly arrangement of water molecules in an ice crystal has changed to the disorderly and somewhat random motion of the molecules in the fluid state. When a hot substance is put in contact with a cold substance, heat flows from the high temperature to the low until the two substances reach the same intermediate temperature. At the beginning of the process we can distinguish two classes of molecules: those with a high average kinetic energy (the hot object), and those with a low average kinetic energy (the cooler object). After the process in which heat flows, all the molecules are in one class with the same average kinetic energy; we no longer have the more orderly arrangement of molecules in two classes \u0026ndash; Order has gone to disorder Furthermore, the separate hot and cold objects could serve as the hot and cold-temperature regions of a heat engine, and thus could be used to obtain useful work. But once the two objects are put in contact and reach the same temperature, no work can be obtained. Disorder has increased, because a system that has the ability to perform work must surely be considered to have a higher order than a system no longer able to do work. When a stone falls to the ground, its macroscopic kinetic energy is transformed to thermal energy. Thermal energy is associated with the disorderly random motion of molecules, but the molecules in the falling stone all have the same velocity downward in addition to their own random velocities. Thus, the more orderly kinetic energy of the stone as a whole (which could do useful work) is changed to disordered thermal energy when the stone strikes the ground. Disorder increases in this process, as it does in all processes that occur in nature. 20.8 Unavailability of Energy; Heat Death (not covered) # In any natural process, some energy becomes unavailable to do useful work\nThat is, as time goes on, both energy is degraded and entropy increases A rock that falls to the ground could instead used it\u0026rsquo;s energy towards useful work versus exerting kinetic/thermal energy while falling Two separate hot and cold objects could serves as the high and low temperature regions for a heat engine (obtaining useful work). Instead, if the tow objects are put in contact with one another, they\u0026rsquo;ll eventually reach the same uniform temperature and not be able to do any work. Heat Death: All energy of the universe degrades into thermal energy Very far out Scientists are unsure whether this is inevitable or whether we can even extrapolate the 2nd law to the scale of our universe 20.9 Statistical Interpretation of Entropy/2nd (not covered) # We can only realistically observe macrostates and not microstates However, we can make inferences about microstates with probabilities Each microstate is equally probable of occurring Thus, the number of microstates that give the same macrostate correspond to the relative probability of that macrostate occurring The most probable state of a gas is one in which the molecules take up the whole spaces and move about randomly (in a maxwell distribution) At the same time, the very orderly arrangement of all molecules located in one corner of the room and all moving with the same velocity is extremely unlikely Therefore, the probability is directly related to the disorder and hence entropy of the system The most probably state is the one with greatest entropy or greatest disorder and randomness It\u0026rsquo;s also the macrostate that corresponds to the most microstates The netropy of a system in a given macro state can be written as: $$ S = k \\ \\ln\\mathscr{W}$$ .$k$ is the Boltzmann\u0026rsquo;s constant and .$\\mathscr{W}$ is the number of microstates corresponding to the given macrostate That is, .$\\mathscr{W}$ is proportional to the probability of occurrence of that state .$\\mathscr{W}$ is also called the thermodynamic probability or the disorder parameter 20.10 Thermo Temperature; Third Law (not covered) # Ideal Carnot Cycles always have the ratio $$\\frac{Q_L}{Q_H} = \\frac{T_L}{T_H}$$ Note that this relation doesn\u0026rsquo;t depend on the working substance, thus it can server as the basis for the Kelvin scale The closer a temperature is to abs zero, the more difficult it is to reduce the temp further Third Law: It is not possible to reach absolute zero in any finite number of processes\nThus, since .$e = 1 - \\frac{T_L}{T_H}$ and because .$T_L$ can\u0026rsquo;t ever be zero then 100% efficiency is never possible "},{"id":51,"href":"/physics-7b/21/","title":"21: Electric Charges \u0026 Fields","section":"Physics 7B","content":" 21.1 Static Electricity; Electric Charge and its Conservation # \u0026ldquo;Charged\u0026rdquo; objects posses a net electric charge Unlike charges attract; like charges repel Charges on glass are positive, charges on plastic is negative Law of Conservation of Electric Charge: Whenever a certain amount of charge is produced in one object, an equal amount of the opposite type of charge is produced in another object Charges cannot be destroyed or created E.x. a plastic ruler is rubbed with a paper towel. The plastic acquires a negative charge and the towel obtains an equal positive charge In other words, the net amount of electric charge produced in any process is zero: .$\\Sigma Q = 0$ 21.2 Electric Charge in the Atom # Atoms are made up of positive nucleus surrounded by at least one negatively charged electron. Inside the nucleus are protons which are positively charged and neutrons which have no charge The charges of electrons and protons are equal in magnitude E.x. neutral atoms with no charge contain an equal number of protons and electrons When an atom gains a charge (by losing/gaining electrons), it then has a net charge and is called an ion Neutral objects have a net charge of zero Over time, objects left alone with a charge tend to lose their charge This is because over time, electrons are exchanged with water molecules in the air Water molecules are polar: They are neutral, their charges aren\u0026rsquo;t equally distributed Thus, on rainy days it\u0026rsquo;s harder for an object to maintain a charge for too long 21.3 Insulators and Conductors # Conductor: Material that allow charge to flow between objects Metals tend to be good conductors Electrons (charges) are relatively lose: can move freely within metal, but can\u0026rsquo;t leave easily Called free or conduction electrons Insulator: Opposite of conductors; don\u0026rsquo;t easily allow a flow of charge Most materials other than metals tend to be good insulators Notably rubber and wood Electrons are bound very tightly to the nuclei Almost no free electrons Semiconductors: Somewhere between the two former Silicon, germanium Less free electrons than a conductor, but more than an insulator 21.4 Induced Charge; Electroscope # Conduction: Charge transfer by physical contact E.x. a positively charged metal rod touches a neutral metal rod. Free electrons from the neutral rod will then flow (transfer) to the charged rod, leaving the formerly neutral rod now slightly positively charged Induction: Charge distribution altered by bringing two objects close, but not touching Unlike conduction, induction doesn\u0026rsquo;t alter the net charge of objects when the inducer is taken away However, induction can redistribute the existing charges on the induced object Grounded Objects Objects can be ground to the earth with a conducting wire The earth is very large and can conduct, so it easily accepts/gives up electrons Therefore, when an object is induced by another charged object, the original objects will become charged If the wire is ever cut when the object is under induction, the charge will stay in the object Electroscope .$\\vec F \\propto \\text{angle of deflection}$ .$y$-axis: .$F_{T1} \\sin \\theta_1 = F_{21}$ .$x$-axis: .$F_{T1} \\cos \\theta_0 = m_1 g$ .$F_{21} = m_1 g \\tan \\theta_1 \\approx m_1 g \\theta_1$ .$F_{21} = - F_{12}$ ( Newton\u0026rsquo;s Third) .$ \\Longrightarrow \\theta_1/\\theta_2 = m_2/m_1$ .$d = l (\\theta_1 + \\theta_2)$ 21.5 Coulomb\u0026rsquo;s Law # Coulomb\u0026rsquo;s Law: $$E_\\text{source} = k \\frac{Q_\\text{source}}{r^2} \\Longrightarrow F = EQ = \\bigg(k \\frac{Q_1}{r^2}\\bigg) (Q_2) = k\\frac{Q_1 Q_2}{r^2}$$ where .$k$ is a constant equal to .$\\frac{1}{4\\pi\\varepsilon_0} = 8.988 \\cdot 10^9 \\text{ N m$^2$/C$^2$}$ Very similar to universal gravitation equation However\u0026hellip; .$F_C$ can repel, whereas .$F_G$ is always attractive .$F_C$ only acts on charged objects, whereas .$F_G$ acts on neutral objects too .$F_G/F_C \\approx 10^{-40} \\Longrightarrow F_C \\gg F_G$ The coulomb (.$\\text{C}$) is the SI unit for charge Properties of Coulomb Force: It can be attractive and repulsive It is not a contact force Inversely proportional to .$r^2$ Proportional to amount of charge .$Q$ The smallest charge we\u0026rsquo;ve observed is the elementary charge: .$e = 1.6022 \\cdot 10^{-19} \\text{ C}$ Electrons have a charge equal to .$-e$ Protons have a charge equal to .$+e = -Q_\\text{electron}$ Charges are Quantized That is, all charges are multiples of .$e$ Since electrons are elementary particles, by definition they can\u0026rsquo;t be divided. .$k$ can also be written as .$\\frac{1}{4\\pi\\varepsilon_0}$ .$\\varepsilon_0$ is called the permittivity of free space .$\\varepsilon_0 = \\frac{1}{4\\pi k} = 8.85 \\cdot 10^{-12} \\text{C$^2$/N m$^2$}$ 21.6 Electric Field # Electric fields extend outward from every charge and permeates all of space $$\\overrightarrow E = \\lim_{q\\to0}\\frac{\\overrightarrow F}{q} \\Longrightarrow \\overrightarrow F = q \\overrightarrow E$$ .$q$ is a positive charge .$\\overrightarrow F$ is the forces the field exserts on .$q$ Has units newtons per coulomb (.$\\text{N/C}$) We can combine this with Coulomb\u0026rsquo;s law to get $$\\overrightarrow E = \\frac{kqQ/r^2}{q} = k \\frac{Q}{r^2} = \\frac{1}{4\\pi\\varepsilon_0} \\frac{Q}{r^2}$$ We see that .$\\overrightarrow E$ is independent of the non-source particle .$q$ .$Q$ is the particle that is responsible for the field in the first place An electric field at a given point is the sum of all other electric fields that act on that point $$\\overrightarrow E = \\overrightarrow E_1 + \\overrightarrow E_2 + \u0026hellip;$$ 21.7 Electric Field Calculations for Continuous Charge Distributions # We can extend our previous definition to calculus as $$\\overrightarrow E = \\int d \\overrightarrow E = k \\int \\frac{1}{r^2}\\ dq = \\frac{1}{4\\pi\\varepsilon_0} \\int \\frac{1}{r^2}\\ dq$$ .$dq = \\lambda\\ dl \\text{ (line)} = \\sigma\\ dA \\text{ (disk)} = \\rho\\ dV \\text{ (sphere)}$ Calculating field generated by a continuous charge distribution\nDraw an arbitrary \u0026ldquo;piece\u0026rdquo; of charge distribution; don\u0026rsquo;t choose a special point such as the end or exact middle. The piece should be infinitesimally long and/or wide. Thus, its length or width will be something like .$dx$ or .$ds$ Write an expression for .$dq$, the corresponding infinitesimal charge of that piece in terms of .$dx$ or .$ds$ or whatever. Recall .$dq = \\frac{\\text{total charge}}{\\text{total length}} \\times \\text{(tiny length of piece)}$ Using Coulomb\u0026rsquo;s law, find the infinitesimal electric field at that point of interest (e.x. some point .$P$) generated by the piece chosen in step 1. When necessary, break .$d\\vec E$ into components, .$dE_x$ and .$dE_y$ Integrate .$dE_x$ or .$dE_y$ over the whole charge distribution to obtain the total electric field in the .$x$ or .$y$ direction respectively When solving problems, it\u0026rsquo;s a good idea to use symmetry, check charge direction, and (when applicable) use bounds of .$r \\in [0, \\infty]$ We can write equation for an infinite plane holding a uniform surface charge density .$\\sigma$ $$2A \\cdot \\overrightarrow E = \\frac{\\sigma A}{\\varepsilon_0} \\Longrightarrow \\overrightarrow E = \\frac{\\sigma}{2\\varepsilon_0}$$ This also applies in the case where a charge is close to an infinite surface (so that the distance to the surface is much greater than the distance to the edges) In the case where there are two oppositely charged sheets parallel to one another, the field is .$\\vec E = \\frac{\\sigma}{\\varepsilon_0}$ since there are two charges creating the field The case involving an infinitely long wire can be written generally as $$\\overrightarrow E \\cdot 2\\pi RL = \\frac{\\lambda L}{\\varepsilon_0} \\Longrightarrow \\overrightarrow E = \\frac{\\lambda}{2\\pi\\varepsilon_0 \\cdot r}$$ .$r$ is the distance from a particle to the wire 21.8 Field Lines # To visualize electric fields, we draw electric field lines or lines of force Three properties of Electric Field Lines: Electric field lines indicate the direction of the electric field; the field points are in the direction tangent to the field line at any point \u0026ndash; see point .$P$ in .$\\text{(a)}$ The lines are drawn so that the magnitude of the electric field, .$E$, is proportional to the number of lines crossing unit area perpendicular to the lines (i.e. a circle \u0026lsquo;hugging\u0026rsquo; a point charge). The closer together the lines, the stronger the field. Electric field lines start on positive charges and end on negative charges; and the number starting or ending is proportional to the magnitude of the charge. .$\\text{Density} = \\frac{\\text{number of lines crossing surface}}{\\text{area surface}}$ .$\\text{1 Coulomb} = \\frac{1}{\\varepsilon_0} \\cdot \\text{ lines}$ .$\\therefore \\text{Density} = \\frac{q}{\\varepsilon_0 4\\pi r^2} \\Longrightarrow \\vec E$ In the case of two oppositely charged parallel \u0026amp; equally spaces plates \u0026ndash; such as case .$\\text{(d)}$ \u0026ndash; we can write the field as $$\\overrightarrow E =\\text{const.} = \\frac{\\sigma}{\\varepsilon_0}=\\frac{Q}{\\varepsilon_0 A}$$ .$Q =\\sigma A$ is the charge on one plate of area .$A$ Field lines never cross because it wouldn\u0026rsquo;t make sense for an electric field to have two directions at the same point. Electric Dipole # A combination of two equal but opposite charges next to one another \u0026ndash; see .$(\\text{a})$ above Dipole Moment is when represented by vector .$\\vec{p}$ of magnitude .$Ql$ Molecules that have dipole moments are called polar molecules A dipole in a uniform electric field feels no net force, but does have a net torque (unless .$\\vec p \\parallel \\vec E$) If .$\\vec p \\not \\parallel \\vec E$, .$W =\\int_{\\theta_1}^{\\theta_2} \\tau d\\theta$ where .$\\tau = -\\vec p\\vec E\\sin\\theta = \\vec p \\times \\vec E$ Simplifies to .$W =\\vec p\\vec E(\\cos\\theta_2 - \\cos\\theta_1)$ Thus, work/torque is most at .$\\theta = 90^\\circ$ or .$180^\\circ$ depending on .$\\vec E$ direction Pay attention to right hand rule when solving If .$r \\gg l \\Longrightarrow \\overrightarrow E \\propto 1/r^3$ 21.9 Electric Fields and Conductors # The static electric field inside a conductor is zero (in static situations where electrons have had time to stop moving) For that reason, any net charge on a conductor distributes itself on the surface Charges inside conductors act as if the conductor isn\u0026rsquo;t there All the electric field lines just outside a charged conductor are perpendicular to the surface 21.10 Motion of Charged Particle # Vector Form of Forces $$\\overrightarrow F_{12} = k \\frac{q_1 q_2}{r^2} \\cdot \\widehat r_{21}$$ Notation: .$\\overrightarrow F_{12}$ means force on .$q_1$ by .$q_2$ since .$q_2$ is the source charge .$\\widehat r_{21} = - \\widehat r_{12} \\Longrightarrow \\overrightarrow F_{12} = -\\overrightarrow F_{21}$ Direction If .$q_1 q_2 \u0026gt; 0$ (same sign, repulse), then the force and unitary vectors both point away from the two charges If .$q_1 q_2 \u0026lt; 0$ (opposite sign, attract), then the force vector points towards the two charges and the unitary direction vector still points away from the two charges Superposition Principle In a system considering multiple (3+) charges, forces acting on .$q_1$ by .$q_2$ (.$F_{12}$) is independent from whether other charges are present Total forces acting on .$Q_1$ can be written as .$\\overrightarrow F = \\overrightarrow F_{12} + \\overrightarrow F_{13} + \\dots$ Remember to break down the vectors into .$x/y$ components when adding them E.x. .$F_{1x} = F_{12x} + F_{13x} + \\dots$ Realize that the axis are arbitrary .$\\theta = \\tan^{-1}\\Big(\\frac{F_x}{F_y}\\Big)$ Charges in Fields Charge moving with .$\\vec v$ that is parallel to uniform field .$\\overrightarrow E$ .$\\overrightarrow F = q \\overrightarrow E = m \\vec a \\Longrightarrow a_x = \\frac{q}{m}\\overrightarrow E = \\text{const.}$ .$\\vec v = \\sqrt{2a_x \\vec d} = \\sqrt{\\frac{2q}{m}\\overrightarrow E_x \\vec d}$ Charge moving with .$\\vec v$ that is orthogonal to uniform field .$\\overrightarrow E$ Similar to projectile in gravitational field: .$\\vec g \\sim \\overrightarrow E$ .$\\overrightarrow F_x = 0 \\Longrightarrow v_{x2} = v_{x1};\\ \\ a_x = 0$ .$\\overrightarrow F_y = q \\overrightarrow E = m a_y;\\ \\ a_y = \\vec a = \\frac{q}{m}\\overrightarrow E = \\text{const.}$ .$y(t) = \\frac{1}{2} \\frac{q\\overrightarrow E}{m}t^2$ 21.11 Electric Dipoles # Notes for this chapter are under 21.8 \u0026ndash; Electric Dipole "},{"id":52,"href":"/physics-7b/22/","title":"22: Flux \u0026 Gauss's Law","section":"Physics 7B","content":" 22.1 Electric Flux # Electric Flux: Electric field that passes through a given area E.x. for a uniform field .$\\vec E$ passing through an area .$A$ at angle .$\\theta$ between the field direction and line perpendicular to the area, the flux is defined as $$\\Phi_\\vec{E} = \\vec E_\\perp A = \\vec EA_\\perp = \\vec E A \\cos \\theta = \\vec E \\cdot \\vec A$$ The .$N$umber of field lines passing through unit area perpendicular to the field .$A_\\perp$ is proportional to the magnitude of the field .$\\vec E$ $$\\vec E \\propto N/A_\\perp \\Longrightarrow N \\propto \\vec EA_\\perp = \\Phi_\\vec{E}$$ For non-uniform fields: We divide up the surface into .$n$ small elements of surface whose areas are .$dA$ where .$dA$ is small enough (1) to be considered flat and (2) so .$E$ varies so little it can considered uniform $$\\Phi_\\vec{E} = \\oint_A \\vec E \\cdot d\\vec A$$ If .$\\Phi \u0026gt; 0$, flux is entering the volume and .$\\Phi \u0026lt; 0$ is flux leaving Direction: For closed surfaces, .$\\vec A$ points outwards from the enclosed volume, so flux is positive Further, .$\\theta$ (angle between .$d\\vec A$ and .$E$) should always be, for electric field\u0026hellip; Leaving the volume: Less than .$\\pi/2$ (so .$\\cos\\theta \u0026gt; 0$) and .$\\Phi \u0026gt; 0$) Entering the volume: Greater than .$\\pi/2$ (so .$\\cos\\theta \u0026lt; 0$ and .$\\Phi \u0026lt; 0$) Net Flux In the example above, every line that enters also leaves so .$\\Phi = 0$ meaning there is no net flux into or out of the enclosed surface Flux will only be nonzero if one of more lines start or end within the surface Flux through .$A_1$ is positive, .$A_2$ is negative Net flux through .$A$ is negative 22.2 Gauss\u0026rsquo;s Law # Gauss\u0026rsquo;s Law: We can relate flux through a surface and net charge enclosed within said surface by $$\\Phi = \\oint \\vec E \\cdot d\\vec A = \\frac{Q_\\text{enclosed}}{\\varepsilon_0}$$\nThis tells us the difference between the input and output flux of the electric field over any surface is due to charge within that surface. This is because we defined .$1 \\text{ Coulomb} = \\varepsilon_0^{-1} \\text{ field lines}$ Notice that it doesn\u0026rsquo;t matter the distribution of the charge inside the surface A charge outside the chosen surface may affect the position of the electric field lines, but it won\u0026rsquo;t affect the number of lines entering of leaving the surface Irregular Surfaces: Since flux is proportional to the flux lines passing in/out, and the number of lines is the same for .$A_1$ and .$A_2$, so $$\\oint_{A_1} \\vec E \\cdot d \\vec A = \\oint_{A_2} \\vec E \\cdot d \\vec A = \\frac{Q}{\\varepsilon_0}$$ Therefore, this is true for any surface surrounding a single point charge .$Q$ The superposition principle from last chapter also applies to Gauss\u0026rsquo;s law: The total field .$\\vec E$ is equal to the sum of the fields due to each separate charge: $$\\oint \\vec E_i \\cdot d \\vec A = \\oint \\Big(\\Sigma \\vec E_i \\Big) \\cdot d \\vec A = \\sum \\frac{Q_i}{\\varepsilon_0} = \\frac{Q_\\text{enclosed}}{\\varepsilon_0}$$\n22.3 Applications of Gauss\u0026rsquo;s # Gauss\u0026rsquo;s Law to calculate electric fields\nUsing symmetry and intuition, draw the electric field lines. Enclose all or part of the charge distribution with a Gaussian surface. The electric field should have the same strength at all points on (at least part of) the surface. Apply Gauss\u0026rsquo;s law: .$\\Phi_\\vec{E} = \\oint \\vec E \\cdot d\\vec A = \\frac{Q_\\text{enclosed}}{\\varepsilon_0}$. If .$E$ is constant over (part of) the Gaussian surface, you can pull it outside the integral. This simplification is what allows you to solve for the field. Recall .$Q_\\text{enclosed} = \\frac{\\text{(total charge)}}{\\text{(total area)}}\\times \\text{(area enclosed by Gaussian surface)}$ If you can\u0026rsquo;t pull .$E$ outside the flux integral, then Gauss\u0026rsquo;s law don\u0026rsquo;t work! Use the continuous charge distribution strategy from the prior chapter. Uniformly Charged Solid Spherical Conductor # Charge Outside: .$\\vec E$ will have the same magnitude at all points along the surface .$A_1$ Since .$\\vec E$ is always orthogonal to the surface, the cosine is always .$1$ $$\\Longrightarrow E = \\frac{1}{4\\pi\\varepsilon_0}\\frac{Q}{r^2}$$ We see that the field outside is as if all of the charge was from a single point Charge Inside: .$\\vec E$ will have the same magnitude at all points along the surface .$A_2$ Thus, .$Q = 0$ because the charge inside the surface .$A_2$ is zero Hence, .$E = 0$ for .$r \u0026lt; r_0$ Initial radius is .$r_0$; outside radius is .$r$ Enclosed charge has charge .$Q$ This result is the same for both hollow and solid spheres because all the charge would lie in a thin layer at the surface. If .$Q \\neq 0$, current would flow inside the conductor which would build up charge on the exterior of the conductor. This charge would oppose the field, ultimately (in a few nanoseconds for a metal) canceling the field to zero. Solid Sphere of Charge # Charge .$Q$ is distributed uniformly throughout a nonconducting sphere of radius .$r_0$ Charge Outside: Same rational as before, $$\\oint \\vec E \\cdot d\\vec A = E (4\\pi r^2) = \\frac{Q}{\\varepsilon_0}$$ $$\\Longrightarrow E = \\frac{1}{4\\pi\\varepsilon_0}\\frac{Q}{r^2}$$ Again, the field outside is the same as for a point charge in center of sphere Charge Inside: $$\\oint \\vec E \\cdot d\\vec A = E (4\\pi r^2) = \\frac{Q_{\\text{enclosed in }A_2}}{\\varepsilon_0}$$ Since .$Q_{\\text{enclosed\u0026hellip;}} \\neq Q$, we define the charge density .$\\rho_E$ as the charge per unit volume (.$dQ/dV$) which is constant We can then write $$Q_{\\text{enclosed}} = Q \\cdot \\frac{\\frac{4}{3}\\pi r^3 \\rho_E}{\\frac{4}{3}\\pi r_0^3 \\rho_E} = Q\\cdot \\frac{r^3}{r^3_0}$$ $$ \\Longrightarrow E = \\frac{1}{4\\pi\\varepsilon_0}\\frac{Q}{r_0^3}r$$ $$$$ "},{"id":53,"href":"/physics-7b/23/","title":"23: Electric Potential","section":"Physics 7B","content":" 23.1 Electric Potential Energy and Difference # PE can only be defined for conservative forces That is, work done by said force is independent of the path taken Coulomb\u0026rsquo;s Law is conservative because the dependence on position is conservative Hence, we define .$\\Delta U = -W$ with .$\\Delta U = U_b - U_a$ is for a situation where a point charge .$q$ moves from point .$a$ to point .$b$ This is equal to negative work, .$-W = -\\vec F d = -(q\\vec E) d$ (for a uniform .$\\vec E$) 23.2 Relation between Electric Potential and Field # Electric Potential: Electric PE per unit charge, such as for a charge at point .$a$ $$V_a = \\frac{U_a}{q}$$ We only really care about difference though, which is defined as $$V_{ba} = \\Delta V = \\frac{U_b - U_a}{q} = - \\frac{W_{ba}}{q}$$ We can now also define PE in terms of electric potential: $$\\Delta U = U_b - U_a = q(V_b - V_a) = qV_{ba}$$ Electric potential difference is a measure of how much energy an electric charge can acquire in a given situation. Since energy is the ability to do work, the electric potential difference is also a measure of how much work a given charge can do. The exact amount of energy or work depends both on the potential difference and on the charge. If a positive charge is free, it will tend to move from high to low potential Inverse for opposite charge 23.3 Potential due to Point Charges # $$\\Delta U = U_b - U_a = - \\int_a^b \\vec F \\cdot d \\vec l$$\n.$dl$ is an infinitesimal increment of displacement along the path from .$a$ to .$b$ Keep in mind that .$\\vec F$ must be conservative Thus the integral can be taken along any path from point .$a$ to point .$b$. Knowing .$\\vec E = \\vec F / q$ and .$V_{ba} = (U_b - U_a) / q$, we can write the electric potential equation as\u0026hellip; $$V_{ba} = V_b - V_a = - \\int_a^b \\vec E \\cdot d \\vec l$$ $$V_{ba, \\text{uniform $\\vec E$}} = -E\\int_a^b d\\vec l = -Ed$$ \u0026hellip;where .$d$ is the distance of a straight line from point .$a$ to .$b$ Charged Conducting Sphere # 1. Electric Potential Outside Sphere # We know .$\\vec E = \\frac{1}{4\\pi\\varepsilon_0} \\frac{Q}{r^2}$ for outside a conducting sphere (.$r \u0026gt; r_0$) Therefore, we can write $$V_{ba} = - \\int_{r_a}^{r_b} \\vec E \\cdot d \\vec l = - \\frac{Q}{4\\pi\\varepsilon_0}\\int_{r_a}^{r_b} \\frac{dr}{r^2}$$ $$\\dots = \\frac{Q}{4\\pi\\varepsilon_0} \\bigg(\\frac{1}{r_b} - \\frac{1}{r_a}\\bigg)$$ $$\\dots = \\frac{1}{4\\pi\\varepsilon_0} \\frac{Q}{r} \\text{ [$r_b = \\infty$]}$$ 2. Electric Potential On Sphere # From .$(a)$, as .$r$ approaches .$r_0$, we see $$V = \\frac{1}{4\\pi\\varepsilon_0} \\frac{Q}{r_0}$$ at the surface of the sphere. This makes sense because the charge is distributed on the surface of the sphere. 3. Electric Potential Inside Sphere # Inside the conductor, .$\\vec E = 0$ Therefore, there is no change in .$\\vec E$ from .$0$ to .$r_0$ (or any point within the conductor) gives zero change in .$V$ Hence, within the conductor, .$V$ is a constant: $$V = \\frac{1}{4\\pi\\varepsilon_0} \\frac{Q}{r_0}$$ Thus, the whole conductor, not just its surface, is at this same potential. We can also generalize the first case to the electric potential .$r$ from a single point charge .$Q$ Coulomb potential # The potential outside a uniformly charged sphere is the same as if all the charge were concentrated at its center The potential near a positive charge is large, and it decreases toward zero at very large distances For a negative charge, the potential is negative and increases toward zero at large distances 23.4 Potential due to Any Charge Distribution # If .$\\vec E$ is a function of position (or otherwise unknown), we can find .$V$ by calculating the potential due to the many tiny charges that make up .$\\vec E$: $$V = \\frac{1}{4\\pi\\varepsilon_0} \\int \\frac{dq}{r}$$ where .$r$ is the distance from a tiny element of charge .$dq$ to the point where .$V$ is being determined 23.5 Equipotential Lines and Surfaces # The electric potential can be represented by drawing equipotential lines, or, in three dimensions, equipotential surfaces An equipotential surface has all points at the same potential. That is, the potential difference between any two points on the surface is zero Thus, no work is required to move a charge from one point on the surface to another. Equipotential surfaces are perpendicular to the electric field (field lines) For a positive point charge, the equipotential surface with the largest potential is closest to the positive charge Unlike electric field lines, which start and end on electric charges, equipotential lines/surfaces are always continuous and never end Electric field lines and equipotential surfaces for a point charge. Equipotential lines (green, dashed) are always perpendicular to the electric field lines (solid red) shown here for two equal but oppositely charged particles (an electric dipole). 23.6 Potential Due to Dipole (Moment) # $$V = \\frac{1}{4\\pi\\varepsilon_0} \\frac{Q}{r} + \\frac{1}{4\\pi\\varepsilon_0} \\frac{(-Q)}{(r+\\Delta r)} = \\frac{Q}{4\\pi\\varepsilon_0} \\frac{\\Delta r}{r(r + \\Delta r)}$$\n.$r$ is the distance from (some arbitrary point) .$P$ to the positive charge and .$r + \\Delta r$ is the distance to the negative charge If .$r \\gg l$, then .$r \\gg \\Delta r \\approx l \\cos \\theta$ so we can neglect .$\\Delta r$ $$V = \\frac{1}{4\\pi\\varepsilon_0} \\frac{Ql \\cos\\theta}{r^2} = \\frac{1}{4\\pi\\varepsilon_0} \\frac{p \\cos\\theta}{r^2} $$ Notice the potential decreases .$\\propto r^2$, whereas for a single point charge the potential decreases .$\\propto r$ It is not surprising that the potential should fall off faster for a dipole: When you are far from a dipole, the two equal but opposite charges appear so close together as to tend to neutralize each other 23.7 .$\\vec E$ Determined from .$V$ # We know that .$V_b - V_a = - \\int_a^b \\vec E \\cdot d\\vec l$, which we can write in differential form as .$dV = -\\vec E \\cdot d\\vec l = - E_l dl$. This can be written as $$E_l = - \\frac{dV}{dl}$$ .$dV$ is the tiny difference in potential between two points a distance .$dl$ apart, and .$E_l$ is the component of the electric field in the direction of the tiny displacement .$d\\vec l$ This is called the gradient of .$V$ in a particular direction: The general case is $$\\vec E = - \\nabla \\vec V = - \\bigg\\langle \\frac{\\delta V}{\\delta x}, \\frac{\\delta V}{\\delta y}, \\frac{\\delta V}{\\delta z} \\bigg\\rangle$$ This states that the electric field points \u0026ldquo;downhill\u0026rdquo; towards lower voltages (where there is lower potential) 23.8 Electrostatic PE; The Electron Volt # The electric potential and energy potential due to one point charge .$Q_1$ on another point charge .$Q_2$ separated by .$r_{12}$ are $$V = \\frac{1}{4\\pi\\varepsilon_0} \\frac{Q_1}{r_{12}}$$ $$U = Q_2 V = \\frac{1}{4\\pi\\varepsilon_0} \\frac{Q_1 Q_2}{r_{12}}$$ The PE is the negative work needed to separate the two charges to infinity. For three points, we can use the superposition principle like we have prior to write $$U = \\frac{1}{4\\pi\\varepsilon_0}\\bigg( \\frac{Q_1 Q_2}{r_{12}}+ \\frac{Q_1 Q_3}{r_{13}} + \\frac{Q_2 Q_3}{r_{23}} \\bigg)$$ Electron Volt # Joules are a very large unit for dealing with energy of the electron scale; as such, the electron volt (.$eV$) is often used One electron volt is the energy acquired by a particle carrying a charge .$e$ (the magnitude of an electron) as a result of moving through a potential difference of .$1 V$ $$1 \\text{ eV} = 1.6022 \\cdot 10^{-19} \\text{ J}$$ E.x., an electron (charge .$e = 1.6\\cdot10^{-19}$) that accelerates through a potential difference of .$1000 \\text{ V}$ will lose .$1000 \\text{ eV}$ of potential energy and gain .$1000 \\text{ eV}$ of kinetic energy 23.9 Digital; Binary Numbers; Signal Voltage (not covered) # Batteries and wall sockets provide a steady supply voltage as power Signal voltage provide/carry information Analog signal voltage has voltage that varies continuously (i.e .$\\sin$) Digital signals are more complicated and encode information, often in binary Bytes have 8 bits which allow .$2^8 = 256$ numbers Digital signals are transmitted at some rate (bit-rate) given in .$\\text{Mb/s}$ Analog to digital converters, ADCs, convert analog signals to boxy digital waves The difference between the original continuous and it\u0026rsquo;s digital approximation is called the quantization error / loss This error varies by primarily: Resolution or bit depth which is the number of bits for the voltage of each sample Sampling rate which is the number of times per second the original analog voltage is measured (sampled) E.x., CDs are sampled at .$44.1 \\text{ kHz}$ with a bit depth of .$16 \\text{ bits per sample}$ The red analog sine wave, which is at a 100-Hz frequency (1 wavelength is done in 0.010 s), has been converted to a 2-bit (4 level) digital signal (blue). Digital Signals Digital to Analog, DACs, exist too because some appliances require an analog signal Digital signals can be compressed: Repeated information can be reduced so that less memory (bits) is needed Fun fact: Bit is the contraction of \u0026ldquo;binary digit\u0026rdquo;, leaving out the 8 letters between Digital signals are more resistant from noise, which badly corrupts analog signals Any electronic signal involves electric charges whose electric field can affect charges in another nearby signal External fields, as from high voltage wires, motors, or fluorescent lamps, can produce noise Thermal noise refers to random motion of electrons, much like the “thermal motion” of the molecules in a gas Moving electrons can be affected by the medium (wire, etc.), altering the signal "},{"id":54,"href":"/physics-7b/24/","title":"24: Capacitance, Dielectrics, Electric Energy Storage","section":"Physics 7B","content":" 24.1 Capacitors # Capacitors are devices that store an electric charge Normally consists of two conducting objects; plates, sheets When a voltage is applied, the two plates become charged: one positive, one negative Conductors are placed near one another, but not touching This distance is typically due to an insulator between sheets Capacitors are typically rolled so that they take up less room Two main use cases Storing energy for later use; e.x. camera flash Block surges of charge and energy to protect circuits The amount of charge .$Q$ acquired by each plate is proportional to .$V$: The potential difference of the two plates (Volts) .$C$: The constant capacitance of the capacitor (Coulombs per volt, farad) $$Q = CV$$ 24.2 Determination of Capacitance # In the real world, capacitance is determined experimentally by using the prior equations For ideal cases where the sheets are separated by a vacuum or air, however, we can use the following equations For a parallel-plate capacitor where .$A$ is the area of each plate and .$d$ is the distance between plates: $$E = \\frac{\\sigma}{\\varepsilon_0} = \\frac{Q}{\\varepsilon_0 A}$$ We also know this because .$E = \\sigma / \\varepsilon_0$ and .$\\sigma = Q/A$ Since .$V = \\int E\\ dl = \\frac{Qd}{\\varepsilon_0 A}$, we can relate it to .$C$ as $$C = \\frac{Q}{V} = \\varepsilon_0 \\cdot \\frac{A}{d}$$ Capacitance-finding strategy\nAssign an arbitrary charge .$\\pm q$ to the two plates. Using Gauss\u0026rsquo;s law or other techniques, calculate the electric field between these two plates From that electric field, calculate the potential difference between the plates, .$V = -\\int \\vec E \\cdot d \\vec s$ Calculate the capacitance using .$C = q/V$. The arbitrary charge .$q$ from (1) should cancel out. 24.3 Capacitors in Series and Parallel # Series # The current/charge on each capacitor has the same magnitude: $$Q = Q_1 = Q_2 = \\dots$$ The total voltage across all capacitors is sumo of the voltage drops of the individual components: $$V = V_1 + V_2 + \\dots = I(R_1 + R_2 + \\dots)$$ And since .$V = Q/C$, capacitance is then $$\\frac{Q}{C_\\text{eq}} = \\frac{Q}{C_1} + \\frac{Q}{C_2} + \\dots \\Longrightarrow \\frac{1}{C_\\text{eq}} = \\frac{1}{C_1} + \\frac{1}{C_2} + \\dots $$ Notice that the equivalence capacitance is smaller than the smallest contributing capacitance Parallel # The total current/charge is the sum of the currents flowing through each component $$Q = Q_1 + Q_2 + \\dots = V (R^{-1}_1 + R^{-1}_2 + \\dots)$$ Voltage (potential difference) is the same across all paths/capacitors $$V = V_1 = V_2 = \\dots$$ Therefore, we can use .$V = Q/C$ to write the equivalent capacitance as $$Q = C_1 V + C_2 V + \\dots$$ $$Q = C_\\text{eq} V = (C_1 + \\dots)V \\Longrightarrow C_\\text{eq} = C_1 + \\dots$$ The net effect of connecting capacitors in parallel is to increase the capacitance Makes sense: We\u0026rsquo;re essentially increasing area of the plates The overall working voltage is always limited by the smallest working voltage of an individual capacitor. 24.4 Storage of Electric Energy # The energy stored in a capacitor is equal to the work done to charge it. Initially, an uncharged capacitor requires no work to move the first few bits of charge As more charge is stored, more work is needed to add more charge of the same sign because of the electric repulsion That is, the more charge already on a plate, the more work required to add additional charge Since we know .$dW = V\\ dq$ and .$V = q/C$, we can write the work needed to store charge .$Q$ as $$W = \\int_0^Q V\\ dq = \\frac{1}{C}\\int_0^Q q \\ dq = \\frac{1}{2} \\frac{Q^2}{C}$$ Since .$U = W$ and .$Q = CV$, we can write the energy stored in a capacitor with charges .$+Q$ and .$-Q$ on its two conductors as $$U = \\frac{1}{2} \\frac{Q^2}{C} = \\frac{1}{2}CV^2 = \\frac{1}{2}QV$$ It is useful to think of the energy stored in a capacitor as being stored in the electric field between the plates. E.x. lets find the energy stored in a parallel-plate capacitor in terms of the electric field We know for two close parallel plates we can find the potential difference as .$V = Ed$ where .$d$ is distance between plates We also know .$C = \\varepsilon_0 A/d$ for parallel plate capacitors, thus we can write $$U = \\frac{1}{2}CV^2 = \\frac{1}{2}\\bigg(\\frac{\\varepsilon_0 A}{d}\\bigg)(E^2 d^2) = \\frac{1}{2} \\varepsilon_0 E^2 Ad$$ We can recognize .$Ad$ as the volume between the plates where .$E$ exists If we divide both sides of by this volume, we can an equation for the energy density .$u$: $$u = \\frac{\\text{energy}}{\\text{volume}} = \\frac{1}{2}\\varepsilon_0 E^2$$ Thus, electric energy stored per unit volume in any region of space is proportional to the square of the electric field We proved this with parallel plates, but this can be shown for any region with an electric field 24.5 Dielectrics # Dielectrics are the insulating material sheet placed between conductors They serve to Because they don\u0026rsquo;t break down, they allow electric charge to flow as easily as air so higher voltages can be applied without charge passing across the gap Allow the plates to be placed closer together without touching, allowing an increased capacitance because the thickness .$d$ is smaller Dielectrics increase the capacitance by a factor .$K$ (known as the dielectric constant) $$C = KC_0$$ .$C_0$ is the capacitance when the space is a vacuum/air .$C$ is the capacitance with the dielectric filling the space For parallel-plate capacitors, we use .$C = Q/V = \\varepsilon_0 A/d$ and .$C = KC_0$ $$C = K \\varepsilon_0 \\frac{A}{d}$$ Energy density also changes with a dielectric as $$u = \\frac{1}{2}K \\varepsilon_0 E^2 = \\frac{1}{2}\\varepsilon E^2$$ Likewise, .$E$ and .$V$ are both also altered: With no dielectric, the field is .$E_0 = \\frac{V_0}{d}$ where .$V_0$ is the potential difference If the capacitor is isolated (i.e. not connected to a battery) so that the charge stays constant, potential difference drops: .$V = V_0/K$ Therefore, .$E = \\frac{V}{d} = \\frac{V_0}{Kd} = \\frac{E_0}{K}$ .$\\varepsilon$ is the permittivity of the dielectric material defined as .$\\varepsilon = K \\varepsilon_0$ "},{"id":55,"href":"/physics-7b/25/","title":"25: Electric Current and Resistance","section":"Physics 7B","content":" 25.1 The Electric Battery # Batteries produce electricity by transforming chemical energy into electric energy Simple battery (cells) contain two plates or rods of dissimilar metals called electrodes The portion of rods outside of the solution are called the terminals Anode: The positive electrode Cathode: The negative electrode These electrodes are emersed in the electrolyte: a solution such as a dilute acid Chemical Process:\nThe acid dissolve the zinc electrode, causing zinc atoms to leave two electrons behind on the electrode and enters the solution as a positive ion. The zinc electrode thus acquires a negative charge. Then the electrolyte becomes positively charged and can pull electrons off the carbon electrode. Thus the carbon electrode becomes positively charged. Because there is an opposite charge on the two electrodes, there is a potential difference between the two terminals. When a battery isn\u0026rsquo;t connected, only a small amount of zinc is dissolved The zinc electrode becomes increasingly negative Thus, any new positive zinc ions produced are attracted back to the electrode That is, if a charge is allowed to flow then the zinc can dissolve The voltage depends ot the electrodes\u0026rsquo; material and their relative ability to give up electrons 25.2 Electric Current # When a circuit is formed, charge can move (flow) through the wires from one terminal to the other Any flow of charge is called an electric current Flow can only occur on a continuos conducting path (a complete circuit) If there\u0026rsquo;s any break, our circuit is called an open circuit and no current flows The symbol for battery is the following: Conventional current from .$+$ to .$-$ is equivalent to a negative electron flow from .$-$ to .$+$ Current in a wire is defined as the net amount of charge that passes through the wire\u0026rsquo;s full cross section at any point in time: $$\\bar I = \\frac{\\Delta Q}{\\Delta t} \\Longrightarrow I = \\frac{dQ}{dt}$$ Current is measured in coulombs per second; ampere (amp): .$\\text{1 A = 1 C/s}$ 25.3 Ohm\u0026rsquo;s Law: Resistance and Resistors # For a current to exist, there must be a potential difference (e.g. between the terminals of a battery) That is, the current is proportional to the potential difference: $$I \\propto V$$ E.x., a wire connected to a .$6V$ battery results in a current twice that of a .$3V$ battery The current depends on the resistance that the wires offers The electron flow is impeded partly due to the atoms in the wire .$R$ is this proportionality factor between voltage and current Thus, we get Ohm\u0026rsquo;s Law: $$V = IR$$ Ohm\u0026rsquo;s law only works for when .$R$ is a constant, i.e a metal conductor In reality, .$R$ isn\u0026rsquo;t constant if temperature changes much Materials that follow Ohm\u0026rsquo;s law are labeled as \u0026ldquo;ohmic\u0026rdquo; Resistance has the units/notation .$\\text{1 $\\Omega$ = 1 V/A}$ Resistors are used to limit/control the current in a circuit toolbox.mehvix.com/resistor As a current passes through a resistor, the charge/current stays the same but the electric potential decreases Clarifications of Behavior\nCurrent\u0026rsquo;s magnitude depends on that device\u0026rsquo;s resistance Can be though of as the \u0026ldquo;response\u0026rdquo; to the voltage: increases if voltage increases or resistance decreases Current is constant \u0026ndash; it\u0026rsquo;s energy so it cannot be destroyed by components and it\u0026rsquo;s not created by a battery Resistance is a property of the device/wire Voltage is external to the wire of device \u0026ndash; it\u0026rsquo;s applied across the two ends of the wire Batteries maintain a constant potential difference \u0026ndash; act as a source of voltage 25.4 Resistivity # Resistivity has experimentally been found as $$R = \\rho \\frac{l}{A}$$ .$\\rho$ is the resistivity (constant of proportionality) and depends on the material Has units .$\\Omega \\cdot \\text{m = V/A $\\cdot $ m}$ .$l$ is the wire length .$A$ is the cross-section area The reciprocal of resistivity is conductivity: .$\\sigma = \\rho^{-1}$ Temperature # Resistivity varies (generally increasing) with temperature $$\\rho_T = \\rho_0 = \\bigg[ 1+ \\alpha (T-T_0)\\bigg]$$ .$\\rho_0$ is the resistivity at some reference temperature .$T_0$ (i.e .$0^\\circ \\text{ C}$) .$\\rho_T$ is the new resistivity at the current (higher) temperature .$T$ .$\\alpha$ is the temperature coefficient of resistivity that depends on material Note that the temperature coefficient for semiconductors can be negative. At higher temperatures, some of the electrons that are normally not free in a semiconductor can become free and contribute to the current. Thus, the resistance of a semiconductor can decrease with an increase in temperature. 25.5 Electric Power # Electric energy is transformed into thermal energy (and light) in stove burners, toasters, etc. The current creates collisions between the moving electrons and the atoms in the wire That is, the KE from the wire\u0026rsquo;s atoms increases meaning the temperature increases too $$P = \\frac{dU}{dt} = \\frac{dq}{dt}\\cdot V$$ This is because energy is transformed when a tiny charge .$dq$ moves through a potential difference .$V$ is .$dU = V\\ dq$ The charge that flows per second, .$dq/dt$, is the electric current .$I$: $$P = IV = I^2 R = \\frac{V^2}{R}$$ The SI unit for power is the watt: .$\\text{1 W = 1 J/s}$ We get the last two equations by plugging in .$V = IR$ 25.7 Alternating Current # When a battery is connected to a circuit, the current moves steadily in one direction (DC: Direct Current) Electric generators at power plants produce AC: alternating current Reverses direction many times per second and is commonly sinusoidal $$V = V_0 \\sin(2\\pi ft) = V_0 \\sin(\\omega t)$$ .$\\omega$ = .$2\\pi f$ .$f$ is the frequency: number of complete oscillations per second Commonly .$\\text{60 Hz}$ in NA Potential .$V$ oscillates between .$\\pm V_0$, the peak voltage Current equation still works: $$I = \\frac{V}{R} = \\frac{V_0}{R}\\sin\\omega t = I_0 \\sin\\omega t$$ .$I_0 = V_0/R$ is the peak current Avg current is 0; it\u0026rsquo;s positive and negative for an equal amount of time Doesn\u0026rsquo;t mean that no heat is created or no power is needed Electrons are still moving though! Power is also consistent $$P = I^2R = I_0^2 R \\sin\\omega t = \\frac{V_0^2}{R} \\sin\\omega t$$ Power is always positive because current is squared Since the .$\\sin\\dots$ oscillates between 1 and 0, the average power is $$\\overline P = \\frac{1}{2}I_0^2R = \\frac{1}{2} \\frac{V_0^2}{R}$$ This can also be calculated by using the RMS values for .$I$ and .$V$ $$I_\\text{rms} = \\sqrt{\\overline I^2} = \\frac{I_0}{\\sqrt{2}} \\approx 0.707 I_0$$ $$V_\\text{rms} = \\sqrt{\\overline V^2} = \\frac{V_0}{\\sqrt{2}} \\approx 0.707 V_0$$ $$\\dots \\Longrightarrow \\overline P = I_\\text{rms} V_\\text{rms} = I_\\text{rms}^2 R = \\frac{V_\\text{rms}^2}{R}$$ Fun fact: we can use the rms of a value to find the peak of it, e.x. $$V_0 = \\sqrt{2} V_\\text{rms}$$ Keep in mind that this is the average power. Instantaneous power varies from .$0$ to .$2\\overline P$ 25.8 Microscopic View of Current # We\u0026rsquo;ve seen that electric current can be carried by negatively charged electrons in metal wires, and that in liquid solutions current can also be carried by positive and/or negatively charged ions When a potential difference is applied to the two ends of a wire, the direction of the electric field .$\\vec E$ is parallel to the walls of the wire This field within the conducting wire does not contradict our earlier result that .$\\vec E = 0$ inside a conductor in the electrostatic case, as we are no longer dealing with the static case. That is, charges are free to move in a conductor, and hence can move under the action of the electric field. If all the charges are at rest, then .$\\vec E = 0$ Current Density # Current density, .$\\vec j$, is the current per area $$j = \\frac{I}{A} \\Longrightarrow I = \\int \\vec j \\cdot d \\vec A$$ .$I$ is the current through the whole surface .$d\\vec A$ is an element of surface area over which the integration is taken Direction of the density is the same direction as .$\\vec E$ \u0026ndash; the direction that a positive charge would move Drift Speed # Inside a wire, we can imagine the free electrons as moving about randomly at high speeds, bouncing off the metal atoms of the wire Somewhat like the molecules of a gas When an electric field exists in the wire the electrons feel a force and initially begin to accelerate but they soon reach a more or less steady average speed, known as their drift speed .$v_d$ Collisions with atoms in the wire keep them from accelerating further The drift speed is normally very much smaller than the electrons\u0026rsquo; average random speed inside the metal wire Black zagged line represents the motion of an electron in a metal wire due to an electric field. The field .$\\vec E$ gives electrons in random motion a net drift velocity .$\\vec v_d$. Its direction (the net charge flow) is in the opposite direction of .$\\vec E$ because electrons have a negative charge and .$\\vec F = q \\vec E$ We can relate drift speed with the macroscopic view: In some time, the electrons travel (the average) distance .$l = v_d \\Delta t$ In that same time, electrons in volume .$V = Al = A v_d \\Delta t$ pass through area .$A$ of the wire If there are .$n$ free electrons each of charge .$-e$ per unit volume, then the total electrons is .$N = nV$ Thus, the charge is $$\\Delta Q = \\text{(number of charges, $N$)$\\times$(charge per particle, $-e$)}$$ $$\\dots = (nV)(-e) = -(nAv_d\\Delta T)(e)$$ We can then easily find the current (density): $$I = \\frac{\\Delta Q}{\\Delta t} = -neAv_d$$ $$j = \\frac{I}{A} = -nev_d$$ Notice that the negative sign indicates that the direction of (positive) current flow is opposite to the drift speed of electrons. Field inside a Wire # Voltage can be written in terms of microscopic values (in addition to the macro: .$V = IR$) Recall that resistance is related to density by .$R = \\rho \\frac{l}{A}$ We can then write .$V$, .$I$ and .$j$ as $$V = El = IR = (jA)\\bigg(\\rho \\frac{l}{A}\\bigg) = j \\rho l$$ $$I = jA$$ $$j = \\frac{1}{\\rho}E = \\sigma E$$ .$\\sigma$ is the conductivity of the wire .$\\rho, \\sigma$ do not vary with .$V$ and thus neither .$E$ We can then write the microscopic statement of Ohm\u0026rsquo;s Law: $$\\vec j = \\sigma \\vec E = \\frac{\\vec E}{\\rho}$$ 25.9 Superconductivity # At very low temperatures, the resistivity of certain metals and certain compounds or alloys becomes zero Materials in such a state are said to be superconducting In general, superconductors become superconducting only below a certain transition (critical) temperature, .$T_C$ "},{"id":56,"href":"/anthro-c12ac/","title":"Anthro C12AC","section":"Docs","content":" Anthropocene # The time when human activity began to have an influence on (global) landscape due to our use of fire Large subject used in a range of fields \u0026ndash; no single definition Defining feature: combustion of carbon and greenhouse gases Ice core measurement technique As ice forms, methane and CO2 get trapped along with ash/dust/pollen which scientists can measure by coring Ice can be dated so we can compare these variables so we can see change in greenhouse gases over time Beginning is disputed Industrial revolution (1780s) Most popular among scholars Atomic Testing (1940s) The isotopic by-products of bomb testing provide a distinctive marker horizon in ice cores, ocean and lake sediments, and soils Stages idea: Includes vital events such as forest cutting and grassland conversion: the two largest spatial transformations of Earth\u0026rsquo;s surface in human history 1.8 million years ago: When fire was discovered 6000-4000s years ago: With neolithic agriculture 1780s: Industrial revolution Identifying fire requirements: Evidence of temporal or spatial changes in fire activity and vegetation Demonstration that these changes are not predicted by climate parameters alone Temporal/spatial coincidence between fire regime changes and changes in the human record Pyrogeography # History of the variation of fire activity over space and time at the landscape scale in different regions of the world Pyrogeography started in Silurian period when plant life began Fire requirements: 13% Oxygen in a normal environment and 30% Oxygen in damp vegetation. Fires are a selection force in the evolution of plants Four phases for Pyrogeography # 1. Natural Biospheric Fire \u0026ndash; Natural Fire Regime # (Potential) start date for pyrogeography During Silurian and Devonian Periods (440-400 HYA) Natural fire regime started during this period because it was the first time that the fire triangle came together Fire triangle: Ignition Source: (Since beginning) Natural ignition from lightening (most common), volcanoes, (rarely) falling rock sparks These natural sources tend to only begin fires in the dry season Lightening is most common in mountain regions (over a costal region) Oxygen Source: (Cambrian period) Atmospheric Oxygen (from photosynthetic plants) leads to appearance of photosynthetic organisms Fuel Source: (Silurian and Devonian Periods) Enough terrestrial plants in ecosystems to acts as fuel During this time, natural fire regimes evolved As coal became more common (Carboniferous period), fires did too Started long time ago, before humans and dinosaurs 2. Wildland Anthropogenic Fire \u0026ndash; Hunter/Gatherer Fire Regime # When people began acting as the ignition source Primarily used fire for domestic cases Heating, cooking, warmth, etc. When people move to a new land Major changes in fauna, vegetation, and fires (charcoal) People seem to bring fire with them as they migrate Start dates 40 ka for Australia 90% of fauna went extinct Lots of evidence of fire 45 ka for Highland New Guinea 50 ka for lowland Borneo 20 ka for the Americas Extinctions of many animals and vegetation Debate: are these because of natural process like climate change (ice age -\u0026gt; post ice age?) or do people play a large role Native Perspective \u0026ndash; Indigenous people have been here since time began When did humans actually discover fire? # Defining the bridge between phase one and two is difficult Definition problems Do we ask when did (modern humans / hominin ancestors) develop the ability to control and utilize fire? We also need to distinguish between (1) controlling / utilizing fire and (2) being able to start a fire on a whim Archeological problems Fire exists naturally, so we can\u0026rsquo;t assume all fire evidence is from human action Other natural processes can look like fire (e.g. staining by minerals in soil, oxidation causing reddish patches) Combustion of natural objects (e.g. bushes) can leave charcoal which looks like a human hearth Additionally, evidence of a hearth doesn\u0026rsquo;t mean that humans started/controlled fire Provides a single snapshot, has little temporal depth Archaeological Record Analysis In the field: Observation and collection of materials Study the geology of the site In the lab: Microscopic analysis to see if there was burning If so, could the location of the sample been transported after combustion? Further, how does the history of the burned object associate with cultural items How long has fire been controlled? # Europe: Strong evidence of 400,000 - 300,000 years ago Western Asia: One established case from 780,000 years ago. Other sites are similar to Europe Africa: Claims have been made for a cave site that shows fire around 1.5-1.6 million years ago Note that opportunistic use of fire could have happened much earlier Eg. lighting a torch from a natural-starting forest fire 3. Agriculture Anthropogenic Fire \u0026ndash; Agricultural Fire Regime # Required fire to alter the natural vegetation from perennial-dominated to annual-dominated landscapes. People preferred to live in fire-prone places because the burning provides advantages for hunting, foraging, cultivating, and livestock herding 4. Industrial and Domestic Anthropogenic Fire \u0026ndash; Industrial Fire Regime # Low-severity surface fire regimes are being replaced with low-frequency, high-intensity crown fires that are outside the historical range of variability for these ecosystems Western US: forests have also experienced an increase in hazardous fuels due to highly effective fire suppression policy that excluded fires for much of the 20th century Eastern US: Fire suppression has shifted oak and pine woodlands to mesophytic hardwoods consequently reducing flammability and fire activity Globally: urban areas have steadily expanded into wildland areas Producing more ignition sources (arson and accidental) Exposing more people to wildfire Key Factors in Fire Regimes (Discussion 8-30) # Frequency The interval of fire occurrences E.x. every four years Area Size, distribution, location Ground fires (primarily dead plants) vs crown (burning upper canopy of living trees) Crown fires areas are more difficult to manage because controlled burns still damage natural resources Severity How destructive a fire is (high mortality = high severity) Change in dominant species / change in ecosystem Quantifiable by how much soil on the ground is visible Seasonality How the season affects fires May or may not be annual May arise due to weather conditions Ex. Annual to decadal cycles of drying conditions Interactions General activity on the landscape leading to different fire outcomes Droughts leading to stress on fuels Beatles eating bark, making trees more vulnerable Fire suppression leading to less severe fires Climate change making fires more severe Biodiversity and Fire as a Selective Variable in Evolution # C4 grass Spread during seasonal climate in the tertiary period (when fires became more common) Fires lead to woodlands and created environments favorable to C4 grasslands Since C4 is high flammability, it would have produced a feedback process that further increased fire activity, Thus maintaining the grassland-dominated landscape This process is similar to the one currently maintaining many of our savannas Plant attributes Heat shock Certain species have seeds that will open with heat Not exclusive to fire; correlated with soil heat too Smoke Highly selective and specific to fire Smoke is a mixture of specific chemicals unique to itself Note that plants become resistant to certain fire regimes, not necessarily all Changes in fire regime can kill off fire resistant plants Even small differences in the deployment of fire outside of natural lightning strikes can alter patterns of forest succession, fuel availability, and seasonality of ignitions Fires Relating to Evolution # Beneficial Attributes Cooking hypothesis Key claim: Fire + cooking started with the Homo erectus. As such, humans have evolved around a cooked diet that they can\u0026rsquo;t live without Led to fitness advantage More energy + nutrient from food, enabling body and brain size increase Detoxing effect Increase digestibility of all food Cooking takes time, leading to social development Distribution of tasks among group: (collection, preparation, even stealing) Cooking accounts for reduction in jaw, tooth size (due to softer food), stomach, and digestive system size There is no evidence of modern human societies existing without cooked food Counterpoints: It\u0026rsquo;s still unclear that Homo erectus controlled fire There are some sites that show no example of cooking: e.g. Neanderthal sites in cold climates Energetic effects aren\u0026rsquo;t well quantified Digestive evolution may not have been linear; other adaptations related to fire occurred after Homo erectus Protection benefits at night (especially versus the alternative: sleeping in trees) Allowed much better vision in caves Enables cave art Evidence that some hominids could use fire to morph certain woods into tools (e.g. digging sticks, hafted spears) Allowed humans to colonize colder environments Increase prey abundances, maintain mosaic landscapes, and increase pyrodiversity and succession stage heterogeneity Social bonding Led to camp fires Allows people to stay up later Fire could be used as a story telling enhancer, means to pass on history, culture, etc. Provides a sense of intimacy and openness Opportunity for music Fire-stick farming clearing ground for human habitats facilitating travel killing vermin, hunting regenerating plant food sources for both humans and livestock warfare among tribes Woody, closed-canopy shrublands were opened up or entirely displaced Led to spread of fast-growing annual species that provided greater seed resources, travel, and hunting and planting opportunities Ex. CA land was only used for agriculture after burnings, which led to many other alien plants spreading too Reductions in arboreal cover and woody understory have the most potential to enhance erosion Reshaping of landscapes has posed problems for ecologists trying to understand contemporary landscape patterns Overview of Fires in California # Conflagrating California\n~54% of CA ecosystems depend on fire The remaining ecosystems that aren\u0026rsquo;t too extreme for fires (so not deserts, stony summits, wetlands, etc.) are fire adapted \u0026ldquo;Fire season is 13 months\u0026rdquo;\nCA has a diverse ecosystem; each site is similar to another one elsewhere though What sets CA apart is the scale and intensity of it\u0026rsquo;s fires CA\u0026rsquo;s fires are unique in that they lead national discourse Texas views the US as France views the EUnion \u0026ndash; a canvas to project it\u0026rsquo;s ideals Alaska view the US as a source of subsidies. It\u0026rsquo;s isolated and treated almost as a commonwealth. California shares sizes, political isolation, and sense of selfhood with the aforementioned. Unlike AL, it has a strong and wide economy Unlike TX, it has been independent but not secessionist and, while CA and TX both have strong cultures, CA doesn\u0026rsquo;t project its Importance - 1/9 Americans live in CA - 8th largest economy - Social uses 1/2 the national fire budget - Source (and testing ground) of new firefighting technology \u0026ldquo;CA is like the rest of the US, just more so\u0026rdquo;\nTwo CA North and south Sierra and Seacoast Rich and poor Scrubland, megalopolis, \u0026amp; wilderness Lowest and highest elevation in nation (Mount Whitney @ 14,500 ft and Death Valley @ -280 ft) Sierra Nevada is analogous to NoCal Big tilt Highest in SE and slants lower to N + W Mostly timber More frequent fires Transverse Range is analogous to SoCal Big Kink Highest in E, bends sharp W then trails to pacific and N Mostly bush More intense fires 56% of ppl live on 8% of land CA is a national innovator Created geological survey in 1860 Created Board of Forestry in 1885 Set standard for fire control Light-burning controversy Pro-burn: Frontier practices (the Indian way of forest management) Advocated for regular burning montane woods and lowlands NoCal Pro-protection Use govt to prevent\\fight fires SoCal National issue debated in CA 1923: Light-burning anathematized Leopold report, Wilderness Act, and Tall Timbers fire ecology conferences Began in 1962 Aimed to transform fire control into fire management Good for conservation + private land owners who wanted traditional working landscapes Loggers and ranchers started moving out at this time Didn\u0026rsquo;t target urban areas Urban Areas Used fire suppression CA to spawn as AK to wilderness\nFires in SoCal rained embers onto cities SoCal retaliated w larger firefighting force New Practices Operation Firestop: Transform tech into operational programs 1956: Aerial tankers used to drop retardant on firelines 1961: Specialty fire crews expanded nationally 1963: Forest Service opened Western Fire Lab to coordinate fire suppression w air attacks 1970: Organized SoCal fire agencies towards common practices Area Specialty Florida: Prescribed fire N Rockies: Management over back countries (So)Cal: Fire suppression Land management in Cali was synonymous to fire management, thus fire suppression As CA grew, the motivation for fire suppression was primarily economical This lead to divisive debates of suppression vs let-burn When there were flames, it was fight or flight Many fire ideas spread from CA Often transformed and simplified Fires are especially susceptible due to Scioecological Systems (SES) History of fires suppression Climate change More extreme fire weather Expanding development Droughts Difficult to predict future fire regimes well Changes in human behavior can amplify, but tend to cancel out climatic effects on fire regimes For example, humans alter through changing land use, ignitions, fuel conditions, or fire suppression Fire activity is influenced by climate variability More (less) fire occurred in dry (wet) and warm (cool) years, and high-fire years were preceded by moist and sometimes cool conditions 1\u0026ndash;4 y earlier. (Sierra Nevada) fire-regime shifts correlate to SES changes, not shifts in climate. Are best bet is to look at history and see how fire regimes changed from past changes in socioeconomic variables Climate increased fire activity on a large-scale after Native American depopulation reduced the buffering effect of due to their burnings Sierra Nevada tribes were hunter-gatherers who used sophisticated burning practices to manage resources The fire index nearly doubled after depopulation Later Euro-American settlement and fire suppression buffered fire activity from temperature increases Logging, fire suppression, livestock increase (grazing effects) The overall sensitivity of fire regimes to low-frequency temperature variation is related to temperature-driven vegetation changes that alter fuel structure and fuel type Peopling of North America # Traditional Perspective \u0026ndash; Clovis First Model 13,000 ka, there was an ice-free corridor that opened up and let people come to north America People from Asia follow herds of megafauna across Bering Strait ( Beringia) We have found kill sites across America Butchered mammoth, horses, bison, ground sloths, etc. New Thoughts about Peopling of Americas: Multiple Migrations of People 20,000-40,000 years ago Coastal Migration Model: Use Boats to Come to Americas from Asia Some travels may not have been successful which is why there is less evidence for this theory Based on: Evidence sophisticated cultures Art, pottery, (primitive) technology, etc. Evidence of maritime seafaring at early date (Australia, New Guinea, Japan) People follow the Kelp Highway Maritime Kelp forests very productive (food source) Kelp Highway went along Pacific Rim Peopling of California # Earliest well dated sites in CA (13,000-10,000 BP) [Based on radio carbon dates]\nSites located in SoCal: Channel Islands, South Coast These Islands have always been separated by water so we know people had boats relatively early Find Evidence of Shell Middens\nContain shell, fish, other maritime foods Have evidence of tools for kelp + technology Big Game Kill Sites rare in California Dietary Differences (Midwest/Plains vs California) Perhaps because of dietary differences Suggests different groups of people, so potentially different groups of migrants Major Changes observed on Channel Islands\nChanges in Fauna (Pygmy Mammoths) Changes in Flora Evidence of Fires! Debate about what caused these changes\nDue to\u0026hellip; Climate Change Climate change at end of Ice Age Mega herbivores died after ice age, increasing fuel sources (vegetation) which we can see with charcoal signatures Causes warming + drying (leading to fire) Changes in vegetation and animals because they couldn\u0026rsquo;t adapt Comet Newest theory Estimated that 5km comet hit earth somewhere We\u0026rsquo;ve found comet-diamonds and various sites that have chemical signatures potentially from a comet Cloud from comet would have affected photo-synthetic processes, killing plants + animals People People may have over-hunted animals May have brought fire with them Potentially due to multiple reasons Kent thinks it\u0026rsquo;s likely climate change + people Concluding Points\nClearly people knew about fire from earliest times Very Sophisticated Maritime Peoples (Kent\u0026rsquo;s opinion) Early for early anthropogenic Influenced fire regimes from earliest times Implications: the Holocene Epoch in CA (last 10,000 years); you cannot assume That only Natural Fire Regimes existed Must consider the influence of people Historical Fire Records in California # Methods # Key Question: What methods can we use to get historical information on fires? 1. Coring Lakes # Annual layers are laid down in some lakes Realistically you can date to around fifty years of accuracy This samples have deeper temporal depth vs other methods We can also radiocarbon date these samples to determine the time period What to sample Charcoal present is indicates fire Larger particle tend to travel small distances Smaller particles can travel much longer distances Some plants may be easier/harder to identify than others Pollen analysis can give you knowledge on what (wind pollinated) plants were common at the time in that area Provides broad perspective of vegetation over time Pollen can travel far, so resolution isn\u0026rsquo;t great Phytoliths Tiny particles formed in many plants They don\u0026rsquo;t break down \u0026ndash; can stay in soil for hundreds to thousands of years! Can be used to identify plant species Cons: Not all plants produce them; overrepresented in grasses 2. Coring Trees # You basically jab a metal straw into a tree and get a sample of the tree Doesn\u0026rsquo;t harm tree, tree naturally patches the hole itself These sample contain rings Give us age and growth rate of the tree Growth increase in rings can allude to neighboring trees being killed Dead neighboring trees means less canopy blocking sunshine and less trees competing for nutrients Multiple samples can give us an overview of a landscape Do we see a multi-age forest? Do we see synchrony in growth rate? Crossdating: To identify events, we compare the sample\u0026rsquo;s tree rings to those of other \u0026ldquo;regular\u0026rdquo; trees in different areas at the same time Note that the example above is a rare occurrence 3. Fire Scars # Fire scars occur when fire kills part of the cambium below bark, leaving a wound The scar itself takes ~10 years to show up Multiple dated wounds allude to multiple fire occurrences You can also get the seasonality too Spring trees have warmer water so the scars are lighter in color Fall/winter trees have slower cell movement, so the scars are darker (this is called \u0026ldquo;latewood\u0026rdquo;) Therefore, how deep the scar is in the wood corresponds to the season the fire occurred Use crossdating for high accuracy and precision Pines, White oaks, Sequoia, Redwood, Incense, and Cedars are all good trees to take samples from Wedges: You can take wedges from the tree to get access to the ring view Both living and dead wood samples can be dated Pines, redwood, cedar, giant sequoia are all rot resistant thus good species to sample You ideally want to cut a thin wedge that has a large surface area and includes center of tree Downsides Trees heal covering up scars or scars are in an exposed cavity that can be seen Problem when fire interval \u0026lt; 10 years (such as cultural burnings) Most of the time fires aren\u0026rsquo;t severe enough to scare deeply enough and if they do scar, it\u0026rsquo;s feint Only 5% of trees sampled scarred in Sierra Nevada Fire scars have a finite lifespan (tree lifespan) Paints picture only of a certain plant in the landscape Fire History Study Sierra Nevada # Two large sampling areas, one north and south Sierra Nevada Systematic fire history sample in mixed conifer forests Important confluence of at least 3 Tribes: Sierra Miwok, Yokut, and Western Mono. Burned extensively for multiple objectives This periodic burning limited severity of fires Fire area did not exceed 1500 ha in any year. Approximately 50% of area This is tiny compared to the scale of other fires in California! Area burned in California # Before 1800 Lightning fire Indian burning Burned most of grasslands, wetlands, oak woodlands, some forests 4.5 million acres/yr. burned Costal California Today # Large area: scotia to Morro bay Very diverse vegetation type From mixed conifer to costal prairie Fire regimes depend on vegetation, thus they\u0026rsquo;re diverse Likewise, some regions don\u0026rsquo;t consider burning at all and only do suppression whereas others use burnings frequently General ignition sources At higher mountains, lighting ignition occurs (still rare, however) At lower elevations / costal areas, human ignition is the main sources Looking at history, we see that Indian fires dominated for thousands of years Sudden oak death (SOD) in Tanoak Forest Invasive pathogen in Marin Country Dead trees increase the severity of fires Prescribed fire periods Done in fall if not drying to damage trees Trees are in dormant phase, less damaging Done in spring if trying to control/limit Wet spring conditions are easier to manage Location Types # Costal Prairie Interval: Short Frequent fires critical to killing shrubs/trees Therefore, due to human intervention douglas-fir and shrublands have began to take over prairies Source: Indian Type: high \u0026ndash; removes overstory of grass Size: Small to moderate Coast Redwood Severity: low Have thick, adapted bark Interval: Short up to 1880\u0026rsquo;s Size: Small to moderate Source: Indian burnings Can re-sprout after fires Redwoods are very fire and rot resistant so many contain information about fires Difficult to date because the asynchronous ring structure California Annual Grasslands Interval: Short Interval needs to be short so that shrubs don\u0026rsquo;t come in Practicing fire suppression/exclusion results in quick spreading, non-fire resistant plants like douglas-firs taking over Severity: high \u0026ndash; removes overstory of grass Size: Moderate to Large Non-native plants dominate today, very different Human intervention lead to overgrazing and drought which enhanced ability for mediterranean plants to prosper It\u0026rsquo;s hard to go back and restore to native ecosystem state Coastal Scrub \u0026ndash; Coyote Bush (Oakland hills) Interval: Moderate Severity: High Size: Moderate Source: Indian burnings for diverse objectives Oak Woodlands, Mixed Oak Woodland Severity: Low Have thick, adapted bark Frequency: High Size: Small Naturally dense Very dense nowadays due to lack of Indian burnings Dense locations are less productive than managed and open areas Mosaic of vegetation patches created that limit fire spread Chaparral Interval: Low-Moderate Frequent burnings can lead to (invasive) grassland conversion (especially in SoCal) Many other \u0026ldquo;fire follower\u0026rdquo; species begin growing after fires too Severity: High Very volatile even in spring (non-dry conditions) Size: Moderate to high Stand replacing regime 30-75 yr. interval Crown fire adapted High intensity burns Climate driven Droughts Low fuel moistures Foehn winds \u0026ndash; Winds from east that are generally dry and warm Fire scars \u0026ndash; not common Knobcone Pine Severity: High - High severity required to activate seeds Interval: Moderate to long Size: Moderate to large Overstory tends to burn completely Douglas-fir in North Coast Severity: Moderate Not adapted for fire (have thin bark) Interval: Low to moderate Size: Moderate Changed from fire exclusion, harvesting, and fire suppression Native Californians # Long-History of Human Occupation in CA Archaeological Evidence: 13,000 years or more Evidence for multiple immigrations from both sea and land to California Diverse composition of people Native communities are still in California 110+ recognized tribes today in CA We can learn from them now about how they treated fire over history 80-100 languages spoken between all tribes Evidence for multiple immigration waves The concentration of native communities is most dense North of Mexico Packed Landscapes: Many Tribes (Tribelets) or Small Nations 100-1000 people make up polities Mostly small Tribal territories Crowded Landscapes Complex Societies Village Communities have Elaborate Ritual and Political Organizations Different people would specialize in different areas Food Storage (Granaries) Sophisticated Material Culture: Baskets, Shell Beads, etc. Non-Agrarian People No formal agriculture practices outside SE CA Sustainer primarily by hunting/gathering (use of wild plants and gatherers) Adjacent Areas \u0026ndash; neighboring people would grow corn, beans, squash but Indians chose not to practice agriculture Perception about Non-Agrarian people Changed over time (Falsely) Seen initially as passive foragers that minimal impact to environment Indigenous Stewards of Land and Sea # Better way of describing CA Indians (compared to hunter gatherers) Indians worked/work as Active Agents to Augment Environmental Productivity and Diversity Seascape Stewardship: Various ways Native people enhanced the productivity and sustainability of shellfish populations and fisheries Landscape Stewardship: Anthropological Rethinking back to 1940s, picks up steam in 1970s, 1980s that culminates with Anderson 2005 publication Various Methods Employed in Landscape Management Practices: Transplanting Water Diversion Pruning/Coppicing Weeding/Tillage Sowing/Broadcasting Seeds All for the purpose of enhancing productivity of natural plants/animals Most Important: Anthropogenic or Cultural Burning was the key method of stewardship Long History in California (Channel Islands)? Mediterranean Climate is Fire Enabler Wet cool winters lead to high plant production Dry and drought-ful summers provide a dry, extensive fuel base Native Californians realized that fire was a natural occurrence and learned to live with fire Reasoning for Cultural Burnings (Landscape scale) Fires Control Insects/Pests Remove Detritus, clean-up landscape Allows light through which leads to healthy growth Open Pathways Use Fire to Hunt Game (driving animals into traps/valleys) and Insects Produce Straight Stems for cordage, baskets Augment Growth/Diversity of Plants and Animals in Territory Grasslands and other plants may have deeper roots which make them more difficult to kill Increases productivity of nutrient rich plants that animals eat Deer heard sizes are correlated with burnings Stimulate Growth of broad spectrum of economic resources Through burning in Patchy Mosaics How Areas Were Burned Instigate fire regimes with frequent, small, low-intensity surface burns Seasonality of burning very important to minimize risk of catastrophic fires Additionally, risk reduced by reduce fuel loads + creating fuel breaks Burn to increase productivity and stimulate growth of broad spectrum of economic resources Notably shrubs with berries, oak woodlands, coniferous Intentionally create Patchy Mosaics Increase quantity, diversity and sustainability of key plants and animals Different environments benefit from different style burnings resulting in checkerboard-esque patterns Used for foods, medicines, and raw materials Minimize risk of catastrophic fires due to reduced fuel loads and fire breaks Burn areas contained by natural rivers/hills/ridges/basalt flows Implications of Cultural Burning Not Pristine Wilderness, but Managed Anthropogenic Landscape \u0026ndash; you have to take into account Indians when looking at CA\u0026rsquo;s landscape You can\u0026rsquo;t do restoration without bringing in people Now, because of a lack of recent cultural burnings, some habitats are struggling Controversy regarding cultural burnings # Some argue that cultural fires were small in size, near villages, and had little impact on the larger ecosystem Rather, natural fires best explain these fires Evidence Types tend to be soft; Tribal Oral Traditions have a Short Temporal Depth for cultural burnings Observations limited primarily to last 250 years Limitations arise when primarily referring to Ethnohistoric Sources Ethnohistoric Studies are from (typically Spaniard around 1600-1700) explorers and not trained as anthropologists Accounts tend to be spotty, geographically biased, and not very detailed Sources don\u0026rsquo;t include key fire regime aspects (freq, seasonality, severity, area, synergy) Susceptible to biases, uncertainty, and anecdotes Ethnographic Studies Information is word of mouth history from within the tribe that\u0026rsquo;s passed from one generation to another Recorded by anthropologists Information is very brief/general and lacking in our definition of fire regime now Old archeologists may have viewed Indians as hunter/gatherers (which is wrong!) Colonizers prevented cultural fires (fire exclusion) Fire exclusion is when fire is indirectly removed from the environment E.x. adding livestock removes fuels, reducing/removing fire E.x. Colonizers removing Natives (for reasons other than their cultural burnings) removed fires, changing fire regime Whereas fire suppression is mindful + target specifically at stopping fires E.x. Using firetrucks, airplanes, etc. to stop a fire from spreading E.x laws preventing you from creating fires during the dry season Thus, when we started Ethnographic studies in 1901(ish), we didn\u0026rsquo;t see the history before because current tribes weren\u0026rsquo;t allowed to perform cultural burnings Instead, critics would like to see hard ecological data (fire scars, pollen analysis, charcoal accumulation, phytoliths) Two main reasons to address these criticisms Differential impacts due to colonialism in different spaces and at different times Some areas colonized much earlier than others in the state Central and Souther coast colonized 2.5 centuries ago by Spaniards Others only were colonized during the gold rush) Some tribes subjected to Fire Exclusion/Suppression policies Lacking specific information about when these policies were enacted Tribes colonized during gold rush tend to have more information than those colonized earlier in time Consequently, some tribes have not been able to practice Cultural burning for generations Political Implications Some scholars argue that there is no real evidence that Native people were good stewards of the land Point to times when animals/plants went extinct Consequently, they should not play a special role in decision making Ecological restoration projects today Nor play much of a role in the management of our public Lands today Eco-archaeological Research # Eco-archaeological Research: When we bridge ecological studies with anthropological / archeological studies and native Oral traditions Tries to tackle Major Challenge of Eco-Archaeological Research: Differentiating Natural Fire Regimes from Anthropogenic Fire Regimes Compare Expected Fire Regime from Lightning Strikes with Observed Fire Regime Based on multiple lines of evidence Long-Term Diachronic Approach that Employ Multiple Lines of Evidence: Native Oral Histories/Traditions Ethnohistorical Accounts Ethnographic Studies Ecological Studies of Fire Archaeological Research Other methods Involves Researching with Tribes Two Case Studies # 1. Sierra Nevada Mountains # Difficult to carry out research because of high ignition rate due to lightening being common Hard to differentiate between natural and anthropogenic fires Research done by Linn Gassaway Specifically looks at Yosemite Valley which Home of Southern Sierra Miwok people Utilized Ethnohistory, Ethnography, Archaeology, Dendroecology (Fires Scars) Lightning Fires less Frequent in Valley Floor, more common on mountain peaks Natural Fire Regime in the valley has a Long Interval Between Lightning Ignited Fires Three Basic Findings By Gassaway: Fires more Frequent for prehistoric, protohistoric historic times than expected for Natural Fires Alone Evidence for potential Cultural Burning Still evidence of fire activity in 1800s (When Native peoples were removed) Miwok people still maintaining traditional fire practices? 1890s: Fire frequency reduced when Military takes over administration of Yosemite Valley \u0026ndash; Evidence for Fire Suppression 2. Eco-Archaeological Study of Central Coast of CA # Research done by John Keeley Central Coast great place to do study of Cultural Burning Hypothesis Presented for Strong Likelihood of Cultural Burning Here Low Frequency of Lightning on Coast Natural Fire Return Interval around 50 to 100 Years Greater Bay Area: About 2-5 lightning ignitions per hundred kilometers squared per century Conditions of high frequency (around 5 years) fire interval (cultural burnings) Forests dominated by Douglas fir would be suppressed Redwood (fire adapted) forests would remain dominant, but have a more open understory with less fuel Shrublands would be suppressed and costal parries would take over Conditions of low frequency fire interval (natural) Shrublands and forests would be common Central California Coast Project Amah Mutsun Tribal Band Issues of Food Security and Food Sovereignty Limited Access to Native Foods Do not own much property Tribe has Commitment to Ecological Restoration Want to return indigenous plants and animals to the land Established the Amah Mutsun Land Trust (AMLT) Purpose is to restore natural resources Purpose it to steward lands and waters Return to path of traditional ecological knowledge AMLT working with resource agencies who own much of the land (CA State Parks, BLM, etc.) AMLT also working to purchase land for the tribe Established Native Stewardship Corps Boots on the ground for ecological restoration Amah Mutsun Tribe interested in Collaborative Work Legacy of Colonialism Tribe working with collaborative team to bring Knowledge of indigenous stewardship practices out of dormancy Opportunity to integrate Indigenous Science With Western Science Amah Mutsun work with Team of Scholars from UCB, UCSC, and California State Parks Three components of project Synthesize Previous Work: Ethnohistory, Ethnographic Observations, Tribal Oral, Traditions, Historical Fire Ecology Cuthrell: Early Spanish explorers demonstrate extensive grasslands that must have been maintained through prescribed cultural burnings, as well as established Indigenous burning systems Undertake Field/Lab work—Fire Ecology Studies: Take samples of Fire Scars (Redwoods), Pollen/Charcoal, Phytoliths, Lake Cores Cuthrell: Phytolith Data from soil cores in Quiroste Valley indicate a long term history of grassland vegetation over the last ~1500 years Archaeology Archaeology provides understanding of past cultural practices (e.g., tools, settlements) Emphasis on recovering floral and faunal remains; evidence of kinds of resources harvested; Intersection of Archaeology with Fire Ecology; When we see changes in fire regimes, vegetation \u0026ndash; Do we see changes in kinds of resources harvested As outcome of landscape stewardship? Study number of sites: Middle Holocene (6500-3000 BP) Late Holocene (3000-500 BP) Historical (after 500 BP) Collaborative Field Schools Involves UCB students, faculty State Park Archaeologists, ecologists Amah Mutsun Tribal Band, AMLT Native Stewardship Corps Employ Low Impact Methodology Geophysical survey work (ground penetrating radar) Examples of Sites CA-SCR-7 (6500-4000 BP) CA-SCR-10 (1300-1200 BP) Bolcoff Adobe (1830s-1840s) Recovery of Plant and Animal Remains Excavate Soil Flotation of Soil to Recover Remains Laboratory Analysis Cuthrell: Food remains indicate costal prairie seed foods (which came from prairies) were prominent parts of people\u0026rsquo;s diets during the last 1000+ years. Charcoal from hearth fires indicates fire-compatible trees such as redwood were the primary fuel sources, while fire-vulnerable trees like Douglas firs were not as common Our Findings: Observed fire frequency greater than Expected from Lightning Ignitions Alone Strong evidence of Cultural Burning Extensive Cultural Burning: Begins about 1769-1770 CE Pattern of frequent, low intensity burns for the past 9-10 Centuries; up to Portola Expedition Create Coastal Prairie Environments, Evidence of grasses, clovers, tarweeds, hazelnuts, etc. Also evidence of plant foods being harvested that required frequent fires to maintain Patchwork of fires—burning areas every 1 to 5 years or so \u0026ndash; maintain coastal grassland Work in other areas corroborating our Findings Other Study Areas Coastal Prairies \u0026ndash; extend from California to British Columbia Conclusion Key Point for Rest of Class: How can these Lessons from the Past derived from studies Indigenous Landscape Stewardship Practices be employed today? Colonization of California # Introduction # Maritime Exploration The first real instance of Colonization Happened from 1542-1603 then a long break Spanish Missions, Presidios (forts), Pueblos (civilian settlements) Happened from 1769-1823 Primary goal was to Christianize and Civilize Created large farm areas Recruited thousands of Natives as labourers Select locations where existing towns had reduction policies Reduction policies moved natives into a single village after their land was taken Mexican Missions, Ranchos When Mexico took over CA Happened from 1822-1846 Established huge, private ranches (Ranchos) Ran by natives Generated lots of money Colony Ross Russian Colonization from 1812-1841 Carried out by Russian American Company Hunted sea otters Sold pelts to China Recruit Native Alaskans who served as primary hunters Impact # Native population declines Many deaths due to diseases (some of which hit instantly, some which took time) Violence and warfare between natives and soldiers (Sexual) Abuse of young women Unleashing of Foreign Weeds, Pests (Crosby 2004) Raised range of new foods on large ranches Thousands of cattle changed landscape too by overgrazing Weeds spread aggressively at expense of indigenous plants Land remains changed even today (90% of biomass in grasslands today isn\u0026rsquo;t native) Archaeological Evidence of Spread of Weeds Mission excavations and adobe (mud) bricks Grazing Economy (Dart-Newton, Erlandson 2006) Free-Range Grazing (no fences) In addition to cattle, brought Sheep, Goats, Horses, and Pigs Wild pigs are still an issue today Major impacts to local environments and Native Foods Overgrazing trample indigenous Ecological habitats Help spread weeds Commercial Hunting of Mammals Terrestrial Fur Trade Primarily hunted beaver Pelt used to make hats in Europe Exterminate beavers from Western North America, Northern California Major Impacts for wetland habitats Maritime Fur Trade (Colony Ross) Use Native Alaskan Hunters Very Efficient Harvest primarily Sea Otters Nearly eliminated, otter population still low population today Major Changes to Kelp Forests which impacted Fisheries of California Fire Exclusion (Timbrook) Began in 1793 when the Governor prohibited burning from Natives Explicit Policies Enacted to Stop Native Peoples from Cultural Burning Affected both Native Neophytes and so Called Gentile Indians (not part of missions) Implications # Loss of Traditional Ecological Knowledge (TEK) Amah Mutsun Tribal Band (Lopez 2013) Could not conduct ceremonies, prayer songs Could not Steward the land Unable to pass information to next generation (loss of TEK) Willing to work with archaeologists, fire ecologists \u0026ndash; bring knowledge out of dormancy Working to Restore Landscape Today Chumash People (Timbrook et al 1993) Located around Santa Barbara Ethnohistoric accounts of Native Burning, but largely faded from tribal memory when ethnographic research conducted in 1920-1930s Significant Impact to Local Coastal Environments Increase of Foreign Plants and Animals Decline in Diversity and Quantity of Indigenous Flora and Fauna Decreases Loss of Food Security, Food Sovereignty, Native Medicine, Raw Material, etc. for Natives Many Coastal Tribes Displaced Colonial Invaders take away Native land Especially costal land Major Transformation in California Fire Regimes Cultural Burning on Coast Stopped Coastal Prairies get overrun by woody vegetation quickly Even if grazing kept off shrubs, new grasses would begin growing Some Disturbance keep areas open: Grazing, Colonial Burning? Altered Fire Regimes due to new plant types Timber Owners and Early Use of Fire (10/4) # 1880: Advocates for ‘light burning\u0026rsquo; in California appear \u0026ndash; some thought fire was needed to manage forests Whereas cultural burnings are done for specific intentional reasons, light burning is done for other reasons such as reducing fuel, increasing food for livestock, and other economic reasons That is, cultural burning is a form of light burning 1900: Debate start regarding fire management near Lake Almanor in CA Private forest land owners practiced ‘light burning\u0026rsquo; following early Indian and sheep herder traditions Most foresters dismissed this proposal as mere ‘Paiute Forestry\u0026rsquo; (derogatory term based on an Indian tribe) 1902: H.J. Ostrander attacked ‘protectionist\u0026rsquo; policy of fire control as worse than effective because it allowed fuel to accumulate US National Academy of Science committee on forest reserves urged fire control policy 1908: Gene Tolly: Sierra National Forest Ranger from 1905-1912 and cowboy assigned to range management for USFS Gifford Pinchot: 1st Chief of the USFS Tolly took Gifford on a back country trip through the Sierra National Forest to try and convince him to allow Indian Burning to keep meadows and forests from getting overgrown but Gifford didn\u0026rsquo;t buy it 1910: Sunset magazine was an outlet used for forestry discussion at the time G.L. Hoxie wrote in the magazine to advocated light burning claimed it should not be merely acceptable but made mandatory Made economic, not ecological argument Unfortunately at same time, Great Idaho Fire occurred Burned 3 million acres and killed 85 people. Happened due to drought period with ignitions from lightning, locomotives, and logging Resulted in national debate: Senator Weldon Heyburn (R-Idaho) led effort to disband the USFS so that private land owners could manage their own land Others argued that the newly found US Forest Service (1905) should be expanded so that it could prevent future fires (happened, turns out govt tends to only grow bigger) 1930s: Two experiments were occurring in northeast CA around this time Full Control McCloud River Lumber Company Use trains, hand crews, etc Light Burning Thomas Barlow (T.B.) Walker was raised from Minnesota and ran the Red River Lumber Co. near Lake Almanor Clinton Walker (T.B\u0026rsquo;s son) wrote a letter to the Red River Lumber Company in 1938 (years after he had left the Company) saying: The general condition of the forests when the white man first came into CA was very excellent Then came the foresters from Yale University and put the tourniquet on the forests Would prefer to remove the tourniquet in our timber matters [which] is the lack of fire I requested permission [to burn] from the State Forester and the USFS DuBois. Both refused We proceeded to burn anyway, and Chief Forester Graves came out from Washington and DuBois and many others with cameras and notebooks to get damaging evidence They stayed several days and followed the burning, with comment by Graves that the work was excellent DuBois apologized to me for panning me in the newspapers previously Graves suggested that T.B underwrite a chair of fire protection at Yale University Walker agreed, contributing $100,000 (a huge amount of funding 1900\u0026rsquo;s). Harvey Chapman hired who was huge force in longleaf pine is southeast US Over the next decade trials done across the US, especially in pine belts of the West and South Stuart Show (from Stanford btw) looked for light burning area around Mt. Hough, Claremont, Meadow Valley Country Found a suitable area near Snake Lake Commit data/scientific fraud by adding more fuel: \u0026ldquo;Ed and I did 250 acres alone and, except for the long hitch of work, didn\u0026rsquo;t have any trouble. The only dishonest thing we did was to pile some pine limbwood in big fire scars of a few large pines, with the gratifying result that they burned down and became damage statistics\u0026rdquo;\nDave Rogers already had made up his mind regarding light burnings \u0026ldquo;Went over nonchalantly to reburn Snake Lake (in 1920). It would have been O.K. except that Plumas Supervisor Dave Rogers came out the night of the burn, grabbed a brush burning torch and ended up stringing fire outside the line on two opposite ends. Then he left.\u0026rdquo; \u0026ldquo;You can understand our deep affection for Dave and the Plumas\u0026rdquo;\nThe use of fire in forest management was not given an objective evaluation anywhere in the Western US including Indigenous burning 1940s: Harold Biswell was a professor in the Berkeley Forestry Department from 1947 - 1972 Biswell conducted prescribed fire research in the forests of Georgia from 1942 to 1947 when he was a researcher for the US Forest Service Southern Research Station Began prescribed fire work in Ponderosa pine forests in 1951 (Boggs Mountain near Middletown as well as Teaford Ranch in the Southern Sierra near Bass Lake) Both of these locations were private land because no government agency would let him burn in public land Said in his 1958 paper: \u0026ldquo;At the time the idea of burning was fairly new to me and I looked upon fire as the arch enemy of forests and forestry\u0026rdquo;\nHis 1958 paper showed it was possible to use fire in CA forest management and warned about future problems if fuels were not reduced His conclusions were very controversial at the time His PhD students recall \u0026ldquo;There was little opposition to him burning in grasslands and shrublands, but when he began burning experiments in ponderosa pine forests, active and open criticism of him and his work exploded. He was referred to as \u0026ldquo;Harry the Torch,\u0026rdquo; \u0026ldquo;Burn-Em-Up Biswell,\u0026rdquo; and other derisive names, and not always behind his back\u0026rdquo;\nNo forestry faculty would work with Biswell. UCB Forestry faculty voted to forbid him or his graduate students to work at Blodgett Forest (Scott\u0026rsquo;s Opinion:) At the very least, some UC Berkeley forestry faculty should have worked cooperatively with Biswell to further explore this topic In 1973, he was given the Berkeley Citation, awarded for his contribution to the University of California that \u0026ldquo;go beyond the call of duty and whose achievements exceed the standards of excellence in their fields.\u0026rdquo; He was the first UCB forestry professor to get this award Early Foresters and Fire (10/6) # Some debate, but suppression wins in 1910s However, one exception: Southern US continues to use fire Pre WWII: 1933: Civilian Conservation Corps labor (CCC) used to fight fires Hired many people due to great depression 1935: 10 AM (National) Policy: All fires should be out by 10AM the day after they\u0026rsquo;re detected before conditions that would make fire more severe arise During WWII Firefighting resources (people, machines) tied up with the war Consensus objectors put into fire fighting crews Many were ridiculed but they fought the fire with little technology and overhead Japanese sub fires shell near Santa Barbara Fear of Los Padres NF and others being burned Japanese also launch fire balloons Hundreds hit the US, but they\u0026rsquo;re kept hidden from public Public urged to be careful and fire is connected to the war USFS organizes Cooperative Forest Fire Protection Campaign Fire protection was framed as National Security through public campaigns Huge push to save resource during the war \u0026ldquo;Careless matches aid the Axis\u0026rdquo; \u0026ldquo;Our carelessness, their secret weapon\u0026rdquo; Bambi brought (in 1944) Movie showed terrible fires started by hunters Only on loan from Disney for 1 year Smokey Bear named in 1945 Named after asst. Chief of NYC Fire Dept. 1919-1930 \u0026ndash; \u0026ldquo;Smokey\u0026rdquo; Joe Martin Post WWII Smokey Bear (cont.) \u0026ldquo;Only YOU can prevent forest fires!\u0026rdquo; created in 1947 Canada steals him in 1956. Focus broadened to appeal to children 1950: The \u0026ldquo;Real\u0026rdquo; Deal Las Tablas fire breaks out and a bear cub found almost dead Cub is treated and heals, eventually ends up in D.C. Zoo Still endearing One of most recognized figures in U.S. behind Santa and Mickey Mouse Stamps made (1984) Own zip code (20252) \u0026ndash; you can write him a letter and get a reply Office, Web page, Twitter, Facebook, Instagram, YouTube, etc. Has Smokey\u0026rsquo;s message been TOO successful? Original message was very black and white Colonialism (10/8) # Settler Colonialism In The American # Earlier Colonial Enterprises (1769-1846) # Missionary Colonies: Franciscan Missions \u0026ndash; interested in converting Mercantile Colony: Colony Ross (Fort Ross) \u0026ndash; interested in pelts and profit Common Features to both: Predate Settler Colonialism Involved Few Euro-Americans \u0026ndash; workforce was almost all Native people Self-Contained Agrarian Systems Native people integral to their success Major Outcomes ( 9/27/21 Lecture) Loss of Traditional Ecological Knowledge Significant Environmental Impacts to Coastal CA Many Coastal Tribes Displaced Transformations in Fire Regimes Settler Colonies # Immigration of European/foreign settlers British employed Settler Colonialism in Eastern USA Establish Permanent Residences Predicated on Removal of Indigenous People Employed \u0026ldquo;Logic of Elimination\u0026rdquo; \u0026ndash; Moral Ground for Taking Land Away After American Revolution \u0026ndash; USA Adopts Settler Colony Practices; Manifest Destiny; Millions of people moved westward Key Dates for California: 1840s: Movement of settlers along California, Oregon, Mormon Trails 1846: Mexican-American War, Annexation of CA 1848: Treaty of Guadalupe Hidalgo Gold Rush 1850: California became the 31st State 1869: Transcontinental Railroad Completed 1850s-1870s:Settler Colonialism really takes off Implications Of Settler Colonialism # 1. Genocide (Madley 2016 Reading) # 1846-1870s \u0026ndash; Dark Ages for Native People Extermination of Indians supported by Governors and other Politicians: First Governor Peter Burnett proclaimed \u0026ldquo;War of Extermination\u0026rdquo; Senator John Weller (Governor in 1858) \u0026ndash; \u0026ldquo;White Man Demands Extinction\u0026rdquo; California Legislature passed anti-Indian laws: Legal slavery of Indian Children Bounty Hunters paid by the scalp No rights in Court (couldn\u0026rsquo;t testify against a white person) Vagrancy Laws, etc. California and Federal Government Supported militias \u0026ndash; volunteer companies (Essentially Death Squads) that removed Indians from their lands California and Federal Governments \u0026ndash; allocated over $2 million dollars for this Also diseases, starvation, etc. took a terrible toll on Indians Outcome of Extermination Policies: Native people Decimated, while Colonists Explode in Numbers: Indian Population: 1769 = 310,000 1846 = 150,000 1900 = 15,000 Settler Colonist Population: 1850 = 92,597 1900 = 1,485,053 2. Reservations # Unlike other Western States, most California Tribes did not have treaties with the Federal Government (as such, tribes cannot practice cultural burnings!) Abysmal Treaty Record in California 1851-1852: 18 Treaties Negotiated with Tribes of California; Proposed Reservation Land of 11,700 sq miles (Anderson 2005 Reading) None of the 18 Treaties Ratified by US Senate even though some Natives already moved Politicians didn\u0026rsquo;t want to give up good agricultural land Subsequent attempts to create Reservation Land \u0026ndash; Not very Successful; 1850s-1870s: Military Reservations 1870s-1900: Federal Funding for Small Land Grants Influence of Helen Hunt Jackson, Her Book, Ramona, very important in Humanitarian movement Fictional book told a love story of an Indian chief which became a best seller and brought attention to the situation 1906-1930: Another period of federal funding for small Land Grants for California Indians but it was all tiny sections of land (called Rancheria lands) 3. Federal Recognition # Discuss Importance of Federal Recognition: Housing, legal assistance, Indian Health, food distribution, child welfare, Indian arts and culture development, fund tribal cultural heritage programs, tribal historic preservation officers, NAGPRA, Indian Gaming Three-Tiered System in California: Gaming Tribes (Fed Recognized) Non-Gaming Tribes (Fed Recognized) Unacknowledged Tribes In California Many Tribes Not Federally Recognized (Unacknowledged Status with Federal Government \u0026ndash; no land or status with feds!) \u0026ndash; Why is this the case? 1851-52: treaties not ratified 1870s-1900s: Award Federal Recognition to tribes with strong continuities with Past; Those that maintained Indian cultural practices over time This was influenced by early anthropologists (i.e. UC Berkeley) Program of \u0026ldquo;Ethnographic Salvage\u0026rdquo; Tended not to work with Tribes who had undergone major transformations, acculturation Discuss Spatial Distribution of Fed Recognized Tribes; Many Unacknowledged Tribes in Former Mission Lands E.x. Ohlone Indians in post-mission times did not disappear 1840s: In East Bay 1860s: Lived at Alisal Rancheria well into 1900s Active Indian community \u0026ndash; inter-marry with other tribes, Hispanics Some Anthropologists claim they were \u0026ldquo;culturally extinct\u0026rdquo; role that early Anthropology Played in Tribes obtaining Federal Recognition, and those that did not (Lightfoot 2005 Reading) 4. Environmental Degradation (Anderson 2005 Reading) # Commodification of Environment Gold Mining, Hydraulic Mining Impacted rivers and lakes Commercial Agriculture and Ranching Drained wetlands Monocropping Commercial Hunting of Game, Birds, Fishing Massive Timber Harvests Clear cut total forests Exploitive \u0026ndash; not forestry Specifically Redwoods, Sierra Nevada Dam Rivers Unleash Plethora of Foreign Plants/Animals Four Outcomes Of Settler Colonialism On Indigenous Landscape Management Practices In California # 1. Fire Prohibition Policies # Initially Directed Against Native People; Racism \u0026ndash; Early Fire Bans Did Not Apply to All Settler Colonists Settlers, Ranchers, and timber companies still burned land Why Natives Signaled out for Ban? Fire Suppression - - Component of Settler Colonialism Strategy to Facilitate the Removal of Natives from their Lands? 2. Minimal Reservation Land Implications for Indigenous Stewardship Practices; # Fewer Restrictions on Indian Trust/Reservation Lands However, Lack of Tribal Lands in California Curtailed ability of tribes to Revitalize Landscape Practices, such as Cultural Burning Compare California with Trust Lands in American Southwest!! 3. Native People Lost Access To Resources From Their Tribal Lands # Major problem for tribes—they were not granted trust land (reservations) AND they had minimal access to resources on Public Lands (California State Parks, BLM, National Park Service, US Forest Service) Until Recently - Tribes not allowed to undertake Stewardship practices on Public Lands no cultural burning no harvesting of foods, medicines, raw materials 4. Conservation Practices: Exclusion Of Native People (Anderson 2005, Johnson 2014 Readings) # Influence of John Muir on Conservation Movement in USA in early 1900s Early debate about conservation practices involving light burning, indigenous stewardship Believed land was cathedral of nature and should be untouched (so no Indian burning!) John Muir: Argued for creations of pristine, natural preserves untouched by people Did not Advocate for Native Stewardship Practices, such as Cultural Burning Conservation Model - - Put Fence Around Property and keep people out Fire Suppression Policies of Settler colonialism One component of broader package of developments that have kept tribes from revitalizing cultural burning until recently Policies of extermination, genocide; poverty, diseases, food shortages; limited sized reservations; many California tribes unacknowledged; massive environmental destruction of tribal lands Upshot of Discussion, up until the 1960s-1970s, Native Californians had little or no land to call their own, minimal tribal land, little access to resources on Public Lands in California for harvesting foods, medicines, raw materials for baskets, etc. Greatly curtailed ability to undertake Indigenous Landscape Stewardship Practices, such as Cultural Burning Outline Fire Suppression Policies of US/CA Governments # Nathanial Kenny wrote an article on the US Fire Service Published in 1956 in National Geographic Argued that at the end of the day, science would be the most crucial role in controlling fires \u0026ldquo;I don\u0026rsquo;t believe that equipment and development alone will show us how to keep having the relatively few big fires… Researchers must let their imaginations soar for answers that today would seem fantastic\u0026rdquo;\nFire Suppression: Begins around 1905 Approximately 80,000 fires/year today 98-99% of all wildland fires out at less than 5 acres in size 95% of area burned today is from 1-2% of the fires that escape initial attack Often occurs in terrible conditions 4.5 million acres once burned in CA Over half Tribal burning \u0026ndash; 10-35% of this area burns today in most years but 2020 will burn this amount for 1st time (Stephens et al. 2007) Size is only one aspect \u0026ndash; what is the fire mortality? what is the distribution? etc. Not just area burned, burn patterns inside Historical data yields insight into controls on forest structure in pine-mixed conifer forests In 1911 photograph and datasets we can see much more sunny forests This is due to more frequent, natural/cultural burnings vs today\u0026rsquo;s fire suppression Kern National Forest Structure and Composition (Stephens et al. 2015) Southern California and Baja Forests (Stephens et al 2003) Drought from 1999-2002 in southern California and northwestern Mexico Only 5 inches of rain one year, driest on record in So Cal mountains another year Fire suppression and past forest harvesting have increased forest density in So Cal (Minnich et al 1995) Native bark beetle population increased because of weaker trees Sierra San Pedro Martir / Northern Baja California Mediterranean climate Annual precipitation averages 24 in Area has been grazed by livestock Similar to southern California and eastern Sierra Nevada Fire suppression begins in 1970, no harvesting SSPM Mission in 1794 severely impacted populations Within the California floristic province Forested area of around 40,000 acres Fire suppression and past forest harvesting have increased forest density (Minnich et al. 1995) Elevation upper plateau 8800 ft 3 large plateaus, Peninsular Mountains Jeffrey pine-mixed conifer forests Similar to souther CA and eastern Sierra Nevada Fire suppression begins in 1970s, no harvesting 3 indigenous cultures used portions of the SSPM outside the winter: Paipai Kiliwa, Nakipa SSPM Mission in 1794 severely impacted populations Mission lasted 8 years due to extreme climate SSPM wildfire July 4, 2003 Started in chaparral below forest Fire burned approximately 600 acres More shrubland than forest burned Largest fire in 20 years Occurred at end of sever drought, 1999-2002 Same drought as in SoCal SoCal drought killed millions of trees w/o fire Only 20% of trees killed Jeffrey pine more dominate after fire, trees and seedlings (less white fir, incense-cedar) Fire was very patchy Directly linked to heterogeneity of forest structure and fuels pre-fire Mortality very low even after 4 year drought and wildfire (Stephens et al. 2008) Incredible forest resilience!! Gives us hope in California To answer Nathanial Kenny \u0026ndash; Fire back, restoration thinning, and stewardship are also key Implications for Local Ecosystems # Changes to Grasslands Encroachment of woody species Pinyon pine-Juniper in Southwest Tallgrass prairie Mountain meadows Coastal Prairies Fire taken out of system Fire return intervals increased Whole ecosystems change Examples from Montana and Arizona Rim Wildfire Largest fire in Sierra Nevada Fifth largest fire in history of CA Ignited August 17, 2013 Total area burned: 270,000 acres (102,000 ha ) Fire effects Very large high severity patches Cost of suppression : $127.2 million Management Response Forest fuel reduction treatments implemented to reduce fire hazards and fire effects Reduction of surface and ladder fuels critical (Agee and Skinner 2005) Example of fuel reduction treatments Research has determined that treatments are effective in reducing potential fire behavior and effects (Fulé et al. 2012) Permanent Backlog: Sierra Nevada 2.9 million acres (60% of FS acreage) will always remain fuel loaded 2/3\u0026rsquo;s of this acreage is pine-dominated and mixed-conifer forest types This is a disaster! Only gets worse with climate change Summary Tree density increased 2-3 times in mixed conifer forests since 1911 Forest change has decreased resiliency Climate change will make this situation worse Need increased fuel reduction treatments and wildfire for resource benefit frequent fire forests - critical California legislation $200 Million/yr. through 2028 Fuel treatments on 1 million acres/yr. by 2025 US Forest Service management plans being revised Best chance in decades to change trajectory Next 1-2 decades absolutely critical Leave options available for future managers We are running out of time!! Leopold and Forest Restoration # Leopold et al. 1963 # Asked to carry out report by US govt Carried out in Yellowstone Took view of the whole ecosystem, not just a single issue Animal populations \u0026lsquo;protected\u0026rsquo; from hunting and habitats \u0026lsquo;protected\u0026rsquo; from wildfire Wolfs (predators) were hunted before so that elk could thrive Elk thrived too much; Elk became hunted because their population grew so high Habitat is not stable that can be set aside and preserved behind a fence Goal of Park Management in the United States Each park be maintained, or where necessary recreated, in the condition when first visited by the white man Wilderness isn\u0026rsquo;t devoid of people; need to consider native stewards A primitive America could be recreated, using skill, judgment, and ecologic sensitivity The forty-niners poured over the Sierra Nevada and found big trees in gigantic magnificence Ground was grass parkland, in springtime carpeted with wildflowers (No mention of indigenous management) Today dog-hair thicket of young trees and mature brush—a direct function of protection from natural fires Is it possible that the primitive open forest could be restored? And if so, how? (Big question even for today) Policies of Park Management Research, not intuition, should form the basis for all management Agency best to study park management is the National Park Service Management without knowledge would be a dangerous policy indeed Methods of Habitat Management Of the methods of manipulating vegetation, controlled use of fire is the most \u0026ldquo;natural\u0026rdquo; and the cheapest and easiest to apply (easy?? Maybe in the 1960\u0026rsquo;s) Forest and chaparral areas protected from fire may require careful advance treatment (Harold Biswell influence?) Trees and mature brush may have to be cut, piled, and burned before ground fire (Harold Biswell did this with his early burning) Once fuel is reduced, periodic burning can be conducted safely and at low expense (Biswell) Against wind and down hill results in slowest and least severe fire conditions \u0026ldquo;We are calling for a set of ecologic skills unknown in this country today\u0026rdquo; (Indigenous?) \u0026ldquo;It will not be done by passive protection alone\u0026rdquo; (Big statement at the time, Still important today) Very powerful ideas included, changes NPS course of action (no Indigenous people though) Sneeuwjagt et al. 2013 # Rick was the head of bushfire and prescribed fire in Western Australia which also has a Mediterranean climate Opportunities for improved fire use and management in California: lessons from Western Australia (WA) Treatment size - Prescribed burn units much larger than those implemented in the US - make ours bigger Burns averaging in excess of 5000 ac, with some as large as 25,000 acres in Western Australia Department of Environment and Conservation (DEC) fire managers take advantage of relatively few burn windows to burn large areas Managers use weekends and nights, if conditions are favorable Have multiple burn plans in place in each region in order react quickly to conditions DEC begins training new seasonal crews on prescribed fires in the spring burning season DEC has responsibility for fire management across all public lands in the state of Western Australia (integrated versus USA system) Impossible to combine the many county, state, and federal agencies of CA under one agency but we need to do better Similar to California, prescribed fire in Western Australia is a contentious issue, particularly some urban residents who not aware of benefits of prescribed fire Indigenous burning not integrated with DEC burning \u0026ndash; rare across Australia at least in 2013 DEC uses a focused outreach effort to the public and politicians \u0026ndash; otherwise, fire program could get cut Fire program severely tested from an escaped prescribed burn in November 2011 that destroyed 32 houses Part of Reason Rick retired The first instance of home loss from a DEC burn in 50 years More stringent risk management assessment of the program occurred Despite community anxiety, support for continuation of extensive prescribed burn program remains high (similar to Florida from home losses 5 years ago) These are mature programs, California is in the beginning and we will need patience and the support from the public and politicians We will have problems: Indigenous fire use could help us move forward Hopefully can learn from them and build a program MT2 Review # Construction of railroad occurred after statehood of CA Colonization from San Fran do San Diego was unique in that it was primarily to set up missions Paiute Forestry was an offensive term that was directed at light burning, calling it \u0026lsquo;primitive\u0026rsquo; because tribes practiced it Stuart Show manipulated data to make burnings look more dangerous Canopy fires Higher severity \u0026ndash; upper trees aren\u0026rsquo;t fire adapted like the base of trees Used to be rare pre-1800s, now more common due to build up of surface fuels Latter fuels allow fire to travel up fuel, up to canopies Clinton Walker was an advocate of light burning There is little short-term economic gain to do fuel treatment CA\u0026rsquo;s first governor (Peter Burnett) supported the Native American protection act and endorsed the war of extermination At least 4.5 million acres were burned annually pre-colonization (1800s) Ron Good claimed that cultural burning was primarily for cultural reasons + increase value of raw materials and increase visibility Harold Biswell was a UCB professor and was an advocate of prescribed burning when it was taboo Was ostracized at the time, but is now recognized Early EU explorer period was different from the Spanish missionary period in that the EUs didn\u0026rsquo;t settle down and instead explored/traded while the Spanish established settlements Gifford Pinchot was very against any kind of burnings 1910 fire occurred in Idaho and led to many advocating for fire suppression Population reversal of CA Indians occurred due to the finishing of the rail road (letting more people in to CA, i.e during the Gold Rush) Essay Question: Settler colonialism differed from earlier Missionary trips in that\u0026hellip; Settler missions went inwards + cared primarily about making money During Missionary trips, Natives were a large part of the workforce and the main purpose to convert them to Christianity Two sides of light burning controversy: Natives encouraged to burn so that there wasn\u0026rsquo;t fuel build up, etc. Foresters who opposed ranchers employing any form of burning, called it Paiute Forestry Settler colonialism impacts Intersection of many invasive plants/animals that radically altered ecosystem (e.x. cattle) Draining of lakes/dams to flood areas (altering ecosystem) and other forms of land modification for the purpose of agriculture Hydrolic mining leading to soil erosion Logging + clear cutting of forests Anderson 2005, settler period Forceful removal \u0026ndash; through raids, violence, giving false land titles, destroying food/water source Extermination (killing) \u0026ndash; Genocide (Benjamin Madley) Assimilation \u0026ndash; Native Children went to school to be whitewashed (re-educated) How to plan for prescribed burning, comparison with Florida # Prescribed fire definition: A wildland fire burning under pre-defined conditions that will accomplish certain planned objectives. Fire is ignited by people by drip torch or helicopter Whereas cultural burnings exist only for cultural reasons Prescribed burner must integrate: Weather (present day and forecast) Weather has strongest correlation with severity of the fire + it can change rapidly Topography Fuels (load, moisture content) Spotting (embers flying across lines) occurs when fuel is very dry Ignition patterns (how fair is put on the ground) Art side \u0026ndash; factor you can control E.x. head fires, backing fires, strip-head fires Crew size and experience, where to assign people Safety (highest priority of all fires) Risk (all fires have risk, goal is to minimize it and have a plan in place to execute if something goes wrong) Prescribed fire is both art and science You can use science, but it ultimately depends on land/weather Biswell did not take measurements His graduate student Jan van Wagtendonk developed prescriptions for his PhD at UCB Southeast USA # Forests, savannas, and grasslands of the southern US, well-established history of fire, whether from lightning, Indigenous ignitions, or Anglo-Europeans Florida, Georgia, Alabama, Carolinas, all have great prescribed burning programs Carried down from tradition Burning used to increase productivity of sap for turpentine, decrease hardwoods in timber lots, enhance grazing forage Became a family tradition Plant adaptations diverse: closed cones, large terminal buds, sprouting, bark thickness Longleaf pine Low intensity, frequent surface fire to reduce competition Thick, corky insulating bark Self-pruning at maturity Large terminal bud Chapman studied \u0026ndash; Yale USFS tried to eliminate but not successful Kobziar et al. (2015) conducted survey of eastern US burners from in 2011 - 2012 Most respondents (75%) were employed by State or Federal forest or wildlife agencies, with the remainder landowners or contractors Fuels reduction primary goal of prescribed burning overall Program continues to be successful Florida burned \u0026gt; 1.2 million acres of forest last year Longleaf pine in \u0026lt;5 year, low intensity, understory, summer, brown rot Big shrub (sand pine) 25-100 year, high intensity, crown, spring or summer, large areas Bald Cypress / tupelo swamp, \u0026gt; 200 years, small area, mixed severity, only during drought Georgia said it surpassed Florida in terms of acres this year Prescribed fire in California - Low - maybe 50,000 acres Brown Administration Bill passed to provide $200 million/yr. for fuels management projects - Apply for grants Continues under Newson Administration and funded through 2028 Goal to increase prescribed fire in CA to 1,000,000 acres/yr by 2025: 500k acres State and Private and 500k acres Federal What is our baseline? Cultural burning combined with Rx fire? Task forces created in State to increase pace and scale of restoration and prescribed fire Public lands managers - limited budget and staffing are impediments, also planning issues We don\u0026rsquo;t have the experience, crews, or political or institutional support in place today for a large program We need to be patient - will take time Managed Wildfire Effects on Water and Forest Health # Fire and Hydrology in Western US Watersheds - Project Starts 2002 Managed Lighting Fire, Increase Forest Resilience Illilouette Creek Basin, Yosemite National Park Fires in occur naturally through lightening here every 9 years 50 years of fire use, 40,000 ac watershed Reburn fires Interactions between adjacent fires Historically open, patchy stands with large trees not everywhere Evidence of small proportions of stand-replacing fire (5-15%) Show and Kotok (1924): \u0026ldquo;…no large fires occur without a certain amount of heat-killing\u0026rdquo; \u0026ldquo;This loss, it should be noted, represents the complete or nearly complete wiping out of small patches of the stand rather than a uniformly distributed loss over the entire area\u0026rdquo; Vegetation Change from Photos: Fires Reduced Forest Area by 22% Wet meadows increased by 200% Dry meadows increased by 200% Shrublands increased by 30% In Yosemite amount of stream water leaving watershed has increased or remained stable since 1974 - modeling study increased by 60 mm Three other control watersheds significantly decreased Flood risk unchanged Soil water increased Lower tree mortality in drought Since fire suppression ended… Runoff ratio increased or stable Duration of spring snowmelt longer Soil water storage increased, less mortality drought Stream discharge up 3-6%, deep storage up Use of lightening ignited wildfires in Yosemite has provided several benefits to forest and water Indigenous fire could complement lightning fire California is water scarce and is experiencing an increasing number of severe wildfires Despite warming climate, managed wildfires in Illilouette promote a healthy watershed Increased streamflow from wildfires will persist in a warming climate Water agencies supportive of bond funding to manage watersheds but not their base budgets - should change 2020 North Complex Fire burned the largest watershed that feeds the largest lake in the State Water Project (Oroville Lake) Next 1-2 decades absolutely critical in California frequent fire forests Optimistic but we must move decisively Case Studies of Cultural Burning # Background On Tribal Revitalization # Timeline 1960s - Watershed time in California 1963 - Leopold Report \u0026ldquo;Wildlife Management in National Parks\u0026rdquo; bring fire back to National Park Service Managed Lightning Fires Prescribed Burning 1960s - Also Time of Native Activism in CA Historical Perspective: By the 1960s tribes facing some real structural challenges: Recovering from Genocide Little land to call their own Consequence of 1851-52 treaties not being ratified Many California tribes not federally recognized Tribes denied access to federal/state lands to harvest plants and animals Fire Prohibition Policies Fire Exclusion - Spanish, Mexican and Russian Colonists Fire Suppression - American period 1890s - Sequoia and Yosemite 1900s - Elsewhere in California Upshot for Tribes: A. Not allowed to implement Indigenous landscape Stewardship practices B. Poor Health of CA Environments Fire Suppression Policies: Major Impacts to indigenous plants and animals fuel loads increase less ecological diversity loss of patchy mosaic Some Native species disappeared fire adverse species take over hydrology affected some ecosystems - become endangered (e.g., coastal prairies) Native Activism # 1960s - Native Activism, part of broader Civil Rights Movement in USA Protest series of issues: discrimination, poverty, unemployment, religious freedom, fair housing, broken treaties, lack of resources Natives not given right to vote until 1924 Snyder Act Some states did not allow Indians to vote until 1950s, 1960s 1968 - Civil Rights Act signed by President Johnson Guaranteed rights to not only African-American, but American Indians 1968 - when American Indian Movement Founded (AIM) Advocate for rights on Native people 1968-1971 - Occupation of Alcatraz Island by tribes Post-1968 - series of protests, demonstrations, marches for American Indian causes Tribal Revitalization In California # Last few decades - major renaissance with Tribes Keep in mind that Tribes still facing many issues Poverty Unemployment - not much of a land base, FAR from urban jobs Addiction issues - alcohol, drugs Health issues, such as diabetes, obesity Gang issues with younger tribal members Still explicit discrimination But despite these lingering problems from colonization Major transformations taking place in many tribal groups Restoration of local habitats \u0026ndash; Crucial part of this process But need to look as this within broader context Tribal revitalization - involves various facets of Native Life TODAY Native Languages Estimated that 80-100 languages spoken in California \u0026ndash; but many languages endangered Legacy of Colonialism Indian Boarding Schools (late 1800s-mid 1900s) Assimilation Policy of US Government Carlisle Boarding School, Pennsylvania Sherman Institute, Riverside, CA \u0026ldquo;Civilize\u0026rdquo; Indian Children away from negative influences of parents, tribes Children forcibly removed from home and sent to Boarding Schools Taught Western ways Speaking Native Languages Forbidden! By mid-1900s - Many Tribes Facing Crisis with Their Languages Only half of Native languages in CA still spoken Leanne Hinton - UC Berkeley 90% of Native Languages in CA may disappear Major Push Today: language training for California tribes The Advocates for Indigenous California Language Survival \u0026ldquo;Breath Of Life\u0026rdquo; workshop Program on Berkeley campus provides training to Indians studying/learning endangered languages historic linguistic tapes and information in Linguistics Dept, Bancroft Library, Hearst Museum of Anthropology Tribal scholars relearn how to speak dormant languages (Mutsun, Chochenyo Languages) Revitalization Of Native Ceremonies, Dances Many groups involved in reviving spiritual practices Healers, Indian doctors, spiritual specialists Create active dance groups, e.x. Su Su Shinal Kashia Pomo Close Relationship of California Indian religions with Environment Resurgence Of Native Craftspeople Resurgence of Native Crafts Ron Goode - soapstone artifacts woodcarving - Northwest Coast Acorn spoons, boxes, Basket Weaving Premier basket makers in world Baskets served many purposes in Indian households Challenges for Indian Basket Making by 1980s - only a few active weavers Led to Establishment of California Indian Basket Weaver Association host gatherings, workshops more than 650 weavers today! Major Advocate for Indigenous Landscape Stewardship Practices! Great Interest In Native Foodways Traditional menus, traditional foods acorn mush, salmon \u0026ndash; but also seaweed, tar weed, hazel nuts Pow Wows Tribal food security, food sovereignty Café Ohlone - Bancroft Way - bring to Cal campus All Of These Developments - tied into Native Landscapes and Environments Most tribes interested in ecological restoration of their lands Implementing some form of indigenous landscape management practices Bring Fire Back To Landscape \u0026ndash; Tending The Wild and gaining access to various resources on Public Lands Obtain indigenous foods, medicines, dance regalia, raw materials for crafts Ecological Revitalization # Five Case Studies Of Indigenous Stewardship Practices In California\n1. Fowler et al # Fowler, Catherine S., P. Esteves, G. Goad, B. Helmer and K. Watterson 2003 Caring for the Trees: Restoring Timbisha Shoshone Land Management Practices in Death Valley National Park. Ecological Restoration 21(4):302-306.\nFederally Recognized Tribe working with National Park Service Death Valley National Park Timbisha Shoshone tribe in Southern Deserts Granted federal recognition in 2000 Obtained Trust Lands \u0026ndash; Federal Recognition Right to enter traditional management agreements with Federal Agencies (NPS, BLM) Work out co-management agreement with NPS in Death Valley National Park Co-manage Two Key Resources Honey Mesquite, Single Leaf Pinyon Both used as food, raw materials, etc. Timbisha Shoshone Begin to tend the Mesquite and pine groves in Death Valley Had not been tended in years \u0026ndash; in terrible shape! How Undertake Ecological Revitalization While Tribe used Fire to Tend the Land in the Past Current work - using Fire Surrogate Methods \u0026ndash; hope to use fire in future Define Study Plots - Leave half of plot as control Other Half of Plot - Tended by Tribe Trim lower branches of trees Clear ground of underbrush Open trees to sunlight Challenges Insect infestations Changes in Hydrology 2. Codero-Lamb et al # Codero-Lamb, Julie, Jared Dahl Aldern and Teresa Romero 2018 Bring Back the Good Fires. News from Native California 31 (Spring):14-17.\nNon-Fed Recognized Tribe Work on private property Ecological restoration Southern California Chumash - Santa Barbara Area \u0026ndash; only one of tribes federally recognized Santa Ynez Band of Chumash Mission Indians article about non-federally recognized CHUMASH Aftermath of Thomas Fire in Dec 2017 Strong Commitment To Bring Back Indigenous Landscape Practices Bring back good fires recognize issues of working in Chapparal Environments Paper outline difficulties of doing this when not federally recognized, no tribal land No current agreements with government agencies to burn instead - burn on small patches of private land tiny patches tended by Chumash Burn Protocols based on elders, songs, and Scientific Studies Discuss Frequency Or Timing Of Burns Springs - Annually grasses and shrubs for baskets - every 3 years 10-15 years in chaparral Recognize that they must adjust seasonal timing of burns with CLIMATE CHANGE Strong Advocates for Cultural Burning Important Points: For federal and state to restore environment after years of Fire Suppression Need to incorporate local communities (Tribes) Get Tribes Involved In All Aspects of Projects Must Recognize Tek is Valid Strategic Placement of Tended Lands \u0026ndash; can serve as fuel breaks 3. Karuk and Yurok tribes of Northwest California # Lake, Frank K., and Amy C. Christianson 2019 Indigenous Fire Stewardship. In Encyclopedia of Wildfires and Wildland-Urban Interface (WUI) Fires, edited by Samuel L. Manzello. Springer, Cham. https://doi.org/10.1007/978-3-319-51727-8_225-1. Lake, Frank K., Vita Wright, Penelope Morgan, Mary McFadzen, Dave McWethy and Camille Stevens-Rumann 2017 Returning Fire to the Land: Celebrating Traditional Knowledge and Fire. Journal of Forestry 115(5):343-353. Terence, Malcolm 2016 Unleashing the TREX: Why Officials Think Controlled Burns Can Save California from Wildfire. North Coast Journal ( http://www.northcoastjournal.com/humboldt/unleashing-the-trex/content?oid=4132514.\nFederally Recognized Tribe \u0026ndash; work with Federal Agency (USFS) Frank Lake \u0026ndash; key player in bringing Cultural Burning to California Karuk tribal member and works for USFS well positioned for making a difference publishing widely on cultural burning strong proponent for government agencies, conservation groups to develop collaborative partnerships with Tribes Makes this point in Lake and Christianson 2019 Reading How cultural burning can reduce or mediate impacts of wildfires Important information about burning and stewardship passed down through generations Significance of Traditional Ecological Knowledge (TEK) TEK looks at what works, what doesn\u0026rsquo;t, and how things change over time Important for us to use/employ today in how we can learn to live with fire in California Frank helping to bring cultural burning back to Northwest California burns taking place on\u0026hellip;. Private land (fewer restrictions) Tribal lands \u0026ndash; Yurok have some land USFS lands \u0026ndash; control most of traditional tribal lands in the area Five Points to emphasize in burning in Yurok and Karuk lands # Karuk and Yurok Tribes Different process of colonization than Central/Southern California Later in time - post Gold Rush (1848) which is unique Compared to SoCal, where practices were banned early on, Karuk and Yurok have more recent information E.x. Bill Crook learned from his grandma who practiced burns pre-gold rush Considerable Indigenous Knowledge about Cultural Longer continuation of burning USFS Until fairly recently, have not been receptive to Cultural Burning Until fairly recently Now have Native Californian employees high up in management Can\u0026rsquo;t be about certain people, should be about policy Otherwise, if these key people retire then we\u0026rsquo;re screwed! Signed agreements \u0026ndash; MOUs (Memorandum of Understanding) \u0026ndash; between USFS and Karuk Tribe Government To Government Relations Frank Lake et al. 2017 Reading Importance of developing person-to-person relationships and trust workshops, meetings, importance of communication with tribal leaders Integration of TEK and WS (Western Science) Two Different Ways of Knowing WS: Prescribed Burning More utilitarian approach \u0026ndash; can be tailored to smaller area to achieve specific outcome Emphasize Fuel Reduction Residential Areas (W/U Interface) TEK: Cultural Burning requires a more nuanced, patch-like approach Need to consider specific resources in a generally large area E.x. burn early for short basket shoots and later for longer shoots Frank Lake argues that \u0026hellip; Advocate for integrating TEK with scientific disciplines Use multiple lines of evidence to understand past fire regimes and cultural burning Federal Agencies - need to respect TEK Work collaboratively with tribes \u0026ndash; all phases of projects Need to Provide Sufficient Funding! To maintain partnerships, Cultural Burning Programs Need to follow through and show commitment to tribes 4. North Fork Mono Tribe # Ortiz, Beverly R. 2018 Ron Goode: A Life Lived in Service to Community and Environment. News from Native California 31(3 (Spring):18-26. Goode, Ron W. 2015 Tribal-Traditional Ecological Knowledge. News from Native California Spring 2015:23-28. Long, Jonathan W., Frank K. Lake, Ron W. Goode, and Benrita Mae Burnette 2020 How Traditional Tribal Perspectives Influence Ecosystem Restoration. Ecopsychology 12(2):71-82\nCase Study: Non-federally recognized tribe work on private lands + also develop agreement with USFS North Fork Mono: Major advocates of Cultural Burning Ron\u0026rsquo;s knowledge about Cultural Burning comes from his (grand)parents who continued cultural burnings during fire-suppression years Also works with scientists to teach/learn from them Involves other tribes with work to get range of perspectives Cultural Burning \u0026ndash; tied To Spiritual Practices Tribe\u0026rsquo;s Spiritual Philosophy involves prayers and songs Mother Earth And Creator Began burning on private property his family owns 2003 - began working with USFS in partnership Meadow restoration project in tribal territory Restore meadow and spring Enhance deer grass (for baskets) Long et al 2017 Reading discusses\u0026hellip;. Importance of cultural resources (i.e. plants which can be used as medicine) Archaeology Culturally important plants, etc. Differing opinions with USFS E.x. USFS wants to remove all invasive plants whereas Ron argued to leave a few that the tribe would look over 2017-2021 Has been practicing Cultural Burning on private ranch Near Mariposa (Yosemite National Park) Location is associated with major archeological sites Burn for targeted results, as listed below Specific Reasons for Burning Burn for positive result to affect outcomes of targeted cultural resources More than just fuel reduction (which is a trait of prescribed burning) Ron views Prescribed Burning as Industrial Scale burning Cultural Burning: more NUANCED kind of burning \u0026ndash; burn treatment depends on cultural resources being enhanced Sourberry \u0026ndash; important plant for food and basketry material Pretreatment before burns Prune vegetation Create burn piles Burn small plots Post-Burn \u0026ndash; mix burned soils with water to create a nutritious soil bed Summary thus far\u0026hellip; # Tribes \u0026ndash; need to be treated respectfully Need to establish partnerships in CA Resource agencies, tribes, and researchers Tribes should be involved in decision making concerning fire management strategies For their traditional lands Bring in at the outset of revitalization plans Many tribes working with researchers Recognition of importance of TEK and WS Not mutually exclusive! Brought together Too many regulations in bringing cultural fire back Need to ease up on RED TAPE (based scott) Especially at the Federal level, CA is actually pretty good as far as enabling burning Dealing with Intrusive species Areas overgrown from Fire Suppression E.x Chapparal Need Funding for Proactive Work to Manage Environments Funding for pro-active management Funding for environmental revitalization Funding for eco-system corps Maintain programs over time Possibility of Incorporating Prescribed Burning and Cultural Burning? 5. Amah Mutsun Tribal Band # Hannibal, Mary Ellen 2016 Rekindling the Old Ways: The Amah Mutsun and the Recovery of Traditional Ecological Knowledge Bay Nature April-June 2016:28-35. Lightfoot, Kent G., Rob Q. Cuthrell, Cristie M. Boone, Roger Byrne, Andrea B. Chavez, Laurel Collins, Alicia Cowart, and R. Evett, Fine V.A. Paul, Diane Gifford-Gonzalez, Mark G. Hylkema, Valentin Lopez, Tracy M. Misiewicz and Rachel E. B. Reid 2013 Anthropogenic Burning on the Central California Coast in Late Holocene and Early Historical Times: Findings, Implications, and Future Directions. California Archaeology 5(2):371-390.\nValentin Lopez \u0026ndash; Tribal Chair In Week 5 \u0026ndash; discussed eco-archaeological work that UCB and UCSC Partnering with Amah Mutsun + California State Parks Outlined findings employing different lines of evidence Fire scar analysis Pollen/charcoal (wetland coring) Phytoliths \u0026ndash; tells us the extent of grasslands Archaeology Ethnography, ethnohistory, tribal history Our Findings: Strong evidence of Cultural Burning Observed fire frequency greater than Expected from Lightning Ignitions Alone Extensive Cultural Burning: Begins about 1300-1200 BP Pattern of frequent, low intensity burns for 9-10 Centuries up to Portola Expedition in CE (Common Era) 1769-1770 Create Coastal Prairie Environments, Evidence of grasses, clovers, tarweeds, hazelnuts, etc. Rob Cuthrell talked about this Fire Tenders created a productive, patchy landscape mosaic Burning some patches one to 5 years or so cycle maintain coastal grassland! along with neighboring woodlands, wetlands, and forests with fire-adapted species such as hazelnut, California lilac, and redwood What Are We Doing With These Findings? # Ultimate Goal Of Research Program How lessons from the past can be applicable to contemporary CA? How these finding may be incorporated into ecological revitalization programs That are rooted in the deep history of tribal practices The Amah Mutsun working with California State Parks Co-managing some lands \u0026ndash; Quiroste Valley Cultural Preserve MOUs (Frank Lake) Committed to ecological revitalization Understand limitations of fire suppression Bringing back Native plants and animals that once flourished Based on findings from our eco-archaeological findings Tribe \u0026ndash; interested in bringing back patchy landscape mosaics That includes extensive patches of coastal prairies which have become rare with fire suppression Encroached on by conifer forests and shrublands Val Lopez and Tribe Created Amah Mutsun Land Trust Purpose is to restore lands in their territory Undertake research Support Native Stewardship Corps Boots on the Ground for work How we are doing this: # Select Patches Where we Open up Landscape Involves Removing Problematic Invasives Poison hemlock, milk and Italian thistle, jubata (Pampas Grass) Fuel Reduction Use clippers / chainsaws remove brush and small trees Can be used to make burn piles, biswell-style Few places mechanized removal of overgrowth Where Possible Implement Cultural Burns Begin Process Of Bringing Fire Back Work To Bring Back Culturally Important Plants in Patches Plants used for food, medicines and raw materials Based on Eco-Archaeological Work We have good information about plants recovered Two Ways: Patches selected for revitalization Undertake Integrated Cultural Survey Reason: Ama Mutsun have a spiritual obligation from Creator to use fire to take care of ancestral land Goals: (1) To document, protect, and steward cultural and natural resources, (2) locate areas that may be suitable for landscape revitalization + seed collection Landscape components: (1) Non-biological = archeological sites, natural springs, viewsheds, salmon spawning habitat (2) biological = vegetation type and ethnobotany Stratified survey approach: focus on broad areas of flat land near water Include survey to detect ancestral archaeological sites High density of ethnobotanical and culturally-important vegetation (seed and nut foods, basketry and crafting taxa, hazelnut patches) Grasslands being converted to shrublands Also locate other important cultural resources Food plants, medicinal plants AMLT \u0026ndash; Implemented a Native Nursery and Plant Propagation Program Nursery used to raise thousands of plants identified in eco-arch work Tarweeds, California Brome Grass, California Canary Grass, Red maids, yarrow, etc. After propagated in the greenhouse, they plants are moved outside to field beds where they\u0026rsquo;re then \u0026ldquo;harvested\u0026rdquo; for their seeds Enables us to keep rare plants alive Raise seedlings in nursery Some then planted in fields these provide seeds for restoration We will use these seedlings and seeds in our revitalization efforts In Undertaking This Work Hope To Create Fuel Breaks Open Areas \u0026ndash; where we can slow down fire, may not stop major fire but provide where calfire can better manage major fire! Hope to Construct Patchy Landscape Mosaic across tribal territory And if strategically designed and placed, can address three crucial concerns today: Enhance Quantity And Diversity of Indigenous Plants And Animals Provide Better Food Security For Tribal Members Provide Places for tribal harvesting of native foods, medicines, Dance regalia, raw materials for making baskets, other crafts Create Extensive Fuel Breaks In critical areas on public lands Help minimize risk of major fires Eco-System Stewardship Work in CA # Need to Create New Framework for Caring for our Open Spaces - Active Ecosystem Management - Ecological Revitalization - Need a Separate Stewardship Corps dedicated to this mission - Need to get Tribes and Local Communities Involved! - Marks-Block et al 2021 Reading Discusses why we cannot rely on Fire Suppression people to do ecosystem management Requires Locally Situated Practices No one size or one way to manage landscape of California; Employ Various Treatment Plans Depends on Objectives and Options Anderson and Barbour 2003 Reading Managed Wildfires Simulated Wilderness Management Model Broadscale Prescribed Burning Indigenous Landscape Stewardship Practices Cultural Burning Co-Management of Area Simulated Indigenous Management Model Integration of Industrial-Scale Prescribed Burning with Indigenous Stewardship Practices (Cultural Burning)? Is it possible? Prior to Treatment \u0026ndash; Eco-Archaeological Surveys Integrated Approach outlined by Alec Identify Locations of Archaeological Sites; Identify patches of important Ecological Resources: Plants important to tribes for food, medicines, materials, etc. If enough time, may be possible to do refined Eco-Archaeological study Soil samples, flotation, recover plant/animal remains and associated tools from archaeological contexts Obtain physical remains of plants and animals that tribes harvested Areas with Important Arch/Ecological Resources: May call for additional or different treatments from rest of prescribed burn area Similar to Cultural Resources Management work in Urban Context Some Cases \u0026ndash; Prescribed Burning OK But require additional follow-up treatment Other Cases \u0026ndash; need pre-treatment before Prescribed Burn: treat differently from rest of prescribed burn area Ron Goode \u0026ndash; pruning, vegetation removal around site or patch reduce fuel loads Scott \u0026ndash; San Vicente Redwoods Treat oaks differently than redwoods (different burn treatment) Marks-Block et al 2019 California Hazelnut Experimentation Must Deal with Intrusive Species Need to initiate long-term maintenance commitment to the Landscape Costs! Wildland Urban Interface (WUI) and Camp Fire (11-22) # Kramer et al. 2019\nTwo types of WUI \u0026ndash; Wildland Urban Interface Interface WUI - \u0026ldquo;where houses meet\u0026rdquo;: 🏠🏠🏠🌲🌲🌲 Intermix WUI - \u0026ldquo;where houses mingle\u0026rdquo; (with vegetation): 🏠🌲🏠🌲🏠🌲 U.S. WUI was large in 2010: WUI Houses: 43.4 million = 33% of total Around 10% of US population Probably has grown in last 10 years WUI Area: 770,000 km2 = 9.5% of U.S. Concentrated in West U.S. WUI grew rapidly from 1990 to 2010: WUI Houses: 30.8 to 43.4 million \u0026ndash; 41% growth WUI Area: 581,000 to 770,000 km2 \u0026ndash; 33% growth Due primarily due to housing growth (wrt change in vegetation, or both) Destruction in WUI skewed and highly variable Destruction rates ranged between 0% and 100% (averaged 25% for fires with burned buildings) 70% of all destroyed buildings were in the top 20 most destructive fires Firewise communities Communities that have principles for fire Have evacuation / build material standards Are well located, but many are reactive only after wildfires Tends to be run by volunteers \u0026ndash; should aim to try and get more professions / advisors involved CA is a risky place; it\u0026rsquo;s contained\u0026hellip; 12 of the 20 most destructive fires in US 46% of buildings threatened by fire in US 60% of buildings destroyed by fire in US Fire insurance being cancelled Not all buildings are rebuilt In the US, 23% are rebuilt within 5 years In CA, 35% are rebuilt within 5 years In CA, 72% are rebuilt within 20 years (ranging from 13% to 100%) Big differences between affluent and non-affluent communities \u0026ndash; fire insurance Strong growth within all fire perimeters: 1981 Atlas Creek Fire - 65 homes burned 2017 Atlas Fire - 781 homes burned Extreme growth within Tubbs perimeter: For every home present in 1940, there were 26 homes in 2010 (2600% growth) Lessons from Tubbs Fire: Occurred due to eastern winds that traveled from high to low pressure, coastal areas Going downhill means faster winds thus hotter fires Additionally, winds can speed up as they move through canyons and mountain spaces Leads to reoccurring burned areas People likely won\u0026rsquo;t change behavior if incentives are not established Tubbs was highly destructive, and not the norm, but it also was not an anomaly: 6 of the 10 most destructive fires in CA have occurred since 2000 People are continuing to build in areas where fire risk is high and suppression is difficult Camp Fire \u0026ndash; Butte County Electrical power line started fire which the wind then carried Two ignitions related to power lines 85 people died, \u0026gt;18,000 homes lost over 149,000 acres Most expensive natural disaster in world in 2018 in terms of insured losses Mixture of ponderosa pine forests, woodlands, shrublands 2008 BTU Lighting Complex burned before Worst loss of life in a California wildfire Paradise had practiced evacuating Thought was one of most prepared cities Position of Paradise very hazardous with one road in and out Needs: Better community preparation and forest/woodland management, electrical infrastructure improvements Recommendations: Don\u0026rsquo;t reduce regulations to speed rebuilding Mandate good building codes Provide resources to people beyond a few years \u0026ndash; only 2/3 have rebuilt by 5 years Despite regulations on new building, the best incentive could be from insurance More engagement of public to prepare Australia a Leader in WUI: They have done a much better job of engaging people who live in the WUI Stephens et al. 2009, Gill and Stephens 2009\nIn U.S. mostly done by volunteer groups, better with UC extension employees Prepare, stay, defend, or leave early \u0026ndash; Australia Catastrophic fire danger level with climate change Updated Australia policy after 2009 Black Saturday Fires \u0026ndash; 174 people killed Updated Australia Policy PREPARE your family, home or business \u0026ndash; know your risk from bushfire and have a survival plan ACT on the fire danger ratings - put your preparations into action, do not wait to get message on your phone or tablet SURVIVE by monitoring conditions if a fire starts Know bushfire warning alert levels and what you will do if you are caught in a fire Summary # Exposure of human communities to fire in California is huge Communities need to prepare better for fire \u0026ndash; UC Cooperative Extension program Forest and woodland management could improve conditions \u0026ndash; TEK can assist Chaparral less options Electrical infrastructure needs improvement CA can learn from Australian experience Governor Newson\u0026rsquo;s and California\u0026rsquo;s next move? Class Summary # As an AC class, a major objective of this course is to examine diverse populations relationship to fire in California Today, while most of us fear wildland fires, that has not always been the case; Defining fire regimes for local regions of CA: fire frequency, fire season, fire area, fire severity, interaction with other factors; Scott’s Discussions of fire regimes for different CA ecosystems anthropogenic and natural fire regimes; Long-Term use of fire by Human populations: Important component of human evolution Constructive use of fire over hundreds of thousands of years for cooking, warmth, ceremonial use, etc. people bring fire with them when they colonized new landscapes and places across the globe, including California probably 13,000-15,000 years ago; Historical Fire Ecology: How we study the history of past fire regimes: Coring lakes to obtain charcoal, pollen, phytoliths Coring trees Wedges from trees for fire scar analysis Other lines of evidence employed: ethnohistorical accounts, ethnography, Native histories (oral traditions) Eco-archaeology (flotation to recover plant animal remains from archaeological sites) Cultural Burning in California California Indians have lived here more than 13,000 years Traditionally referred to as “Hunter-Gatherers” in anthropology literature,” but best to Refer to Them as “Resource Managers or Landscape Stewards”; Reasons for anthropogenic fires (control pests, remove detritus, open pathways, hunting, habitat revitalization to facilitate growth of tribal foods, medicines, dance regalia, raw materials) Regular, small-scale burning to enhance productivity, diversity, and sustainability of habitats – produce patchy mosaics Holocene times – California characterized byanthropogenic landscape, not “Pristine Wilderness” Controversy about scale and timing of anthropogenic/cultural burning: Central California – evidence for creation of coastal prairies by people 800-1400 CE (Rob Cuthrell, Valentin Lopez – eco-arch Research); Transformations with Colonialism: Significant impacts of Spanish, Russian, Mexican, and American (settler) colonization: altered fire regimes environmental degradation genocide of Native peoples Loss of land few reservations granted – mostly very small in size many tribes not federally recognized detrimental impacts of fire exclusion and later fire suppression Termination of Indigenous landscape stewardship practices on any major scale, but timing varied across state depending on history of colonization Some loss of Traditional Ecological Knowledge TEK (Val Lopez); The Advent of Fire Suppression: Debate about “Light Burning” in 1880s-early 1900s use of fire by sheep herders and by timber companies establishment of Forest Reserves, USFS (1905) influence of Gifford Pinchot (trained in Europe as discussed by Jameson Karns) Light Burning loses out to total fire suppression influence of 1910 Great Idaho Fire and other factors discussed by Scott fire suppression really develops in WWII and Post-war years Smokey the Bear, etc. Impact of Fire Suppression on local Ecosystems: Fire suppression methods very effective: 98-99% of fires out at less than 5 acres in size Major implications for taking fire out of ecosystem: increase fuel loads (ladder, surface, crown fuels), increase density of vegetation, fewer open areas, vegetation that is more homogeneous, encroachment of shrubs and trees, loss of grasslands, meadows, spring (Scott, Brandon Collins) Post-Suppression Times: Importance of Leopold Report in 1963 for National Parks and otherfederal and state agencies USFS, NPS and others experiment with prescribed burning, managed fires, and various fire proxy methods, including manual thinning, mechanized removal of vegetation, etc. (Jim Agee, Jan van Wagtendonk) In regard to prescribed burning, California nowhere near what Florida does. Much more needs to be done here! Tribal Revitalization in California - important concern of Tribes: Health of Environment, work to bring back Indigenous Stewardship Practices - ecological restoration of Tribal Lands - push to bring Cultural Burningand other stewardship practices back to the land: Five Case studies discussed: Timbisha Shoshone (Death Valley) - Chumash (Santa Barbara) - North Fork Mono Tribe (southern Sierra Nevada Mts.) - Karuk And Yurok Tribes (Northwest California) - Amah Mutsun Tribal Band (Central Coast of California); The Future: Rethinking how we manage open spaces, public lands, wildland/urban Interface in California: Use of Managed Fires where possible (Yosemite Case Study) Need much more prescribed burning to reduce fuel loads, develop fuel breaks, forest restoration, etc. Also employ manual thinning and mechanized vegetation removal when burning not a viable option; Need to bring back Indigenous landscape management practices, including strategic use of Cultural Burning (more nuanced treatment of specific resources) Create partnerships with Tribes and Federal and State Agencies Can we combine prescribed burning and cultural Burning in some areas of California? We think so and are working on it\u0026hellip; Proposal: Establish a Stewardship Corps for California that is dedicated to active ecosystem management Implement programs for training and certifying prescribed burn managers (like Florida) Fund stewardship corps work throughout year (prescribed/cultural burns in winter, spring, late fall) Create local/regional stewardship districts where corps undertakes regular maintenance of the land (like painting the Golden Gate Bridge) \u0026ndash; tailored to specific local ecosystems Composition of stewardship corps would include people from local tribes and members of local communities – create career Opportunities; New policies and regulations: fire insurance, zoning issues (try to decrease further development in Wildland/Urban Interface?) Mandate good building codes to fire-proof homes and surroundings – especially after major fires improve electrical infrastructure of California advocate for more community participation to prepare for fires increase funding for UC Extension to work with communities to better prepare for fires; "},{"id":57,"href":"/asamst-20a/","title":"ASAMST 20A","section":"Docs","content":" Week 1 (January 18 \u0026amp; 20): Introduction to the course. History, Memory, and Racialization # Reading: Tataki, Strangers from a Different Shore, Preface and Ch. 1: “From a Different Shore.”\nWhat is Asian American Studies?\nPanethnicity \u0026ldquo;pan\u0026rdquo; - a conglomeration \u0026ldquo;ethnicity\u0026rdquo; - belonging to a social group Panethnic identity example: Asian American Panethnicity Cultural Literacy One\u0026rsquo;s competency about their implicit biases Reading:\nAngel Island versus Ellis Island Angel Island was the west coast immigration entry point for people coming from the west (mainly Asian migration) Ellis Island was the entry point for many immigrrants from the east (mainly European) \u0026ldquo;Multiculuralism was a find of fear and optimism around how we are a very mixed country and what binds us together is this appreciation of different ethnic backgrounds, but we also assimilate under some cultural practice/viewpoint\u0026rdquo; - Michael Chang\nRon Tataki on Multiculturalism (side note - \u0026ldquo;Reasonable minds may differ\u0026rdquo;)\nWhat is \u0026ldquo;epistemology\u0026rdquo;: Theory of knowledge. Asks the question: \u0026ldquo;How do you know what you know?\u0026rdquo;\nRace is socially constructed\nThrough history socio-econmic, labor, and capital conflicts Any type of social norm is dependent upon contestation between people in the majority and minority. The power is asymmetric: Majority \u0026gt; Minority. Cultual Literacy\nIn studying Asian American history we are learning to understand how we have come to understand American history. What are the source we have been taught? What are the viewpoints presented in the curriculum we have been taught as history? How do we believe in cultural literacy, in that we are literate in a diverse range of viewpoints representing the true multicultural background of the United States? Arthur Schlesinger\u0026rsquo;s The Disuniting of America: Reflections on a Multicultural Country\nA very different view of multiculturalism: The concern of a \u0026ldquo;breaking apart\u0026rdquo; or \u0026lsquo;Balkanization\u0026rsquo; due to conflicts Association with multiculturalism with ethnic conflict Reassertion of national unity around assimilation paradigm The concern of the \u0026ldquo;race problem.\u0026rdquo; The \u0026ldquo;American Creed\u0026rdquo; Schlesinger on \u0026ldquo;disuniting\u0026rdquo; Two different viewpoints: Ron Tataki and Arthur Schlesinger\nDiscussion Questions:\nTakaki’s formulation of multiculturalism aligns with the concept of diversity, equity and inclusion of the 2000’s. How is Diversity related to multiculturalism? What does the term \u0026ldquo;American Exceptionalism\u0026rdquo; mean to you? Schlesinger was a proponent of “American Exceptionalism” the idea that America has provided safe harbor to many different groups of people. And that here in America, class, and lineage are not important. How do Asian Americans fit into these models of diversity, multiculturalism and \u0026ldquo;American Exceptionalism\u0026rdquo; Asian Americans fastest-growing electorate\nA naturalized citizen = someone who immigrated to the U.S. and gained citizenship “More than 11 million will be able to vote this year, making up nearly 5% of the nation’s eligible voters (for this analysis, U.S. citizens ages 18 and older). They are also the only major racial or ethnic group in which naturalized citizens – rather than the U.S. born – make up a majority of eligible voters, according to a Pew Research Center analysis of Census Bureau data.”\nMany Asian Americans have limited English proficiency image Aliens Ineligible for Citizenship and the 14th Amendment\n1790 Immigrration and Nationality Act (INA): Asian Americans were restricted from naturalized citizenship from 1790 into the 1940s and until the 1950\u0026rsquo;s depending on Asian ethnic groups. At the same time the 14th Amendment was passed in 1867 as an important legal tool against discrimination by state and local governments. Included an important clause stating that states cannot treat people differently on the basis of race (now it includes gender) \u0026ldquo;Simply stated, all persons must be treated equally without regard to their race, color, or national origin.\u0026rdquo;\nLegislation in 1965: Effectively ended the immigration restrictions and quotas against many Asian immigrants. Removed race and national origin as a restriction for naturalization. As you move through the readings\u0026hellip;\nConsider what \u0026ldquo;power\u0026rdquo;, \u0026ldquo;agency\u0026rdquo; and \u0026ldquo;contestation\u0026rdquo; mean in the context of the history that you reading. Those who are constructed in history as \u0026ldquo;minority\u0026rdquo; also have agency while those with majority power must contend with the contestationo of majority control. How did the process occur in the U.S.? What are the legal and socio-cultural institutions and practices that result in a given standard of a moment in time, and in change? Racial Formation\nMichael Omi \u0026amp; Howard Winant, Introduction, Racial Formation in the United States. 2014. HO Dynamic-constantly in flux Reproduction of political interests Aligns with the notion that within a diverse society, there will be multiple stakeholders (different groups and persons bringing forth different interests). The social construction around access to property and citizenship. Race is socially constructed: came out from social contestation, geopolitics, etc. Racial formation theory explains the fluidity of racial categories and the interest and forces that shape them. These pieces provide historical examples of how “white” identity came to be legally constructed through court cases and through the socio-economic forces supporting these legal outcomes. Do the courts drive the outcome? Legal institions? 1690: the brewing slave trade 1790: contestation around what “white” meant. Before the slave trade was active, the primary source of labor was young Irish women. Came as indentured servants (a form of labor in which a person is contracted to work without salary for a specific number of years). Example of instability around what being “white” meant. Sociologists Omi and Winant describe the fluidity of racial constructions over time dependent on political interests and socio-economic forces, as \u0026ldquo;racial formation.\u0026rdquo; What is Omi and Winant\u0026rsquo;s concept of \u0026ldquo;common sense\u0026rdquo;? “Common sense” derives from Italian philosopher and political economist Antonio Gramsci’s theories on asymmetry in political power and the interest conflicts that may emerge as a result. I.e. today’s “Common sense” surrounding gender is that gender is a social construct as opposed to one’s sex. Gramsci wrote about hegemony as in “how does power operate when there is one party that maintains much of the mechanisms of culture and state?” You have agency. A consensus that results from an agreement (not so much voluntary, but through negotiation) What is a \u0026ldquo;Race\u0026rdquo; versus an \u0026ldquo;Ethnicity\u0026rdquo;? Consider the challenge of formally categorizing groups:\nThe U.S. Census Bureau uses the term \u0026ldquo;Hispanis\u0026rdquo; Who are \u0026ldquo;Hispanics\u0026rdquo;? Who ae \u0026ldquo;Latinos\u0026rdquo;? Is \u0026ldquo;Asian AMerican\u0026rdquo; a race descriptor? Is \u0026ldquo;Hmong American\u0026rdquo; a race or an ethnicity descriptor? Taki referred to the popular perception of Asian Americans as:\nModel Minority: A trope that gained traction in the 1960s that stresses the prototypical Asian’s high achievement in socioeconomic status and focuses on their culture to explain their “success”; denies racism and other hardships that are experienced by Asian Americans; comparison between one minority group and another Perpetual Foreigner: stereotype in which naturalized and even native-born citizens are perceived by some members of the majority as foreign because they belong to a minority ethnic or racial group. Twin pillars of how Asian Americans are constructed\nHow do these sterotypes and categories affect cultural and legal outcomes for Asian Americans?\nThe Social Construction of Race\nThe idea that race is socially constructed emerged as a response to the view that race is based on biology What does it mean to say that race is not based on biology? Does it mean that there are no biological or genetic bases to difference? What is the difference between difference and race? Blood quantum: the idea that if you have any drop of African American ancestry, in that jurisdiction/state, you are legally classified as black For whites to maintain racial purity, property, power, and perpetuated the classification/segregation of people on the basis of race Class Question: How has federal jurisdiction driven civil rights protections from the time of early Asian immigrants to the U.S. to the 2020s?\nFor example, the Interstate Commerce Act addressed the problem of railroad monopolies by setting guidelines for how the railroads could do business. Insiders and Outsiders\nAs cited in footnote 4 of the U.S. Supreme Court\u0026rsquo;s Carolene Products the Tainted Milk, case: “prejudice directed against discrete and insular minorities may call for “more searching judicial inquiry.”\nEnsured that former slave-holding states would abide by this 14th amendment that said that African Americans had to have citizenship rights Created a legal authority when the former slave-holding states tried to reinstitute some form of slavery Racial formation construction? Sojourners vs. Immigrants\nLabor and capital interests drove demand for cheap labor during U.S. industrialization Push and Pull\nGeopolitical political economic factors Alienation of Migration Guangdong Chinese Women sang\nDear husband, ever since you sojourned. In a foreign land. I\u0026rsquo;ve lost interest in all matters. All day long, I stay inside the bedroom, My brows knitted; ten thousand Thoughts bring me endless remorse, in Grief, in silence. I cannot fall asleep on my lonely pillow.\n1882 Chinese Exclusion Act\nRacialized labor conflict and competition The main landmark that stems from aliens ineligible for citizenship Agency, Resistance and Contestation\nYick Wo v. Hopkins, USC 1996 Provides the basis for the important legal evidence standard of \u0026ldquo;disparate impact\u0026rdquo; Supreme Court said that San Francisco zoning law is in violation of the 14th Amendment 120,000 Americans of Japanese descent on the Western U.S. seaboard were sent to internment camps. 75% of internees were American citizens by birth.\nWeek 2 (January 25 \u0026amp; 27): The racialization of Asian Americans: contemporary images and historical transformations. # Reading: Okihiro, Common Ground, Preface \u0026amp; Ch. 2: “White and Black.”\nClass Question: Okihiro writes in his preface that his book Common Ground is about the \u0026ldquo;creation of the American character and subject.\u0026rdquo;\nWhat does he mean by that?\nthe social construction of the nation Binaries as convenient cognitive and socio-political constructs\nBinaries offer coherence, especially during times of social upheaval. They preserve rule amidst apparaent chaos, and stability amidst rapid change, such as during the late eighteenth, nineteenth, and twntieth centuries (i.e industrial revolution, enlightenment, romanticism). Those periods of American history occasioned social reconstitutions of geographies, race, gender, sexuality and nationality that helped to define and regulate identities, the state, and the social formation. Break out question: What are some modern-day binaries and what socio-political institutions or relationships and hierarchies do they support?\ngender norms, environmental conservation, wealth inequality What are some binary ideas or myths based on the social construction of an East vs. West divide?\nRudyard Kipling The Ballad of East and West, 1889 Oh, East is East, and West is West, and never the twain shall meet, Till Earth and Sky stand presently at God\u0026rsquo;s great Judgment Seat; But there is neither East nor West, Border, nor Breed, nor Birth, When two strong men stand face to face, though they come from the ends of the earth!\nThese poems pervade our perceptions and are descriptors for historical moments in time. They are important signifiers of binaries Model Minority vs. Perpetual Foreigner\nAre these two poles of the social construction of Asian Americans binary? How do they serve to construct the \u0026ldquo;Asian American subject?\u0026rdquo; Korematsu v. United States\n\u0026ldquo;In response to the Japanese attack on Pearl Harbor during World War II, the U.S. government decided to require Japanese-Americans to move into relocation camps as a matter of national security. President Franklin Roosevelt signed Executive Order 9066 in February 1942, two months after Pearl Harbor. A Japanese-American man living in San Leandro, Fred Korematsu, chose to stay at his residence rather than obey the order to relocate. Korematsu was arrested and convicted of violating the order. He responded by arguing that Executive Order 9066 violated the Fifth Amendment. The Ninth Circuit affirmed Korematsu\u0026rsquo;s conviction.\u0026rdquo; - Oyez\nConclusion: The Court ruled against Korematsu. The Court believed that the decision was valid because it was a necessity amid the danger of espionage and sabotage. Common ground vs. Margins as the Mainstream\nOkihiro writes that his book seeks to reject binaries and to advocate for a \u0026ldquo;open, borderless\u0026rdquo; common ground\u0026quot; versus his prior argument that the mainstream norms, rights, values of America were created by the margins. meaning that the margins create the mainstream. Race Conscious Judicial Review and Black/White Paradigm\nRace relations in the United States have rapidly evolved in recent decades however, race scholars often point to a persistent paradigm that views racial difference primarily through the construction of black and white relations. Binarism and Black White as a paradigm\nOkirhiro says that the American race paradigm views race relations in terms of black and white. Separate but Equal: Plessy v. Ferguson (1896)\nLouisiana\u0026rsquo;s Separate Car Act challenged under the 14th Amendment \u0026ldquo;Louisiana enacted the Separate Car Act, which required separate railway cars for blacks and whites. In 1892, Homer Plessy – who was seven-eighths Caucasian – agreed to participate in a test to challenge the Act. He was solicited by the Comite des Citoyens (Committee of Citizens), a group of New Orleans residents who sought to repeal the Act. They asked Plessy, who was technically black under Louisiana law, to sit in a \u0026ldquo;whites only\u0026rdquo; car of a Louisiana train\u0026hellip;When Plessy was told to vacate the whites-only car, he refused and was arrested\u0026hellip;Plessy was convicted.\u0026rdquo; - Oyez\nConclusion: The Court declared the state law constitutional. In essence, segregation did not constitute unlawful discrimination. Constructive blacks\nLaw scholar Frank Wu argues that our legal system construes \u0026ldquo;racial groups as white, blacks, \u0026ldquo;honorary whites or constructive blacks.\u0026rdquo; What does \u0026ldquo;constructive\u0026rdquo; mean? The legal system has treated Asians, mostly as blacks. Applied the same rules to Asians as they would to Blacks. He says historically, the U.S. legal system has treated Asians mostly as \u0026ldquo;constructive blacks.\u0026rdquo; People v. Hall, 1854 (CA state court attributed black status to Asians in enforcing a statue (passed by a legislative body) prohibiting the testimony of blacks, reasoning that \u0026ldquo;blacks\u0026rdquo; was a generic term for all nonwhites). Gong Lum v. Rice, 1927 (USC upheld segregated public schools for Asians connoting no difference between segregation for blacks to those of the \u0026ldquo;yellow races\u0026rdquo;). 1790 Immigration and Naturalization Act revisited\nHeld that only \u0026ldquo;free white persons\u0026rdquo; of \u0026ldquo;good moral character\u0026rdquo; could become naturalized citizens. After the Civil War, African Americans who were former slaves, gained official citizenship status. Ozawa v. U.S., 1922 (USC ruled Japanese American were not white for naturalization but lauded performance of white middle class attributes such as attending Cal, Church, and speaking English at home) U.S. v. Thind., 1923 (USC ruled Asian Indian Immigrants not white for naturalization, noting that though anthropologists deemed Indians \u0026ldquo;Aryan\u0026rdquo; this was not the same in the common sense understanding of a white person) Racial bar for naturalization not lifted until the Immigration and Naturalization Act of 1952 (McCarran-Walter Act), and abolished the race restriction of the 1790 INA, though it retained a quota system for nationalities and regions. Are Asians White or Black? Do the Right Thing, Spike Lee (1989)\nAsians as \u0026ldquo;Middle Man minority\u0026rdquo; i.e. Your grandparents may have businesses in neighborhoods that are African-American or Latinx that are in the grey-economic zone. Asians as Other Non-Whites: Social construction of race\nSystematic racism vs. Individual discrimination Rapid change in federal position over a matter of months Trump Executive Order on Diversity Biden Executive Order on Diversity Biden Executive Order on Asian Americans The 1992 Los Angeles Uprising/Riots L.A. Burning: The Riots 25 Years Later\nThe filmed video of African American Rodney King beating was probably on of the first \u0026ldquo;viral video\u0026rdquo; of police excessive force with serious consequences for race relations. Days after the videotaped beating of Rodney King, a Korean American grocery shop owner, Soon Ja Dul, shot and killed a 15 year African American who she mistook for stealing, and grabbed her sweater and backpack and Harlins punched her. Soon Ja Dul went behind the counter to retrieve a shotgun and shot Harlins in the back of her head. Following the acquittal in April of 1992, of Los Angeles Police Department officers on trial for the beating, Los Angeles experienced six days of civil unrest that resulted in the burning of many businesses, including those in Korea town which is located in and near historically black neighborhoods. The Harlins killing has also been attributed as contributing to civil unrest that occurred after the acquittal. Sa I Gu Sa I Gu (official full version)\nA very specific view of the 1992 Los Angeles Uprising/Riots from the perspective of (mostly) Korean American women who were shop owners Afterthoughts: Where do the views of the witnesses in this movie sit? Can we square the views of the Korean American witnesses here with one of diversity and equity? What racial tensions or divides might exist here between KA and African Americans? Is this a race issue or a class issue? Is this a race issue or an immigration issue? The Cold War Construction of the Model Minority Myth\nWhat does \u0026ldquo;model minority\u0026rdquo; mean? Why Do We Call Asian Americans The Model Minority? | AJ+ Black/White paradigm: Does the \u0026ldquo;model minority\u0026rdquo; stereotype support the black/white paradigm? How does it support an assimilationist paradigm? Scholar Robert Lee says that the stereotype supports a view of minorities compliance with \u0026ldquo;accommodation\u0026rdquo; to assimilationist and status quo norms rather than militancy. Lee describes this as a \u0026ldquo;disjuncture between the newly articulated ideals of racial egalitarianism and the practice of racial discrimination\u0026rdquo; as evidence by the USC\u0026rsquo;s decisions in the Japanese interment cases. Disjuncture?\nWhat does Lee mean by this disjuncture? How is this supposed \u0026ldquo;disjuncture\u0026rdquo; exemplified by Cold War politics? 1966 NYT\u0026rsquo;s Magazine article on \u0026ldquo;Success Store: Japanese American Style,\u0026rdquo; and in December of that year, \u0026ldquo;Success Story of One Minority in the U.S.\u0026rdquo; At a similar moment, U.S. Senator Daniel Patrick Moynihan published the influential Report on the Black Family (1965) which was an attempty to use data and sociological methods to understand poverty and income inequality in the African American community but which used the term \u0026ldquo;tangle of pathology\u0026rdquo; and referenced single parent families in the African American community as a source of the generational poverty. How are these relevant to this idea of \u0026ldquo;disjuncture\u0026rdquo;? Week 3 (February 1 \u0026amp; 3): U.S.-Asia relations and early immigration. # Discussion topics:\nWere al the national/ethnic groups who are considered \u0026ldquo;white\u0026rdquo; today considered \u0026ldquo;white\u0026rdquo; in the 1800\u0026rsquo;s? What was the connection between a person\u0026rsquo;s labor/class status and their racial identity according ot Tataki? What does it mean to \u0026ldquo;perform\u0026rdquo; a racial identity. Are you just who you are racially? Are race relations in the U.S. based on a black and white framework? How do Asian Americans fit into a black and white framework? How do Asian Americans not fit into a black and white framework? What is legal discrimination? What is \u0026ldquo;disparate impact\u0026rdquo; versus \u0026ldquo;intentional motivation\u0026rdquo; or \u0026ldquo;animus\u0026rdquo;? What is \u0026ldquo;color-blind\u0026rdquo; legla doctrine? Overblown with Hope\nGam Saan = \u0026ldquo;gold mountain\u0026rdquo;\n1848 shortly after the discovery of gold at John Sutter\u0026rsquo;s Mill, a young man in Canton China wrote to his brother in Boston, \u0026ldquo;Good many Americans speak of California. Oh! Very rich country! I hear good many Americans and Europeans go there. Oh! They find hold very quickly, so I hear \u0026hellip; I feel as if I should like to go there very much. I think I shall go to California next summer\u0026rdquo;\nImmigrant Labor\nContract Laborers Emigration brokers representing sugar planters provided \u0026ldquo;free passage\u0026rdquo; where immigrants would sign labor contracts with a planter for 5 years with wages, shelter, food and medical care. Free Laborers A credit-ticket system, a broker lends money to a migrant for the ticket for passage. The migrant would pay off the loan with interest out of his earnings in the U.S. Chung Kun Ai on money lending\n\u0026ldquo;One condition of his loan of $60 was that each borrower was to pay back $120 as soon as he was able to do so. In all, grandfather must have helped 70 young men from our village and nearby villages to migrate to North and South American and also Australia.\u0026rdquo;\nGender and Migrant Labor: Chinese traditional Culture, Labor and Capital Strucutres, Geopolitics, and race\nHow did traditional Chinese culture affect the gender demographics of Chinese labor migrants? the wife was in a subordinate positon the wife served as an incentive for their husband to return Hawaii vs. U.S. Mainland\nHawaii encouraged the husband to bring their families over, whereas the U.S. Mainland (California) only wanted the men. Complexity in Chinese immigrants\nHakka did not practice footbinding Hawaii made some efforts to promote migration of Chinese women. \u0026ldquo;A Race so Different from Our Own\u0026rdquo; - Justice Harlan, dissenting, Plessy v. Ferguson\nSegregation under the law\nCitizenship Rights are the primary pivot for access to recognition by government and thus rights attached to citizenship Segregation upheld only separation but negative status associated with slavery Randall Kennedy on One Drop Rule and the complexity of Black identity Early Segregation Historical Milestones\n1790: Immigration and Naturalization Act 1857: Dred Scott v. Sandford 1862: Emancipation Proclamation Executive Order 1865: Civil War ends 1882: Chinese Exclusion Act 1886: Yick Wo v. Hopkins 1889: Chae Chan Ping v. U.S. 1896: Plessy v. Ferguson Dred Scott v. Stafford (1857)\nThe Court denies citizenship on the basis of deference to the political branches. Pre-Civil War the Supreme Court reified negative status of Blacks Court holds Blacks are not citizens and are thus subject to legislative laws on slaves as chattel (property) Plessy v. Ferguson\nA legal caste system based on race is created on a fiction of separate but equal. Post-Civil War Supreme Court, holds that segregation of African Americans and Whites does not violate the Fourteenth Amendment. Harlan’s Dissent raises the contradiction of assumedly not segregating Chinese who are excluded from naturalized citizenship while segregating of African Americans who “fought for the union.” Yick Wo v. Hopkins\nA theory of evidence for systemic discrimination is created. Supreme Court established disparate\u000boutcome (impact) as basis for establishing racial discrimination. Chae Chan Ping v. United States\nSupreme Court held that Chinese Exclusion Act of 1882, passed while Chinese man was abroad, applied and revoked his re-entry permit) With law the past is always in the present\nWhy is Chae Chan Ping relevant today? Plenary Power Doctrine\n\u0026ldquo;The plenary power doctrine protects the federal government from claims that it is violating an individual\u0026rsquo;s constitutional right to equal protection when it imposes discriminatory burdens on non-US citizens.\u0026rdquo; - CUNY School of Law i.e. Congress has \u0026lsquo;planery power\u0026rsquo; over who can enter and exit the U.S. Demographic Complexity/An Explosion of Immigrants\nAsian Immigrants (incomplete) 370,000 \u0026ldquo;arrivals\u0026rdquo; of Chinese to Hawaii and CA from late 1940\u0026rsquo;s and early 1880\u0026rsquo;s. 1880\u0026rsquo;s to about 1910: 400,000 \u0026ldquo;arrivals\u0026rdquo; of Japnese to Hawaii and the West Coast. 1900-1933: 7,000 \u0026ldquo;arrivals\u0026rdquo; of Korean, 7,000 Asian Indian, 180,000 Filipinos to Hawaii and mainland. European Immigrants 1850-1930: 35 million European immigrants. Geoolitics and Push and Pull\nColonialism and internal instability European colnial involvement American colonial involvement Internal instability in the modern nation-state era Labor and Capital Racialization of Labor allegations of unfair wage and job competition increases. War and internal instability in China\nAnglo-Chinese War of 1856-60 Piracy change against British citizen leading to war Ports and missionaries Taiping Rebellion 1850-1864 Christianized Chinese believing brother of Jesus led rebellion across Central and South China, impacting Guangdong in particular. Estimated 10 million deaths. Inequity at home tied to colonialism pushes and pulls Filipinos out\nSpanish-American War of 1898; and resultant Treaty of Paris resulted in U.S. possession of Phillipines. Takaki writes that Filipino peasants found that the rich rice lands they cultivated were becoming owned by wealthy men, turning them into sharecroppers. Immigration Laws and Challenges\nLabor and Capital As Chan notes, colonial administrators needed cheap physical labor for their projects, thus the migrants to such colonies were disproportionately young men. In the U.S. mainland context this resulted in labor competition with European Americans in particular, in the immediate post-Civil War era. Legal Outcomes The Chinese Exclusion Act triggered a range of challenges by Asian Americans, Chinese in particular early on, supported by funds from co-ethnic organizations such as the Chinatown based Chinese Benevolent Association. Citizenship: Assimilation vs. Segregation\nWhat does the exclusion exeprience of Asian Americans (early Chinese in particular) from citizenship say about the segregation of African Americans and vice versa? A \u0026ldquo;neutral principle\u0026rdquo; in the \u0026ldquo;Rule of Law\u0026rdquo; and the assertion of Fourteenth Amendment protections for Carolene Products \u0026lsquo;discrete and insular minorities\u0026rsquo;\u0026rdquo;\nNeutral Principle \u0026ldquo;Rule of Law\u0026rdquo;: Often stated as a broader checks and balances based principle with strong consideration of the authirity of each branch as well as the limitations on such authority. Historically asserted by the judicial branch to avoid ruling on \u0026ldquo;political\u0026rdquo; questions. Fourteenth Amendment As a tool to remedy discrimination, (specifically race early in U.S. history). Carolene Products\u0026rsquo; asserted view that allegations of discrimination against \u0026ldquo;discrete and insular minorities\u0026rdquo; requires close scrutiny by courts. This lead into the standard applied in Korematsu (1944). The American Federation of Labor\u0026rsquo;s \u0026ldquo;Meat vs. Rice: American Manhood vs. Asiatic Cooliesim\u0026rdquo;\nLabor and Capital Conflict Social Construction of Race, Gender, and Class Close tie between rise of American unionism and trade associations with racialization through Samual Gompers-elected first president of the AFl in 1886. AFK-CIO June 17, 2020 \u0026ldquo;Condemning and Combatting Anti-Asian Racism\u0026rdquo; statement\n\u0026ldquo;As a labor movement, we must acknowledge our own painful past on this and other racial justice issues. In 1882, then-American Federation of Labor (AFL) President Samuel Gompers pushed for the passage of the Chinese Exclusion Act, citing the dangers of \u0026ldquo;Asiatic\u0026rdquo; men in an essay published by the AFL and submitted as Senate testimony. Fear among White workers propelled this legislation, the first to bar an entire race from legslly entering the United States.\nThe \u0026ldquo;Citizenship Clause\u0026rdquo;\nSection 1: \u0026ldquo;All persons born or naturalized in the United States and subject to the jurisdiction thereof, are citizens of the United States and of the State wehrein they reside.\u0026rdquo; The Fourteenth Amendment\u0026rsquo;s \u0026ldquo;Citizenship Clause\u0026rdquo;: Asian American citizenship rights tied again to slavery and citizenship rights\nPrior to the 14th Amendment\u0026rsquo;s passage on June 8. 1866: Dred Scott v. Sandford (1857), triggered federal legislative response with Civil Rights Act of 1866, asserting all persons born in U.S. or naturalized were citizens of U.S. and residence state Wong Kim Ark challenges his citizenship exlcusion 1898. Born in the USA: Wong Kim Ark, 1898 Another Chinese Exlcusion Act Case\nGiven the Chinese Exclusion Act of 1882, was a born in the USA person of Chinese descent a citizen? \u0026ldquo;Because Wong was born in the United States and his parents were not “employed in any diplomatic or official capacity under the Emperor of China,” the Citizenship Clause of the Fourteenth Amendment automatically makes him a U.S. citizen.\u0026rdquo; - Oyez\nDiscussion: Week 4 # Major Themes\nEarly Asian Immigration/Emigration Influx of Asians to the U.S. start during the mid-19th century 370,000 Chinese (early 1840s - late 1940s) 400,000 Japanese (1880s) 7,000 Koreans (early 1900s to mid-1900s) 7,000 Asian Indians (early 1800s) (incomplete) Push and Pull Factors\nBy definition: factors that caused large groups to emigrate out of a country and or immigrate to another country. China Push Economic Instability: First Opium Wars (1839 - 1842) Second Opium Wars (1857 - 1859) Internal instability Taiping Rebellion Natural Disasters: Drought in Henan Province (1847) Flooding of Yangtze River Famine in Guangdong (incomplete) Japan\nPush: Economic instability Pull: Job opportunity (i.e. farming, fishing) Philippines\nPush: Internal Instability (i.e. Spanish-American War of 1898 and the Treaty of Paris) Pull: Economic opportunities (incomplete) U.S. taking advantage of early Asian migrants\nCheap labor and capital mostly men cheap physical labor labor competition with European Americans U.S. - Asia Relations\nIn early 19th century, white nativists spread xenophobic propaganda -\u0026gt; Chinese Exclusion Act First law in the U.S. that barred immigration based on race With World War II, President Roosevelt passed Executive Order 9066 Japanese Americans were incarcerated, many of whom were naturalized citizens Current Event 1: How Covid-19 reignited long-standing xenophobia against Asians (Specifically Chinese)\n\u0026ldquo;Forever Foreigner\u0026rdquo; stereotype persists Anti-Chinese Rhetoric from political leaders fuels the fire Violence rates against Asians rose after Covid-19 hit the US (incomplete) Current Event 2: The Model Minority Myth of Asian-American Workers Today\nMany Asian-Americans in workforce; less in higher positions \u0026ldquo;Bamboo Ceiling\u0026rdquo; - glass ceiling for Asian Americans Model Minority Myth: Asians painted as hardworking, smart and faithful but also deeemed workers who lacked the ambition, creativity and confidence that being a leader requires Where do we go from here? Change the qualities of what someone in leadership looks like Give more positions to Asian-Americans What happens when there are more Asian-Amerians in the workplace? Studies show a better environment Current Event 3: Biden-Harris Administration Advances Equality and Opportunity for Asian American, Native Hawaiian, and Pacific Islander (incomplete)\nAdvancing safety for Asian Americans Combating hate and violence COVID-19 Hate Crimes Act Helps local and state law enforcement accurately report hate crimes to the FBI Helps combat the potential language barrier stopping AAPI community members from reporting hate crimes Research and Education (NSF) Advancing Immigration Reform United States Citizenship Act Promoting naturalization Addressing the backlog for U Visa Petitioners Week 4 (February 8 \u0026amp;10): Chinese in the labor market. The context for exclusion: broad changes in the economy and polity of the United States. # The Context for Exclusion\nLabor and Capital Conflicts Intersectional across, race, class, and gender and situated in geopolitics and a pre-existing domestic race politics Early Segregation Historical Milestones\n1790: Immigration and Naturalization Act 1857: Dred Scott v. Sandford 1862: Emancipation Proclamation Executive Order 1865: Civil War ends; Start of Reconstruction 1868: 14th Amendment 1875: Page Act 1877: End of Reconstruction 1882: Chinese Exclusion Act 1886: Yick Wo v. Hopkins 1889: Chae Chan Ping v. U.S. 1896: Plessy v. Ferguson Revisiting: Citizenship: Assimilation vs. Segregtion\nWhat does the exclusion experience of Asian Americans (early Chinese in particular) from citizenship say about the segregation of African Americans and vice versa? Fourteenth Amendment 1868 Reminder\nAll persons born or naturalized in the United States, and subject to the jurisdiction thereof, are citizens of the United States and of the State wherein they reside. No State shall make or enforce any law which shall abridge the privileges or immunities of citizens of the United States; nor shall any State deprive any person of life, liberty, or property, without due process of law; nor deny to any person within its jurisdiction the equal protection of the laws Where does power lie?\nAgency and contestation by “discrete and insular minorities” Revisiting: A ”neutral principle” in the “Rule of Law” and the assertion of Fourteenth Amendment protections for Carolene Products’ FN 4 ”discrete and insular minorities”\nNeutral Principle \u0026ldquo;Rule of Law\u0026rdquo;: Often stated as a broader checks and balances based principle with strong consideration of the authority of each branch as well as the limitations on such authority. Historically asserted by the judicial branch to avoid ruling on ”political” questions. Fourteenth Amendment As a tool to remedy discrimination, (specifically race early in U.S. history). Carolene Products’ asserted view that allegations of discrimination against “discrete and insular minorities” requires close scrutiny by courts. This lead into the standard developed in Korematsu (1944). What Happened to the Women?\nRace and sex sterotypes; Asian women A long history of Asian American (women) activists\nHow does history socially construct mainstream understandings of what it means to be an Asian American female? Prof. Um and Fujino on Asian American activism: Centering Women as Active Subjects of History\nHune writes that one constructive approach to situating Asian American women in history is to view them “as active participants in history and agents of social change, negotiating complex structures of power” Alternative spaces where agency was exercised\nDominant historiography of Asian women focused on issues of work, control of labor by the husband’s family and immigration restrictions but other reasons explain absence of Chinese and Indian women in the U.S. They had “separate lives” with ties to kin, friends, and their own cultural worlds important to them which has been obscured by the dominant telling of history. Little is known; Focus on the family unit as the \u0026ldquo;normative model\u0026rdquo; of migration should be questioned\nMigration of women along with their men “was the exception rather than the rule” Social Construction of Asian Women as “Orientalized Chinese women as passive victims - of culture and patriarchy.” - Hune writes that little is known about the lives of Cantonese, Punjabi migrant women that is likely because many of our images of these women’s lives come from the nine-tenth century writings of European-American missionaries.\nMale preference cultures in China and Punjab \u0026amp;\u000bDivergent economies of the home counties created complexity for class status of women affecting female migration\nEvolving patriarchal relations at a time when migrants from a range of counties (Shunde, Zhongshan, Panyu and Nanhai) became merchants in the Chinese American community while “Sze Yup migrants were often laborers and domestics.” Chinese women presumptively sex workers\nPage Act of 1875 (Sect. 141, 18 Stat. 477, 3 March 1875) A pre-Chinese Exclusion Act exclusionary law specifically based on gender Chinese women immigration early on; Prostitution as ”yellow slavery”\u000bRace, culture and religion\nPage Act introduced by Representative Horace Page to \u0026ldquo;end the danger of cheap Chinese labor and immoral Chinese women” barring “undesirable” immigrants defined as: Forced laborer East Asian women engaged in prostitution Convicts Immigration review and requirements in Hong Kong Wives vs. Prostitutes\nAmerican consul reviewed background of Chinese women applying for immigration from Hong Kong with document and questions: Photographic Identification Official declaration of purpose of emigration and personal morality statement. Review by hospital staff for character review. A multi-level review process from American consul in HK officials back to American consul day of departure. Questions about who fathers and husbands were. Break out Questions\nWas the Page Act protective of trafficked Chinese women or was it exclusionary? What were the domestic and geopolitical factors that played a role in Chinese female sex workers and concubines? What are the factors that may have driven the concern around \u0026ldquo;yellow slavery\u0026rdquo; Class Question\nHow did political and socio-economic conflicts in Asia affect the migration of South Asians to the U.S.? How did political and socio-economic conflicts in Asia affect the migration of Chinese to the U.S.? Hostility and Conflict; Racism and Nativism\nChan notes 7 types of hostility against Asian Americans: Prejudice: preconceived notions based on stereotypes ~ discrimination Economic Discrimination Political Disenfranchisement: often references voting rights Physical Violence Immigration Exclusion Social Segregation Incarceration Geopolitical Drivers to Negative Stereotypes of Chinese\nChan references Stewart Creighton Miller three groups of Americans who\u0026rsquo;s interactions in China propagated sterotypes of Chinese: Diplomats resenting protocols of Chinese Court Merchants upset on limitations for freedom of trade Missionaries concerned about slow rate of Chinese conversion to Christianity Range of CA law excluding Chinese: Criminal Proceedings Act of 1850\nCriminal Proceedings Act of 1850 (later covered civil as well) exclusing testimony of Blacks, \u0026ldquo;Mulattos\u0026rdquo; and Native Americans. \u0026ldquo;The 14th section of the Act of April 16th, 1850, regulating Criminal Proceedings, provides that \u0026lsquo;No black or mulatto person, or Indian, shall be allowed to give evidence in favor of, or against a white man.\u0026rdquo; People v. Hall. CA. (1854)\nWhite prospector killed Chinese miner. While prosecuted for the murder, on appeal the conviction was overturned on the basis that Native Americans came over the Bering Strait from Asia and therefore were \u0026ldquo;Asiatics.\u0026rdquo; Thus, the 1850 Criminal Proceedings Act Applied to \u0026ldquo;the whole of the Mongolian race.\u0026rdquo; This prohibited all non-Whites from testifying against Whites not changed until the 1870\u0026rsquo;s. Judicial Rationale in People v. Hall\nincomplete Chinese Massacre of 1871 Massacre, Los Angeles\nincomplete Rock Springs MAssacre, September 2, 1885\nRock Springs Utah, massacre, 1885: Union Pacific railroad hired former Chinese railroad workers for coal mining work for less wages than White labor. Labor Organization and Race\nKnights of Labor Mainstream history on labor history often passes over its origins in race and immigration exclusion "},{"id":58,"href":"/physics-7b/26/","title":"26: DC Circuits","section":"Physics 7B","content":" 26.1 EMF and Terminal Voltage # EMF # To have a current, we need an emf (electromotive force) device to transform one type of energy (chemical, mechanical, light) into electric energy The term “electromotive force” is a misnomer: it does not refer to a “force,” which is measured in newtons. To avoid confusion, we use the abbreviation, emf. EMF of the Source: The potential difference between the terminals of a source when no current flows to an external circuit .$\\mathscr{E}$ is used for emf and it\u0026rsquo;s units is (V)olts Batteries # Batteries don\u0026rsquo;t have constant current (it varies with resistance of the circuit) Voltage is nearly constant, but decreases when battery cannot supply charge fast enough to maintain full emf This occurs because the charge must move between/through the electrodes in the battery Additionally, the battery has some internal resistance, .$r$ Batteries are treated as a perfect emf .$\\mathscr{E}$ in a series with a resistor .$r$ The terminal voltage is .$V_\\text{ab} = V_a - V_b$ When a battery is being charged, a current is forced to pass through it; we then have to write .$V_\\text{ab} = \\mathscr{E} + Ir$ When no current is drawn, .$V_\\text{ab} = \\mathscr{E} - Ir$ .$Ir$ comes from the fact that when .$I$ flows from the battery it causes an internal voltage drop .$Ir$ Since .$\\mathscr{E} - Ir = IR \\Longrightarrow \\mathscr{E} = I(R+r)$ for .$R$ as the resistance of the circuit 26.2 Resistors in Series and Parallel # Series # Any charge that passes through one resistor passes through all Hence, the same current .$I$ passes through each too (constant) If this wasn\u0026rsquo;t true, then it would imply the charge was not conserved Voltage from the battery is split between each resistor proportional to .$R$ $$V = V_1 + V_2 + \\dots = I R_1 + I R_2 + \\dots = I(R_1 + R_2 + \\dots)$$ Thus, .$R_\\text{eq} = R_1 + R_2 + \\dots$ Note that when you add more resistance to the circuit\u0026hellip; The current that passes through each resistor decreases The equivalent resistance increases Voltage stays the same since the battery is unaltered Parallel # Current is split from the source path into branches Thus, paths outside of one\u0026rsquo;s branch doesn\u0026rsquo;t impact/interrupt current The current from each branch must equal the total current; i.e $$I = I_1 + I_2 + \\dots$$ Voltage across each resistor is equivalent; $$I_1 = \\frac{V}{R_1}, I_2 = \\frac{V}{R_2}, \\dots \\Longrightarrow I_\\text{eq} = \\frac{V}{R_\\text{eq}}$$ Thus, .$R_\\text{eq}$ is equal to $$ \\frac{1}{R_\\text{eq}} = \\frac{1}{R_1} + \\frac{1}{R_2} + \\dots$$ Note that when you add another resistor to the circuit\u0026hellip; Net resistance goes down Adding another resistor adds another path causing current to decrease Voltage stays the same since the battery is unaltered Consistent with .$R = \\rho l/A$ definition of resistance Series is effectively increasing the length Parallel increases the area through which current flows 26.3 Kirchhoff\u0026rsquo;s Rules # We use Kirchhoff\u0026rsquo;s two rules when circuits get too complex for trivial analysis\n1. Junction Rule: At any junction point, the sum of all currents entering the junction must equal the sum of all currents leaving the junction. That is, what goes out must come back in Based on conservation of electric charge Mathematically, $$\\sum_{k=1}^{n} I_k = 0$$ .$n$ is the total number of branches with currents flowing towards or away from the node. The current entering any junction is equal to the current leaving that junction: .$I_2 + I_3 = I_1 + I_4$\n2. Loop Rule: The sum of the changes in potential around any closed loop of a circuit must be zero. That is, what goes up must come back down There is as much up as there is down At the battery, the gain/loss on each terminal cancel one another along the closed circuit path Based on conservation of energy Mathematically, $$\\sum_{k=1}^{n} V_k = 0$$ .$n$ is the total number of voltages measured. The sum of all the voltages around a loop is equal to zero: .$V_1 + V_2 + V_3 + V_4 = 0$\n26.4 EMFs in Series and Parallel; Charging a Battery # (a) Two similarly arranged batteries in a series sum their voltages; e.x. 3V (b) Two oppositely arranged batteries in a series subtract their voltages; e.x. 8V How battery charging works The 20V source is charging up the 12V battery Because of it\u0026rsquo;s greater voltage, the 20V is forcing charge back into the 12V (c) Two batteries in parallel, which if the emfs are the same, can provide more energy when large currents are needed. Each of the cells in parallel has to produce only a fraction of the total current, so the energy loss due to internal resistance is less than for a single cell Thus, the batteries will drain less quickly. 26.5 RC Circuits: Resistor \u0026amp; Capacitor in Series # RC Circuits differ in that they have varying current\nCapacitor Charging # After the switch .$S$ closes in the RC circuit shown in (a), the voltage across the capacitor increases with time as shown in (b), and the current through the resistor decreases with time as shown in (c).\n(a) When closed, the current starts flowing through the circuit from the negative terminal through .$R$ and accumulate on the upper plate of the capacitor which creates potential difference equal to .$V_C = Q/C$ Current is then reduced because of this opposing voltage on the capacitor (b) Eventually, the potential equals the emf, .$\\mathscr{E}$, and then no current flows and no potential difference across the resistor Potential difference, .$V_C$, across the capacitor is equal to the charge on it, .$V_C = Q/C$ Because charge increases with time, so does voltage until this point The emf .$\\mathscr{E}$ of the battery will equal the sum of the voltage drops across the resistor and the capacitor: $$\\mathscr{E} = IR + \\frac{Q}{C}$$ .$R$ is total circuit resistance, including battery .$I$ is current at all points in the circuit at any instant .$Q$ is the charge of the capacitor at that same instant Notice: .$\\mathscr{E}, R, C$ are constants, .$Q, I$ are functions of time (c) As charge builds up on the capacitor, the current decreases exponentially in time with a time constant .$\\tau$ equal to .$RC$ The rate at which charge flows thorough the resistor (.$I = dQ/dT$) is equal to the rate at which charge accumulates on the capacitor: $$ \\mathscr{E} = \\bigg(\\frac{dQ}{dt}\\bigg)R + \\frac{1}{C}Q$$ This can then be used to find an equation of .$Q$: $$ \\Longrightarrow \\frac{dQ}{C\\mathscr{E} - Q} = \\frac{dt}{RC} \\Longrightarrow \\int_0^Q \\frac{dQ}{C\\mathscr{E} - Q} = \\frac{1}{RC}\\int_0^t dt$$ $$ \\Longrightarrow \\ln\\bigg(1 - \\frac{Q}{C \\mathscr{E}}\\bigg) = - \\frac{t}{RC} \\Longrightarrow 1 - \\frac{Q}{C\\mathscr{E}} = e^{-t/RC}$$ $$ \\Longrightarrow Q = C\\mathscr{E}(1-e^{-t/RT}) = Q_0 (1-e^{-t/RT})$$ .$Q_0 = C \\mathscr{E}$ represents the maximum charge on the capacitor .$Q_0 \\neq \\text{charge $(Q)$ at $t = 0$}$ The potential difference across the capacitor is .$V_C = Q/C$ so the maximum value is $$ V_C = \\mathscr{E}(1-e^{-t/RC})$$ .$\\tau = RC$ is the axis units on graph (b) and is aptly called the time constant of the circuit Represents the time required for the capacitor to reach .$(1-e^{-1}) = 0.63 = 63\\text{%}$ of its full charge and voltage Also represents the time for the current to drop to .$1/e \\approx 0.37$ of it\u0026rsquo;s initial value Thus, it measures how quickly the capacitor becomes charged We use this as a measurement since the maximums only occur as we take .$t \\to \\infty$, but these values reach 86% of the way in .$2RC = 2\\tau$, 95% in .$3\\tau$, 98% in .$4\\tau$, so on The current in the circuit at any time can be found by differentiating the following: $$I = \\frac{dQ}{dt} = \\frac{\\mathscr{E}}{R}e^{-t/RC}$$ This is an exponential decay function: when .$t = 0$, the current is largest because there is no charge on the capacitor to impede it That is, .$I = I_0 = \\mathscr{E}/R$ As charge builds up, the current decreases exponentially in time (as shown in (c)) Capacitor Discharging # Now imagine the opposite case; we start fully charged at .$Q_0$ with voltage .$V_0$ and have to discharge through resistance .$R$ The voltage across the resistor at any instant equals that across the capacitor: $$V = IR = \\frac{Q}{C}$$ We can use this to find the functions for both .$Q_0$ and .$V_C$: $$ - \\frac{dQ}{dt} R = \\frac{Q}{C} \\Longrightarrow \\frac{dq}{Q} = - \\frac{dt}{RC}$$ $$ \\ln \\frac{Q}{Q_0} = - (t/RC) \\Longrightarrow Q = Q_0 e^{-t/RC}$$ $$ \\dots \\Longrightarrow V_C = V_0 e^{-t/RC}$$ For the RC circuit shown in (a), the voltage .$V_C$ across the capacitor decreases with time, as shown in (b), after the switch S is closed at .$t = 0$. The charge on the capacitor follows the same curve because .$Q \\propto V_C$ $$$$ $$$$\n.$V_0 = Q_0 / C$ is the initial voltage, related to initial charge We can see the charge on the capacitor, thus the voltage across it, decreases exponentially in time Current is found to be $$I = - \\frac{dQ}{dt} = \\frac{Q_0}{RC}e^{-t/RC} = I_0 e^{-t/RC}$$ The charge on the capacitor, the voltage across it, and the current in the resistor all decrease to 37% of their original value in one time constant .$t = \\tau \\ RC$ 26.6 Electric Hazards and Safety (not covered) # Current above .$\\text{1 mA}$ can be felt Current above .$\\text{10 mA}$ cause severe contraction of muscles (may not be able to let go of source) Current above .$\\text{80-100 mA}$ that passes through the torso (passing through the heart for a second) will cause ventricular fibrillation (heart stops pumping blood properly) It\u0026rsquo;s current that harms, even though voltage drives the current The seriousness of a shock depends on the current and thus the applied voltage and the effective resistance of the body More voltage shocks, more current kills Wet skin has resistance of .$10^3 \\Omega$ while dry skin is around .$10^5 \\Omega$ 26.7 Ammeters and Voltmeters: Measurement Affects Quantity Measured (not covered) # Measuring is hard to do both precisely and consistently Ammeters measure current (amps) and voltmeters measure potential difference or voltage (volts) An analog ammeter or voltmeter uses a galvanometer The full scale sensitivity, .$I_m$, is the electric current required to make the needle deflect a full scale; typically .$50 \\mu\\text{A}$ An ammeter is a galvanometer in parallel with a shunt resistor with low resistance, .$R_\\text{sh}$ A voltmeter is a galvanometer in series with a resistor with high resistance, .$R_\\text{ser}$ (b) Because an ammeter is used to measure the current flowing in the circuit, it must be inserted directly into the circuit, in series with the other elements. The smaller its internal resistance, the less it affects the circuit. (c) A voltmeter is connected “externally,” in parallel with the circuit element across which the voltage is to be measured. It measures the potential difference between two points. Its two wire “leads” (connecting wires) are connected to the two points. Only .$R_1$ is being measured above If the resistance of a voltmeter is much higher than the resistance of the circuit, it will have little effect and its readings can be more accurate At least to the manufactured precision of the meter, which for analog meters is typically 3% to 4% of full-scale deflection. Sensitivity: The sensitivity of a voltmeter is specified on its face as, for example, .$10,000\\ \\Omega/\\text{V}$. Then on the .$5\\text{V}$ scale, the voltmeter would have a resistance given by .$\\text{(5V)(10,000 $\\Omega$/V) = 50,000 $\\Omega$}$ Even an ammeter can interfere with a circuit, but the effect is minimal if its resistance is much less than that of the circuit as a whole. For both voltmeters and ammeters, the more sensitive the galvanometer, the less effect it will have on the circuit. "},{"id":59,"href":"/physics-7b/27/","title":"27: Magnetism","section":"Physics 7B","content":" 27.1 Magnets and Magnetic Fields # Every magnet has two ends or faces called poles which are where the magnetic field is strongest If a magnet is suspended so it can move freely, one pole will point north Aptly, this side is called the north pole Magnetic poles aren\u0026rsquo;t like electric charge: Positive or negative charge can easily be isolated, but we can never isolate a magnetic pole That is, if you cut a magnet is half you don\u0026rsquo;t obtain isolated north and south poles. Rather, you end up with two new magnets each with north and south poles Ferromagnetic: Materials with a strong magnetic effect i.e. iron, cobalt, nickel, gadolinium Similar to how we picture electric fields around a charge, we can picture magnetic fields surround a magnet Field lines should be drawn so that (1) the direction of the magnetic field is always tangent to a field line everywhere and (2) the number of lines per unit area is proportional to the magnetic field strength (a) Visualizing magnetic field lines around a bar magnet, using iron filings and compass needles. The red end of the bar magnet is its north pole. The N pole of a nearby compass needle points away from the north pole of the magnet. (b) Diagram of magnetic field lines for a bar magnet.\nEarth\u0026rsquo;s Magnetic Field # Earth\u0026rsquo;s magnetic poles are not exactly through the geographic pole (axis of rotation) The angular difference between the direction of the compass needle (which points along the magnetic field lines) at any location and true (geographical) north varies between .$0 - 20^\\circ$ with location Earth\u0026rsquo;s magnetic field at most location is not tangent to earth\u0026rsquo;s surface Angle of Dip: The angle that Earth\u0026rsquo;s magnetic field makes with the horizontal at any point The Earth acts like a huge magnet. But its magnetic poles are not at the geographic poles (on the Earth\u0026rsquo;s rotation axis).\n27.2 Electric Currents Produce Magnet Fields # (a) Deflection of compass needles near a current-carrying wire, showing the presence and direction of the magnetic field. (b) Iron filings also align along the direction of the magnetic field lines near a straight current-carrying wire. (c) Diagram of the magnetic field lines around an electric current in a straight wire. (d) Right-hand-rule-1 for remembering the direction of the magnetic field: when the thumb points in the direction of the conventional current, the fingers wrapped around the wire point in the direction of the magnetic field. (.$\\vec B$ is the symbol for magnetic field).\n27.3 Force on an Electric Current in a Magnetic Field # By Newton\u0026rsquo;s third law, we can see that a magnet exerts a force on a current-carrying wire The direction of the force is always perpendicular to the direction of the current and also perpendicular to the direction of the magnetic field .$\\vec B$ Use right hand rule! $$dF_\\vec{B} = dq (\\vec v \\times \\vec B) = dq\\bigg(\\frac{d\\vec l}{dt}\\times \\vec B\\bigg) = I (d\\vec l \\times \\vec B)$$ $$\\dots\\ \\vec F = I (\\vec l \\times \\vec B) = I l b \\sin\\theta$$ .$\\vec l$ is the vector whose magnitude is the length of the wire its direction is along the (straight) wire in the direction of the conventional (positive) current We use the last equation if .$\\vec B$ isn\u0026rsquo;t uniform or if the wire doesn\u0026rsquo;t form angle .$\\theta$ with .$\\vec B$ everywhere 27.4 Force on an Electric Charge Moving in a Magnetic Field # Recall, .$N$ particles, each charge .$q$, pass by a given point in time .$t$, they constitute current .$I = N q/t$ Lets say in .$t$ time, a particle charge .$q$ moves distance .$l$ in a magnetic field .$\\vec B$ We know from kinematics that .$\\vec l = \\vec v t$ where .$\\vec v$ is the velocity of the particle Using the 27.3 equation, we can find the force on all of these .$N$ particles as $$\\vec F = I\\vec l \\times \\vec B = (Nq/t)(\\vec v t) \\times \\vec B = Nq\\vec v \\times \\vec B$$ Thus, the force on just one of the .$N$ particles is $$\\vec F = q \\vec v \\times \\vec B = qvB \\sin\\theta$$ Realize that we can save a lot of pain if we know that the magnetic field is uniform in which case .$\\vec F_\\vec{B} = 0$ because the forces on opposite segments (sides) cancel out Uniform Field Path # Force exerted by a uniform magnetic field on a moving charged particle (in this case, an electron) produces a circular path.\nNotice the field goes into the paper, denoted with .$\\times$ Because force is always orthogonal to .$\\vec v$, the magnitude of .$\\vec v$ The centripetal acceleration has magnitude .$a = v^2/r$ Thus we can derive $$F = ma \\Longrightarrow qvB = m \\frac{v^2}{r}$$ $$\\dots \\Longrightarrow r = \\frac{mv}{qB}$$ The time .$T$ it takes a particle with charge .$q$ and speed .$v$ to make a revolution is $$T = \\frac{2\\pi\\cdot r}{v} = \\frac{2\\pi m}{qB}$$ $$f = \\frac{1}{T} = \\frac{qB}{2\\pi m}$$ Problem Solving # Magnetic fields are somewhat analogous to the electric fields, but there are several important differences to recall:\nThe force experienced by a charged particle moving in a magnetic field is orthogonal to the direction of the magnetic field (and to the direction of the velocity of the particle), whereas the force exerted by an electric field is parallel to the direction of the field (and independent of the velocity of the particle). The right hand rule, in its different forms, is intended to help you determine the direction of magnetic field, and the force a field exerts, and/or the directions of electric current or charged particle velocity. The right-hand rules to the right are designed to deal with the “perpendicular” nature of these quantities. With Electric Field # Lorentz Equation: A particle charge .$q$ moving with velocity .$\\vec v$ in the presence of both a magnetic field .$\\vec B$ and electric field .$\\vec E$ experiences a force $$\\vec F = q (\\vec E + \\vec v \\times \\vec B)$$ Realize that the magnetic field cannot alter speed (do work), it can only alter the direction! To change an objects speed, you must apply a force along the objects direction of motion The magnetic field exerts a force on particles moving orthogonal to it Therefore, no work can be done because the particle can only move orthogonal to the magnetic field That being said, realize that this field is responsible for the circular, constant speed, motion This is why earth\u0026rsquo;s magnetic field deflects, but doesn\u0026rsquo;t slow down, charged particles from outer space 27.5 Torque on a Current Loop; Magnetic Dipole Moment # Calculating the torque on a current loop in a magnetic field .$\\vec B$\n(a) Loop face parallel to .$\\vec B$ field lines; (b) top view; (c) loop makes an angle to .$\\vec B$, reducing the torque since the lever arm is reduced. The vector .$\\vec \\mu$ is the “magnetic moment”.\nWhen an electric current flows in a closed wire loop that\u0026rsquo;s in an external magnetic field, the magnetic force on the current can produce a torque $$\\tau = I aB \\frac{b}{2} + I aB \\frac{b}{2}$$ $$\\dots = IabB$$ .$A = ab$ is the area of the loop .$B$ is scalar of the magnetic field Notice, the vertical (orthogonal) sections of wire experience no force from the magnetic field If we have a coil of .$N$ loops of wire, the current is then .$NI$ so torque becomes $$\\tau = NIAB$$ We call .$\\vec \\mu = NI \\vec A$ the magnetic dipole moment The direction of .$\\vec A$ (and thus .$\\vec \\mu$) is defined as perpendicular to the plane of the coil We can then re-write our torque eq as $$\\vec \\tau = NI \\vec A \\times \\vec B$$ $$\\dots \\vec \\tau = \\vec \\mu \\times \\vec B$$ Dipoles have some potential energy, found by $$U = \\int \\tau d\\theta = \\int NIAB\\sin\\theta d \\theta$$ $$\\dots = -\\mu B \\cos\\theta = - \\vec \\mu \\cdot \\vec B$$ 27.8 Hall Effect # When a current-carrying conductor is held fixed in a magnetic field, the field exerts a sideways force on the charge moving in the conductor E.x, if electrons move to the right in the rectangular conductor, the inward magnetic field will exert a downward force: (a) This force is .$F_B = -e\\vec v_d \\times \\vec B$ .$v_d$ is drift velocity Thus, electrons tend to move towards side .$D$ This creates a potential difference (called the Hall emf), creating field .$\\vec E_H$ This field exerts force .$e\\vec E_H$ on the moving charge These forces are equal, that is, $$e E_H = ev_d B$$ $$\\therefore E_H = v_d B$$ Hall emf is then, asm uniform .$E_H$, $$\\mathscr{E}_H = E_H d = v_d B d$$ .$d$ is the width of the conductor The Hall effect. (a) Negative charges moving to the right as the current. (b) Positive charges moving to the left as the current.\n27.9 Mass Spectrometer # Mass spectrometers are used to measure the masses of atoms\nSteps: Ions are produced (by a current or heating) and they pass through slit .$S_1$ Ions then pass through a region with perpendicular electric and magnetic fields. Here, .$F_E = qE$ is equal to .$F_B = qvB$ Therefore, .$v = \\frac{E}{B}$ for all ions that pass the slit into .$S_2$; the rest are deflected Entering .$S_2$, there is only magnetic field .$B\u0026rsquo;$ so the ios follow a circular path Newtons second gives us .$F = ma \\Longrightarrow qvB\u0026rsquo; = mv^2/r$ Since we know all terms, we can solve for mass: .$m = \\frac{qB\u0026rsquo;r}{v} = \\frac{qBB\u0026rsquo;r}{E}$ "},{"id":60,"href":"/physics-7b/28/","title":"28: Sources of Magnetic Field","section":"Physics 7B","content":" 28.1 Magnetic Field Due to a Straight Wire # Magnetic fields due to the electric current in a long straight wire forms circles with the wire at the center This field is proportional directly with current .$I$ and inversely with distance .$r$: $$B = \\frac{\\mu_0}{2\\pi} \\cdot \\frac{I}{r}$$ .$\\mu_0 = 4\\pi \\times 10^{-7} \\text{ T$\\cdot$m/A}$ is the permeability of free space 28.2 Force between Two Parallel Wires # Since current-carrying wires feel a force in magnetic fields, and because current-carrying wires emit magnetic fields, current-carrying wires exert a force on one another Magnetic field .$B_1$ produced by .$I_1$ is given by $$B_1 = \\frac{\\mu_0}{2\\pi} \\cdot \\frac{I_1}{d}$$ Parallel currents in the same direction attract while antiparallel repel Using .$F = IlB$ we can write the force .$F_2$ exerted by .$B_1$ on length .$l_2$ carrying .$I_2$ has magnitude: $$F_2 = I_2 l_2 B_1$$ Notice that the force on .$I_1$ is due to the field produced by .$I_1$ Thus, subbing .$B_1$ into the .$F_2$ formula we find the force on length .$l_2$: $$F_2 = \\frac{\\mu_0}{2\\pi} \\cdot \\frac{I_1 I_2}{d}l_2$$ 28.3 Definitions of the Ampere and the Coulomb # One ampere can be defined as that current flowing in each of two long parallel wires exactly 1 m apart, which results in a force of exactly .$2\\times10^{-7}$ N per meter of length of each wire This was the standard we used prior because it is readily reproducible (that is, it\u0026rsquo;s called an operational definition) Coulomb is defined in terms of the ampere being exactly one ampere-second: .$\\text{1 C = A $\\cdot$ s}$ Now we define an ampere ampere by saying it\u0026rsquo;s .$\\text{1 C = A $\\cdot$ s}$ and we know the Coulomb because it\u0026rsquo;s mutually assigned the exact value .$e = 1.60176636 \\times 10^{-19}\\text{ C}$ 28.4 Ampère\u0026rsquo;s Law # If we don\u0026rsquo;t have a straight line, we use ampere\u0026rsquo;s law given below We take a infinite tiny segments and dot it with the field at the segment: $$\\oint \\vec B \\cdot d \\vec l = \\mu_0 I_\\text{encl}$$ Note that .$\\vec B$ in Ampere\u0026rsquo;s law isn\u0026rsquo;t necessarily due only to the current .$I_\\text{encl}$ As with Gauss\u0026rsquo;s law for the electric field, Ampere\u0026rsquo;s law practical value to calculate the magnetic field is limited, however, mainly to simple or symmetric situations. Its importance is that it relates the magnetic field to the current in a direct and mathematically elegant way. Ampère\u0026rsquo;s law is considered one of the basic laws of electricity and magnetism: It is valid for any situation where the currents and fields are steady and not changing in time, and no magnetic materials are present Problem Solving: # Ampère\u0026rsquo;s law, like Gauss\u0026rsquo;s law, is always a valid statement. But as a calculation tool it is limited mainly to systems with a high degree of symmetry. The first step in applying Ampère\u0026rsquo;s law is to identify useful symmetry. Choose an integration path that reflects the symmetry. Search for paths where .$\\vec B$ has constant magnitude along the entire path or along segments of the path. Make sure your integration path passes through the point where you wish to evaluate the magnetic field. Use symmetry to determine the direction of .$\\vec B$ along the integration path. With a smart choice of path, .$\\vec B$ will be either parallel or perpendicular to the path. Determine the enclosed current, .$I_\\text{encl}$. Be careful with signs. Let the fingers of your right hand curl along the direction of .$\\vec B$ so that your thumb shows the direction of positive current. If you have a solid conductor and your integration path does not enclose the full current, you can use the current density (current per unit area) multiplied by the enclosed area. 28.5 Magnetic Field of a Solenoid and a Toroid # Solenoid: A long looping coil of wire carrying a dc current The current in each loop produces a magnetic field The total magnetic field is the sum of the fields due to each loop Direction is determined by the right hand rule $$\\oint \\vec B \\cdot d \\vec l = \\mu_0 NI$$ Magnetic field due to a solenoid (a) a few loosely spaced loops; (b) for many closely spaced loops, the field is nearly uniform.\nCross-sectional view into a solenoid. The magnetic field inside is straight except at the ends. Red dashed lines indicate the path chosen for use in Ampère\u0026rsquo;s law. .$\\odot$ and .$\\otimes$ are electric current direction (in the wire loops) out of the page and into the page.\n$$\\int_c^d \\vec B \\cdot d \\vec l = Bl_{cd}$$\nWith .$n = N/l$ is number of loops per unit length we can simplify to $$B = \\mu_0 nI$$ Now, we see that the field does not depend on position within the solenoid, so .$\\vec B$ is uniform. This is strictly true only for an infinite solenoid, but it is a good approximation for tightly wound real ones for points not close to the ends. 28.6 Biot-Savart Law # A current .$I$ flowing in any path can be considered as many tiny current elements, such as .$d \\vec l$ Then, .$d\\vec B$ at any point .$P$ in space due to this element of current is given by Biot-Savart law: $$d\\vec B = \\frac{\\mu_0 I}{4\\pi} \\frac{d\\vec l \\times \\hat r}{r^2} = \\frac{\\mu_0 I}{4\\pi} \\frac{dl \\sin\\theta}{r^2}$$ .$\\vec r$ is the displacement vector from the element .$d \\vec l$ to the point .$P$ .$\\hat r$ is the unit vector in the direction .$\\vec r$ .$\\theta$ is the angle between .$d\\vec l$ and .$\\vec r$ Biot-Savart Law The field at P due to current element .$I d\\vec l$ is .$d \\vec B = (\\mu_0 I/4\\pi)(d\\vec l \\times \\hat r/r^2)$\nAn important difference between the Biot-Savart law and Ampère\u0026rsquo;s law is that the later, .$\\oint \\vec B \\cdot d\\vec l = \\mu_0 I_\\text{encl}$, .$\\vec B$ is not necessarily due only to the current enclosed by the path of integration. But in the Biot-Savart law the field .$d\\vec B$ is due only, and entirely, to the current element .$I d\\vec l$ \u0026ndash; that is, to find the total .$\\vec B$ at any point in space, it is necessary to include all currents. Biot-Savart strategy for finding a magnetic field\nSelect some tiny piece of wire .$d \\vec l$ and draw the vector .$\\vec r$ pointing from said piece to the location at which you\u0026rsquo;re finding the magnetic field Using Biot-Savart, calculate the magnitude and direction of the infinitesimal magnetic field .$d\\vec B$ generated by that piece Add up all those magnetic field contributions by integrating over the wire, using vector components if needed. Straight Wire # $$B = \\frac{\\mu_0 I}{4\\pi}\\int_{y =-\\infty}^{+\\infty} \\frac{dy \\sin\\theta}{r^2}$$ $$dy = dl; r^2 = R^2 + y^2$$ $$dy = \\dots = \\frac{r^2 d\\theta}{R}$$ $$B = \\frac{\\mu_0 I}{4\\pi}\\frac{1}{R}\\cdot \\bigg[-\\cos\\theta\\bigg]_{\\theta = 0}^\\pi $$ $$\\dots = B = \\frac{\\mu_0 I}{2\\pi R}$$ Current Loop # $$dB = \\frac{\\mu_0 I dl}{4\\pi r^2}$$ $$d\\vec l \\perp \\vec r; \\vert d\\vec l \\times \\hat r \\vert = dl$$ $$B = \\int dB \\cos \\phi = \\int dB \\frac{R}{r}$$ $$\\dots = \\int dB \\frac{R}{(R^2 + x^2)^{\\frac{1}{2}}}$$ $$\\dots = \\frac{\\mu_0 I}{4\\pi}\\frac{R}{(R^2 + x^2)^{\\frac{3}{2}}} \\bigg[L\\bigg]_{L = 2\\pi R}$$ $$\\dots = \\frac{\\mu_0 I}{2R}$$ Quarter Wire Segment # $$dB = \\frac{\\mu_0 I}{4\\pi}dl$$ $$B = \\frac{\\mu_0 I}{4\\pi} \\bigg[L\\bigg]_{L =\\frac{1}{4}\\cdot 2\\pi R}$$ $$\\dots = \\frac{\\mu_0 I}{8 R}$$\n28.6.1 Magnetic Dipole Field # Recall that a magnetic dipole is .$\\mu = NIA$ (number of loops, current, and area of coil) The magnetic field produced by magnetic dipoles is $$B = \\frac{\\mu_0 IR^2}{2(R^2 + x^2)^{\\frac{3}{2}}}$$ This can be written in tirms of the magnetic dipole .$\\mu = IA = I\\pi R^2$ for one loop: $$B = \\frac{\\mu_0}{2\\pi} \\frac{\\mu}{(R^2 +x^2)^{\\frac{3}{2}}}$$ and at distances far from the loop .$x \\gg R$ this becomes $$B \\approx \\frac{\\mu_0}{2\\pi} \\frac{\\mu}{x^3}$$ 28.7 Magnetic Field Due to a Single Moving Charge # Realize that Biot-Savart works only when considering constant currents that do not change in time significantly over a significant length of wire Additionally, the law is difficult to confirm experimentally: Particles would shoot out of the field before they (and thus the field) could be measured for any significant .$B$. If .$v$ is slow enough to be measured, .$B$ is small enough it\u0026rsquo;s going to be drowned out by experimental noise From the particles frame, it\u0026rsquo;s at rest (not moving wrt itself). And since it\u0026rsquo;s at rest, it shouldn\u0026rsquo;t create a field. However, it\u0026rsquo;s moving because it\u0026rsquo;s creating a field! Einstein explains this with special relativity Observers in two different reference frames, moving relative to each other, will not observe the same .$\\vec E$ and .$\\vec B$ fields What see from the perspective other than the particle: The magnetic field at one single instant due to a single positive charge .$q$ moving at velocity .$\\vec v$.\n28.8 Magnetic Materials—Ferromagnetism (not covered) # Magnetic fields are produced by (1) magnets and (2) electric currents Ferromagnetism: Materials that are magnetic At a small enough resolution (less than 1mm areas!), domains exist which behave like tiny magnets \u0026ndash; they have two poles The more domains that are aligned in one direction, the stronger the magnetic field These domains can be moved around (through dropping or hammering the magnet) Heating also reduces magnetism \u0026ndash; increasing temp increases the random thermal motion of atoms Above the Curie Temperature (1043K for iron), a magnet cannot be made at all As a consequence, at lower temperatures, some materials are magnetic (a) An unmagnetized piece of iron is made up of domains that are randomly arranged. Each domain is like a tiny magnet; the arrows represent the magnetization direction, with the arrowhead being the N pole. (b) In a magnet, the domains are preferentially aligned in one direction (down in this case), and may be altered in size by the magnetization process.\nToday, we believe that all magnetic fields come from electric currents Electrons produce a (tiny) magnetic field, as if they and their electric charges were spinning about their own additional axes Realize that while .$\\vec B$ lines form closed loops, .$\\vec E$ lines go from a positive to negative electron 28.9 Electromagnets and Solenoids—Applications (not covered) # Solenoids act like magnets; they have poles (determined by the right hand rule)\nMagnetic field of a solenoid. The north pole of this solenoid, thought of as a magnet, is on the right, and the south pole is on the left.\nIf a piece of iron is placed inside a solenoid, the magnetic field is increased greatly\nThe domains of the iron are aligned by the magnetic field produced by the current; that is, the iron becomes a magnet. This system of the iron-core solenoid is called an electromagnet The resulting magnetic field is the sum of the field due to the solenoid\u0026rsquo;s current and the field due to the iron, and can be hundreds or thousands of times larger than the field due to the current alone The alloys of iron used in electromagnets acquire and lose their magnetism quite readily when the current is turned on or off, and so are referred to as “soft iron.” (It is “soft” only in a magnetic sense.) Soft iron is usually used in electromagnets so that the field can be turned on and off readily. Iron that holds its magnetism even when there is no externally applied field is called “hard iron.” Hard iron is used in permanent magnets. Whether iron is hard or soft depends on heat treatment, type of alloy, and other factors. Sometimes an iron core is not present—the magnetic field then comes only from the current in the wire loops. A large field .$B$ in this case requires a large current .$I$ which produces a large amount of waste heat since .$P = I^2 R$. But if the current-carrying wires are made of superconducting material kept below the transition temperature then very high magnetic fields can be produced, and no electric power is needed to maintain the large current in the superconducting coils. Note that energy is required to refrigerate the coils at the low temperatures where they superconduct. 28.10 Magnetic Fields in Magnetic Materials; Hysteresis (not covered) # The solenoid field is produced just .$n$ loops per unit length is .$B_0 = \\mu_0 n I$ If a solenoid contains a ferromagnetic material (e.x. iron), then we need to consider it\u0026rsquo;s field .$B_M$ produced in our total field calculation: $$\\vec B = \\vec B_0 + \\vec B_M$$ Often, .$B_M \\gg B_0$ We can also write this equation as $$B = \\mu n I$$ .$\\mu$ is the magnetic permeability (not electric dipole moment!) However, while .$\\mu$ is a characteristic of the material, it is not constant for ferromagnetic materials; rather, it depends on the value of the “external field” .$B_0$ .$\\mu \\gg \\mu_0$ for ferromagnetic materials "},{"id":61,"href":"/physics-7b/29/","title":"29: Electromagnetic Induction \u0026 Faraday's Law","section":"Physics 7B","content":" 29–1 Induced EMF # A changing magnetic field induces an emf That is, changing .$\\vec B$, not .$\\vec B$ itself, induces current Constant magnetic fields produce no current in a conductor This process is called electromagnetic induction It doesn\u0026rsquo;t matter if the magnet or coil moves, only that there is relative motion between the two (a) A current is induced when a magnet is moved toward a coil, momentarily increasing the magnetic field through the coil. (b) The induced current is opposite when the magnet is moved away from the coil (.$\\vec B$ decreases). In (c), no current is induced if the magnet does not move relative to the coil. It is the relative motion that counts here: the magnet can be held steady and the coil moved, which also induces an emf.\n29–2 Faraday\u0026rsquo;s Law of Induction; Lenz\u0026rsquo;s Law # EMF is proportional to the rate of change of the magnetic flux .$\\Phi_B$ passing through the circuit or loop with area .$A$ Given a uniform magnetic field .$\\vec B$ we write $$\\Phi_B = B_\\perp A = BA \\cos\\theta = \\vec B \\cdot \\vec A$$ For non-uniform fields, we need to integrate: $$\\Phi_B = \\int \\vec B \\cdot d \\vec A$$ The unit of magnetic flux is the tesla-meter, called the weber: .$\\text{1 Wb = 1 T$\\cdot$ m$^2$}$ Faraday\u0026rsquo;s law of induction The emf .$\\mathscr{E}$ induced in a circuit is equal to the rate of change of magnetic flux through the circuit: $$\\mathscr{E} = -N\\frac{d\\Phi_B}{dt}$$ .$N$ is the number of loops closely wrapped so the flux passes through each Lenz\u0026rsquo;s Law A current produced by an induced emf creates a magnetic field that opposes the original change in magnetic flux Faraday\u0026rsquo;s law is negative accordingly Realize that we know have to magnetic fields: The changing magnetic field or flux that induces the current, and The magnetic field produced by the induced current (all currents produce a magnetic field) which opposes the charge in the first Note: The magnetic field created by induced current opposes change in external flux, not necessarily opposing the external field It is important to note that an emf is induced whenever there is a change in flux through the coil \u0026ndash; this can be done in three ways: Changing the magnetic field Changing the area .$A$ of the loop in the field Changing the loop\u0026rsquo;s orientation .$\\theta$ wrt the field Problem Solving \u0026ndash; Lenz\u0026rsquo;s Law # Lenz\u0026rsquo;s law is used to determine the direction of the (conventional) electric current induced in a loop due to a change in magnetic flux inside the loop. (The loop may already be carrying its own ordinary current.) To produce an induced current you need (1) a closed conducting loop, and (2) an external magnetic flux through the loop that is changing in time.\nDetermine whether the magnetic flux .$\\Phi_B = \\vec B \\cdot \\vec A$ inside the loop is decreasing, increasing, or unchanged. The magnetic field due to the induced current: (a) points in the same direction as the external field if the flux is decreasing; (b) points in the opposite direction from the external field if the flux is increasing; or (c) is zero if the flux is not changing. Once you know the direction of the induced magnetic field, use the right hand rule to find the direction of the induced current that would give this induced .$\\vec B$ Always keep in mind that there are two magnetic fields: (1) an external field whose flux must be changing if it is to induce an electric current, and (2) a magnetic field produced by the induced current. If a wire is already carrying an electric current, the total current while the magnetic field is changing will be the algebraic sum of the original current and the induced current. 29–3 EMF Induced in a Moving Conductor # If a conductor begins to move in a magnetic field, an emf is induced We can use Faraday\u0026rsquo;s law and kinematics to derive an equation: $$\\mathscr{E} = \\frac{d\\Phi_B}{dt} = \\frac{B dA}{dt} = \\frac{B (l\\cdot v\\ dt)}{dt} = Blv$$ .$v$ is the speed of the conductor .$dA = l\\ dx = lv\\ dt$ is the change in area over time .$t$ Be careful! This is a generalization where .$B, l, v$ are mutually perpendicular If they aren\u0026rsquo;t, we use the component of each that are mutually perpendicular An emf induced on a conductor moving ina magnetic field is sometimes called a motional emf We could also derive this using our force eq from ch27: $$\\vec F = q\\vec v \\times \\vec B$$\nWhen the conductor moves with .$v$, as do its electrons Since .$\\vec \\perp \\vec B$, each electron feels force .$F = qvB$ The direction determined by the right hand rule (red) If the rod is not in contact with the conductor, electrons would collect at the upper end leaving the lower positive Therefore, there must be an induced emf! If the rod is in contact with the conductor, the electrons will flow into the conductor and there will be a clockwise current in the loop To calculate emf, we determine the work .$W$ needed to move a charge .$q$ from one end of the rod to the other against this potential difference: $$W = F \\times d = (qvB) \\times (l)$$ $$\\mathscr{E} = W/q = qvBl/q = Blv$$ (a) A conducting rod is moved to the right on the smooth surface of a U-shaped conductor in a uniform magnetic field .$\\vec B$ that points out of the page. The induced current is clockwise. (b) Force on an electron in the metal rod (moving to the right) is upward due to .$\\vec B$ pointing out of the page. Hence electrons can collect at the top of the rod, leaving .$+$ charge at the bottom.\n29–4 Electric Generators # Electric generators produce electricity by transforming mechanical energy into electric energy Also called dynamos Opposite of what a motor does Consists of many wires wound around an armature that can rotate in a magnetic field This axel is turned by mechanical means (belt, steam, water falling, etc.) and an emf is induced in the rotating coil An ac current is thus the output of a generator An ac generator Each brush is fixed and presses against a continuous slip ring that rotates with the armature If the armature is rotating clockwise; then, .$\\vec F = q \\vec v \\times \\vec B$ applied to a charged particles in the wire Lenz\u0026rsquo;s law tells us that the (conventional) current in the wire .$b$ on the armature is outwards, towards us, thus, the current is outwards, through brush .$b$ After one-half revolution, wire .$b$ will be where .$a$ is now and the current at .$b$ will be inwards. Thus, the current produced is alternating! If the loop is made to rotate in a uniform field .$\\vec B$ with constant angular velocity .$\\omega$, the emf produced is $$\\mathscr{E} = - \\frac{d\\Phi_B}{dt} = - \\frac{d}{dt}\\int \\vec B \\cdot d \\vec A = -\\frac{d}{dt}\\big[BA\\cos\\theta\\big]$$ .$A$ is the area of the loop .$\\theta$ is the angle between .$\\vec B$ and .$\\vec A$ Since .$\\omega = d\\theta/dt \\Longrightarrow \\theta = \\theta_0 + \\omega t$, we choose .$\\theta_0 = 0$ so $$\\mathscr{E} = - N BA \\frac{d}{dt}(\\cos(\\omega t)) = N BA \\omega \\sin (\\omega t)$$ .$N$ is the number of loops in the rotating coil, assumed to be .$1$ unless stated otherwise Thus, the output wave is sinusoidal Amplitude .$\\mathscr{E}_0 = NBA\\omega$ .$\\mathscr{E}_\\text{rms} = \\frac{\\mathscr{E}_0}{\\sqrt{2}}$ .$f = \\omega / 2\\pi$ .$\\text{60 Hz}$ for USA + Canada, .$\\text{50 Hz}$ for EU dc generators Same for ac, except the slip rings are replaced by split-ring commutators (just like a dc motor) The curve output becomes more smooth by placing a capacitor in parallel with the output More commonly, is the use of many armature windings A capacitor tends to store charge and, if the time constant .$RC$ is long enough, helps to smooth out the voltage as shown in the figure to the right. (a) A dc generator with one set of commutators, and (b) a dc generator with many sets of commutators and windings.\n29–6 Transformers and Transmission of Power # Transformer: Device used to increase or decrease an ac voltage Made up of two coils know as the primary and secondary coil Primary is the input, secondary is the output These can be interwoven (with insulated wire); or can be linked by an iron core that\u0026rsquo;s laminated We assume energy losses (in resistance and hysteresis) can be ignored When an ac voltage is applied to the primary coil, the changing magnetic field it produces will induce an ac voltage of the same .$f$requency in the secondary coil However, secondary voltage, .$V_S$, changes according to the number of turns or loops in each coil: $$V_S = N_S \\frac{d\\Phi_B}{dt}$$ .$N_S$ is the number of turns in the secondary coil .$\\Phi_B/dt$ is the rate at which the magnetic flux changes through each coil The input voltage, .$V_P$, is related to this rate too: $$V_P = N_P \\frac{d\\Phi_B}{dt}$$ .$N_P$ is the number of turns in the primary coil This follows because the changing flux produce a back emf, .$N_P\\ d\\Phi_B / dt$, in the primary that exactly balances the applied voltage .$V_P$ This is because of Kirchhoff\u0026rsquo;s rules This is only the case if the resistance of the primary can be ignored (which we assume) Then, we can divide these two equations, assuming little or no flux loss, to find $$\\frac{V_S}{V_P} = \\frac{N_S}{N_P}$$ .$V_S$ and .$V_P$ can be the rms or peak values for both Step-up (.$N_S \u0026gt; N_P$) increase voltage; step-down (.$N_S \u0026lt; N_P$) decrease This is called the transformer equation which tells us how the secondary (output) is related to the primary (input) DC voltages don\u0026rsquo;t work in a transformer because there\u0026rsquo;d be no change in magnetic flux But muh conservation of energy! Power output is essentially the power input since transformers tend to be 99%+ efficient The little amount of energy lost is to heat Since .$P = IV$ and .$P_P \\approx P_S$, we have $$I_P V_P = I_S V_S \\Longrightarrow \\frac{I_S}{I_P} = \\frac{V_S}{V_P} = \\frac{N_P}{N_S}$$ 29–7 A Changing Magnetic Flux Produces an Electric Field # A changing magnetic flux produces an electric field This applies not only to wires and other conductors, but is a general result that applies to any region in space An electric field will be produced (induced) at any point in space where theres is a changing magnetic field These electric fields are not static, as are the electric fields due to electric charges at rest Faraday\u0026rsquo;s Law \u0026ndash; General Form # $$ \\mathscr{E} = \\oint \\vec E \\cdot d \\vec l = - \\frac{d\\Phi_B}{dt}$$\nThe first two terms come from the fact that the emf .$\\mathscr{E}$ induced in a circuit is equal to the work done per unit charge by the electric field This is then combined with the relation of a changing magnetic flux to the the field it produces Forces Due to Changing .$\\vec B$ are non-conservative # Electric field lines produced by a changing magnetic field are continuous; they form closed loops That is, while a conservative force (such as a electrostatic magnetic field) integrated over a line integral is zero .$\\big(\\oint \\vec E_\\text{electrostatic} \\cdot d \\vec l = 0\\big)$, the electric field created by an magnetic field is not zero: .$\\oint \\vec E_\\text{non-static} \\cdot d \\vec l = - \\frac{d\\Phi_B}{dt}$ This implies that forces due to changing magnetic fields are non-conservative and we can\u0026rsquo;t define a potential energy (or even a potential function!) at a given point in space "},{"id":62,"href":"/physics-7b/30/","title":"30: Inductance, Electromagnetic Oscillations, \u0026 AC Circuits","section":"Physics 7B","content":" 30.1 Mutual Inductance # When two wires are near one another, they induce an emf in one another This emf is proportional to the rate of change of the flux passing through it The flux passing through the coil is generated by the other coil\u0026rsquo;s current If the two coils are held in place then the total flux is proportional to the mutual inductance $$M = \\frac{N_2 \\Phi_{21}}{I_1}$$ .$\\Phi_{21}$ is the total flux passing through coil 2 (induced by the current in coil 1, .$I_1$) .$M$ depends on “geometric” factors such as the size, shape, number of turns, and relative positions of the two coils, and whether a ferromagnetic material is present The SI unit for mutual inductance is the Henry; .$\\text{1 H = 1 V$\\cdot$s/A = 1$\\Omega \\cdot$s}$ A changing current in one coil will induce a current in the second coil.\nThe emf induced in coil 2 due to a change in current 1 can be written now as $$\\mathscr{E_2} = -N_2 \\frac{d\\Phi_{21}}{dt} = -M \\frac{dI_1}{dt}$$ 30.2 Self-Inductance; Inductors # This concept also applies to isolated coils too When a changing current passes through a coil (or solenoid), a changing magnetic flux is produced inside the coil, and this in turn induces an emf in that same coil. This induced emf opposes the change in flux (Lenz\u0026rsquo;s law). If the current through the coil is increasing, the increasing magnetic flux induces an emf that opposes the original current and tends to retard its increase. If the current is decreasing in the coil, the decreasing flux induces an emf in the same direction as the current, thus tending to maintain the original current. If this inductance (coil) is in a circuit, it thus can provide a source of emf, in addition to any battery present or other sources of emf. Self-inductance .$L$ and the emf it induces is given by $$L = \\frac{N\\Phi_B}{I}$$ $$\\mathscr{E} = -N \\frac{d\\Phi_B}{dt} = -L \\frac{dI}{dt}$$ An ac circuit always contains some inductance, but often it is quite small unless the circuit contains a coil of many loops or turns. A coil that has significant self-inductance .$L$ is called an inductor Inductance can serve a useful purpose in certain circuits, but it\u0026rsquo;s often just a nuisance in a circuit. If inductance is large, the change in the current will be small, and therefore the current itself if it is ac will be small. The greater the inductance, the less the ac current. An inductance thus acts something like a “resistance” to impede the flow of alternating current. Solenoid Self-Inductance # The magnetic field of a solenoid is .$B = \\mu_0 nI$ where .$n = N/l$ Thus, the flux is .$\\Phi_B = BA = \\mu_0 NIA/l$ so we can derive $$L = \\frac{N \\Phi_B}{I} = \\frac{\\mu_0 N^2 A}{l}$$ 30.3 Energy Stored in a Magnetic Field # When an inductor with inductance .$L$ is carrying current .$I$ which is changing at the rate .$dI/dt$, energy is being supplied to the inductor at the rate $$P = I \\mathscr{E} = LI \\frac{dI}{dt}$$ Thus, the work needed to increase the current in an inductor from zero to some .$I$ is $$W = \\int P dt = \\int_0^I LI\\ dI = \\frac{1}{2}LI^2$$ This is also the (potential) energy stored in a conductor when it is carrying current .$I$ Just as the energy stored in a capacitor can be considered to reside in the electric field between its plates, so the energy in an inductor can be considered to be stored in its magnetic field. E.x. the energy stored in a solenoid is $$U = \\frac{1}{2}LI^2 = \\frac{1}{2} \\bigg( \\frac{\\mu_0 N^2 A}{l} \\bigg)\\bigg( \\frac{Bl}{\\mu_0 N}\\bigg)^2 = \\frac{1}{2} \\frac{B^2}{\\mu_0}Al$$ We can think of this energy as residing in the volume enclosed by the windings; .$Al$ Then the energy density (energy per unit volume) is $$u = \\text{energy density} = \\frac{1}{2}\\frac{B^2}{\\mu_0}$$ This equation is analogous to that for an electric field, .$ \\frac{1}{2}ϵE^2$ 30.4 LR Circuits # (a) LR circuit; (b) growth of current when connected to battery.\nAt the instant the switch connecting the battery is closed, the current starts to flow. It is opposed by the induced emf in the inductor because of the changing current. As soon as current starts to flow, there is also a voltage drop across the resistance; .$V = IR$ Hence, the voltage drop across the inductance is reduced and the current increases less rapidly as it approaches .$I_0 = V_0 / R$ as seen in (b) The emfs in the circuit are the battery voltage .$V_0$ and the emf .$\\mathscr{E} = -L (dI/dt)$ in the inductor (which opposes the current) Hence, we can find sum of potential charges around the loop where .$I$ is the current at any instance by using the loop rule: $$V_0 - IR - L \\frac{dI}{dt} = 0 \\Longrightarrow L \\frac{dI}{dt} + RI = V_0$$ We can integrate the later term from .$t = 0, I = 0$ to a later time .$t$ when current is .$I$: $$\\int_0^I \\frac{dI}{V_0 - IR} = \\int_0^t \\frac{1}{L}dt \\Longrightarrow - \\frac{1}{R} \\ln \\bigg( \\frac{V_0 - IR}{V_0} \\bigg) \\frac{t}{L}$$ $$\\dots \\Longrightarrow \\frac{V_0 - IR}{V_0} = e^{- \\frac{Rt}{L}} \\Longrightarrow I = \\frac{V_0}{R}(1-e^{-t/\\tau})$$ .$\\tau = \\frac{L}{R}$ is the time constant: the time required for the current .$I$ to reach 63% of it\u0026rsquo;s maximum value .$V_0/R$ When the battery is removed from the circuit\u0026hellip; Voltage .$V_0$ becomes zero, so .$L \\frac{dI}{dt} + RI = 0$ We can integrate this and solve for .$I$ $$\\int_{I_0}^I \\frac{dI}{I} = - \\int_0^t \\frac{R}{L} dt$$ $$\\dots \\Longrightarrow I = I_0 e^{-t/\\tau}$$ - .$\\tau = L/R$ is the time it takes current to decrease to 37% of it\u0026rsquo;s initial 30.5 LC Circuits and Electromagnetic Oscillations # Any electric circuit can contain the three basic components: resistance, capacitance, and inductance, in addition to a source of emf. Suppose the adjacent circuit is initially charged so one plate has charge .$Q_0$ and the other .$-Q_0$ and that the potential difference is .$V = Q/C$ At .$t = 0$ the capacitor immediately begins to discharge. As it does so, the current .$I$ through the inductor increases. We now apply Kirchhoff\u0026rsquo;s loop rule (sum of potential changes around a loop is zero): $$-L \\frac{dI}{dt} + \\frac{Q}{C} = 0$$ Because charge leaves the positive plate on the capacitor to produce the current .$I = dQ/dt$ so we can rewrite the equation as $$\\dots \\Longrightarrow \\frac{d^2 Q}{dt^2} + \\frac{Q}{LC} = 0 \\Longrightarrow Q = Q_0 \\cos(\\omega t + \\phi)$$ .$Q_0$ and .$\\phi$ are constants that depend on the initial conditions .$\\omega t + \\phi$ is in radians. Because we have .$\\phi$, the amplitude isn\u0026rsquo;t .$Q_0$ unless .$\\phi = 0$ .$\\omega = 2\\pi f = \\sqrt{ \\frac{1}{LC} }$ We can then plug in our sinusoidal equation to find a function for .$I$: $$I = -\\frac{dQ}{dt} = \\omega Q_0 \\sin(\\omega t + \\phi)$$ $$\\dots \\Longrightarrow I_0 \\sin(\\omega t + \\phi)$$ Energy stored in capacitor: $$U_E = \\frac{1}{2}\\frac{Q^2}{C} = \\frac{Q_0^2}{2C}\\cos^2(\\omega t + \\phi)$$ Energy stored in magnetic field: $$U_B = \\frac{1}{2}LI^2 = \\frac{Q_0^2}{2C}\\sin^2(\\omega t + \\phi)$$ Total energy stored: $$U = U_B + U_E = \\frac{Q_0^2}{2C}$$ Period: .$T = \\frac{1}{f} = \\frac{2\\pi}{\\omega} = 2\\pi \\sqrt{LC}$\n"},{"id":63,"href":"/math-53/trig/","title":"Trig Identities","section":"Math 53","content":" Reciprocal Identities # $$\\sin(x)=\\frac{1}{\\csc(x)}$$ $$\\cos(x)=\\frac{1}{\\sec(x)}$$ $$\\tan(x)=\\frac{\\sin(x)}{\\cos(x)}=\\frac{1}{\\cot(x)}$$ Pythagorean Identities # $$\\sin^2(x) + \\cos^2(x) = 1$$ $$1+\\tan^2(x) = \\sec^2(x)$$ $$1+\\cot^2(x)=\\csc^2(x)$$ Cofunction Identities # $$\\sin\\Big(\\frac{\\pi}{2}-x\\Big) = \\cos(x)$$ $$\\csc\\Big(\\frac{\\pi}{2}-x\\Big) = \\sec(x)$$ $$\\cos\\Big(\\frac{\\pi}{2}-x\\Big) = \\sin(x)$$ $$\\sec\\Big(\\frac{\\pi}{2}-x\\Big) = \\csc(x)$$ $$\\tan\\Big(\\frac{\\pi}{2}-x\\Big) = \\cot(x)$$ $$\\cot\\Big(\\frac{\\pi}{2}-x\\Big) = \\tan(x)$$ Even/Odd Identities # $$ \\sin(-x) = -\\sin(x)$$ $$ \\csc(-x) = -\\csc(x)$$ $$ \\cos(-x) = \\cos(x)$$ $$ \\sec(-x) = \\sec(x)$$ $$ \\tan(-x) = - \\tan(x)$$ $$ \\cot(-x) = -\\cot(x)$$ Bonus fact: .$\\int_{-A}^A \\text{[odd]}(x)\\ dx = 0$; .$\\int_{-A}^A \\text{[even]}(x)\\ dx = \\int_0^A \\text{[even]}(x)\\ dx$ Sum and Difference Formulas # $$ \\sin(u \\pm v) = \\sin(u) \\cdot \\cos(v) \\pm \\cos(u) \\cdot \\sin(v)$$ $$ \\cos(u \\pm v) = \\cos(u) \\cdot \\cos(v) \\pm \\sin(u) \\cdot \\sin(v)$$ $$\\tan(u \\pm v) = \\frac{\\tan(u) \\pm \\tan(v)}{1 \\mp \\tan(u) \\tan(v)}$$\nDouble-Angle Formula # $$ \\sin(2u) = 2 \\sin(u) \\cos(u)$$ $$ \\cos(2u) = 2 \\cos^2(u) - 1$$ $$ \u0026hellip; = 1- 2 \\sin^2(u) $$ $$ \u0026hellip; = \\cos^2(u) - \\sin^2(u) $$ $$ \\tan(2u) = \\frac{2 \\tan(u)}{1 - \\tan^2(u)}$$ Power Reducing Formulas # $$\\sin^2(u) = \\frac{1 - \\cos(2u)}{2}$$ $$\\cos^2(u) = \\frac{1 + \\cos(2u)}{2}$$ $$\\tan^2(u) = \\frac{1 - \\cos(2u)}{1 + \\cos(2u)}$$ Sum to Product Formulas # $$\\sin(u) + \\sin(v) = 2\\sin\\bigg(\\frac{u + v}{2}\\bigg) \\cos\\bigg(\\frac{u - v}{2}\\bigg)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $$ $$\\sin(u) - \\sin(v) = 2\\cos\\bigg(\\frac{u + v}{2}\\bigg) \\sin\\bigg(\\frac{u - v}{2}\\bigg)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $$ $$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\cos(u) + \\cos(v) = 2\\cos\\bigg(\\frac{u + v}{2}\\bigg) \\cos\\bigg(\\frac{u - v}{2}\\bigg)$$ $$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\cos(u) - \\cos(v) = -2\\sin\\bigg(\\frac{u + v}{2}\\bigg) \\sin\\bigg(\\frac{u - v}{2}\\bigg)$$\nProduct to Sum Formulas # $$\\sin(u) \\sin(v) = \\frac{1}{2}\\Big[\\cos(u - v) - \\cos(u + v)\\Big]\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $$ $$\\sin(u) \\cos(v) = \\frac{1}{2}\\Big[\\sin(u + v) + \\sin(u - v)\\Big]\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $$ $$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\cos(u) \\sin(v) = \\frac{1}{2}\\Big[\\sin(u + v) - \\sin(u - v)\\Big]$$ $$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\cos(u) \\cos(v) = \\frac{1}{2}\\Big[\\cos(u - v) + \\cos(u + v)\\Big]$$\n"},{"id":64,"href":"/math-53/trig-calc/","title":"Trig Calculus","section":"Math 53","content":" Derivatives # $$\\frac{d}{dx} \\tan(x) = 1 + \\tan^2(x) = \\sec^2(x)$$ $$\\frac{d}{dx} \\csc(x) = -\\cot(x) \\cdot \\csc(x)$$ $$\\frac{d}{dx} \\sec(x) = \\frac{\\sin(x)}{\\cos^2(x)} = \\tan(x) \\cdot \\sec(x)$$ $$\\frac{d}{dx} \\cot(x) = -\\csc^2(x)$$ $$\\frac{d}{dx} \\log_a(x) = \\frac{1}{x\\cdot \\ln(a)}$$ $$\\frac{d}{dx} a^u = a^x \\cdot \\ln (a) du$$ $$\\frac{d}{dx} \\sin^2(x) = \\sin(2x)$$ $$\\frac{d}{dx} \\cos^2(x) = -\\sin(2x)$$ $$\\frac{d}{dx} \\tan^2(x) = 2\\tan(x)\\cdot \\sec^2(x)$$\nIntegrals # $$\\int a^x dx = \\bigg(\\frac{1}{\\ln(a)}\\bigg) a^x + C $$ $$\\int \\tan(x) dx = -\\ln\\vert \\cos(x) \\vert + C$$ $$\\int \\tan^2(x) dx = \\tan(x) - x + C$$ $$\\int \\csc(x) dx = \\ln\\vert \\csc(x) - \\cot(x)\\vert + C = \\ln \\bigg\\vert \\tan\\bigg(\\frac{x}{2}\\bigg)\\bigg\\vert + C$$ $$\\int \\csc^2(x) dx = -\\cot(x) + C$$ $$\\int \\sec(x) dx = -\\ln\\vert \\sec(x) + \\tan(x)\\vert + C$$ $$\\int \\sec^2(x) dx = \\tan(x) + C$$ $$\\int \\cot(x) dx = \\ln\\vert \\sin(x) \\vert + C$$ $$\\int \\cot^2(x) dx = -\\cot(x) - x + C$$ $$\\int \\frac{1}{\\sin(ax)\\cos(ax)} = \\frac{1}{a} \\ln\\vert\\tan(ax)\\vert + C$$ $$\\int \\frac{1}{x\\sqrt{x^2-a^2}} dx = \\frac{1}{a} \\sec^{-1}\\bigg( \\frac{\\vert x \\vert}{a}\\bigg) + C$$ $$\\int \\frac{1}{\\sqrt{a^2-x^2}} dx = \\sin^{-1}\\bigg( \\frac{x}{a} \\bigg) + C$$ $$\\int \\frac{1}{a^2 + x^2} dx = \\frac{1}{a} \\tan^{-1}\\bigg( \\frac{x}{a} \\bigg) + C$$\nMany more integrals\nTriangle Sub # $$\\sqrt{b^2x^2-a^2} \\Longrightarrow x = \\frac{a}{b}\\cdot\\sec\\theta; \\theta \\in [0, \\pi/2), (\\pi/2, \\pi]$$ $$\\sqrt{a^2-b^2x^2} \\Longrightarrow x = \\frac{a}{b}\\cdot\\sin\\theta; \\theta \\in [-\\pi/2, \\pi/2]$$ $$\\sqrt{a^2+b^2x^2} \\Longrightarrow x = \\frac{a}{b}\\cdot\\tan\\theta; \\theta \\in (-\\pi/2, \\pi/2)$$\n"},{"id":65,"href":"/mcb-c61/1/","title":"1: Evolution of the Mind, the Brain, and Brain Chemistry","section":"MCB C61","content":" \\(\\) Origins # Key Idea; How long has the \u0026lsquo;human mind\u0026rsquo; been around?\nBrief History # We have records of keeping track of astronomical events, e.x. Movement of the moon and sun across the sky Changes of the length of day Equinoxes, Solstices became ritual dates long ago (typically involved with religion) Phases of the moon (new/full moons) Stonehenge Rock structure in England 4,500 years old Speculated that this may have been an ancient astronomical observatory Cave paintings 15,000 \u0026ndash; 40,000 years old \u0026ndash; Paleolithic Discovered in December 1994 Chauvet, Lascoux, and others across SW Europe People went through an effort to go deep into caves with lamps to create these drawings \u0026ndash; thinking in sophisticated ways Ancient Music Have evidence of an ancient flute made from a bear bone 50,000 years ago Human evolution # Oldest fossils of a direct human ancestor are 5-6 MYA Volcanos spew radioactive elements which become buried Estimated that humans and chimps have evolved independently since 7 MYA Hominin fossils fall into 3 primary groups, or genera: Ardipithecus Climbs in woodlands Can walk on two legs Australopithecus Committed biped Small brain Big teeth and faces Homo Our genus \u0026ndash; homo sapiens \u0026ndash; are the only species remaining today Technological primate; depends on culture Found burial of dead (pets too) along with primitive instruments Species Age Brain Size cm$^3$ Ardipithecus ramidus ~ 4.4 million years ago 350 Australopithecus afarensis ~3 to 4 million years ago 500 Australopithecus africanus ~2 to 3 million years ago Australopithecus robustus ~1 to 2.5 million years ago Homo habilis ~ 1.4 to 2.3 million years ago 650 Homo erectus ~ 200,000 to 1.9 million years ago 1200 Homo neanderthalensis ~ 30,000 to 300,000 years ago 1400 Homo sapiens ~ 200,000 years ago to now 1400 In addition to becoming larger specimen, brain size has increased rapidly 2 MYA of evolutionary history; resulting in ever more sophisticated behaviors. i.e\u0026hellip; Bipedality Earliest trait Change in hip/pelvis structure Tool use Nuanced social interaction Language Mathematical skill Complex problem solving abilities Capacity to construct elaborate explanatory frameworks to aid in understanding our world Behavior and Intelligence # Complex behaviors may be an indictor of intelligence\nSurvival of the Kindest # Humans are capable of.. Compassion Empathy Social interaction and connection Caregiving Potential Explanation Mammals and birds put in an incredible amount of energy into caring for their youn This drives the nervous system to evolve for this This behavior generalizes to complex social interactions \u0026ldquo;Those communities which included the greatest number of the most sympathetic members, would flourish best, and rear the greatest number of offspring \u0026ndash; Charles Darwin (1871)\nEven though, biologically, caring traits are more beneficial than our fearful and violent impulses, our culture drives our choices Our evolutionary experiment is still running\u0026hellip; what qualities will be emphasized and selected for by our culture? 2001: A Space Odyssey # Written by Stanley Kubrick and Arthur Clarke in 1968 Clarke\u0026rsquo;s three laws give insight into the way we think about the world scientifically When a distinguished but elderly scientist states that something is possible, they are almost certainly right. When they state that something is impossible, they are very probably wrong. Generalizable The only way of discovering the limits of the possible is to venture a little way past them into the impossible. Any sufficiently advanced technology is indistinguishable from magic. Context of aliens How would we ever know, even if we say it? Consider natural phenomena we can\u0026rsquo;t explain Movie shows hominin evolution starting 5 MYA Hominins are human(-like) animals, including us and our ancestors First scene shows the first act of intentional and willing act of killing on conspecifics Human deaths due to violence in the 20th century: 150,000,000 - 200,000,000 Which transitions to\u0026hellip; HAL, the supercomputer, intentionally and willingly killing off the spaceship\u0026rsquo;s crew HAL certainly appears to be operating as if it has intentions, reasoning, and even emotions This begs the question, Does HAL have a mind? Which leads too\u0026hellip; What is Mind? # At a top level, our subjective experience: our mental states\u0026hellip; Thoughts Feelings Perceptions (visual, auditory, olfactory, gustatory, tactile) Mind is inherently and irreducibly subjective: It is first-person, internal You need a subject, an \u0026rsquo;experiencer\u0026rsquo;, to have consciousness; that is, awareness of our mental experiences - Consciousness and mind are often used interchangeably What is it like to be? Freudian unconscious is Sigmund Freud’s (1856-1939) concept of how cognitive content out of our awareness may nonetheless have substantial impact on our behavior. Because this content is mental, it has the potential to enter awareness This is the goal of psychoanalysis: to bring unconscious things related to one’s behavior into consciousness, into awareness, where they can be subjected to analysis and become amenable to change So to ask if HAL has a mind is to ask if HAL has an experience of what it is like to be HAL Even if HAL can exhibit complex and intentional behaviors, we have no way of knowing if this accompanied by mental experience What necessitates the mental experiences? Leads to thee\u0026hellip;. Mind-Body Problem # What is the mind\u0026rsquo;s relation to our physical processes in the brain and body? The human capacity for mental experience is related to the functioning of our brain and nervous system Does this mean that something akin to a brain and nervous system is necessary for mental experience, for mind? Most scientists believe this is the case, but we have no concrete evidence How do we begin to address this as a question of scientific investigation? Consciousness is what makes the mind-body problem really intractable\n\u0026ndash; Thomas Nagel, What Is It Like to Be a Bat?\nWhat is our science-based description of reality? Metaphysical Framework # Science literally means to seek knowledge, knowing.\nInitially drawn to observing the stars: start of astronomy Which lead to physics, rooted in math Giving birth to chemistry Started as tracking properties of matter Evolved into our fundamental framework of atoms, molecules, electrons and biology, the study of living organisms \u0026ndash; which are the product of specific configurations of atoms and molecules Which neuroscience emerges from! Nowadays, we scientifically follow physical materialism (physicalism) \u0026ndash; we conceive of the world ultimately being made of matter and atoms, made up of protons/neutrons/electrons, made up of quarks/neutrinos/mesons These abstract particles are the fundamental \u0026lsquo;stuff\u0026rsquo; of our universe They energetically interact all the time and manifest these larger, more complex structures Reductionism Approach # Our metaphysical framework is a form of reductionism: how we \u0026lsquo;understand\u0026rsquo;\nWe believe we can explain macro systems by their micro components/processes We (believe we can) explain the function of living organisms by\u0026hellip; Looking at their cellular chemistry, molecular make-up (reducing their neuroscience and biology to chemical components) We can then explain these components and their interactions with physics This is very powerful and has led to many discoveries \u0026ndash; but where is mind? Personal experience is the one objective truth to each of us It\u0026rsquo;s a product of our body and nervous system, thus we should be able to somehow derive it This is our only current lead, but will be suffice? Our current metaphysical framework is largely unconscious Nervous Systems and Brains # Mind-Brain Relation # Who are we? How are we related to everything else we believe we know and understand about the universe?\nModern science utilize\u0026rsquo;s an explanatory framework to investigate the physical and chemical interactions in living organisms William James (1842-1910) Pioneer in study of mind, wrote in his 1890 book The Principles of Psychology: If the nervous communication be cut off between the brain and other parts, the experiences of those other parts are non-existent for the mind. The eye is blind, the ear deaf, the hand insensible and motionless. And conversely, if the brain be injured, consciousness is abolished or altered, even although every other organ in the body be ready to play its normal part.\nThat is, damage to the brain is associated with specific changes in mental functioning. Therefore, wherever mental experience is coming from, the brain is clearly involved. The nervous system: network that functions to manipulate external and internal information. Specialized for rapid communication of signals throughout the body Our brain is considered to be the locus of central control in the nervous system Animals presumably have nervous systems to facilitate movement through the world. Collection and analysis of sensory information and coordination with the mechanisms of movement are needed to safely accomplish the task of moving around in an environment that is often challenging and sometimes unpredictable. Plants and fungi, the other kingdoms of multicellular organisms, do not move around in the fashion of animals and have evolved other, nonneural ways to flourish. The human brain (together with the brains of a few other mammals) is perhaps the most complex structure known. Composed of several hundreds of billions of cells interconnected by hundreds of trillions of connections. Each connections is a locus of signal transfer between cells Brains are made up of two general classes of cells: neurons and glia (Fig. 2.1). It is currently estimated that there are around a hundred billion ($10^{11}$) nerve cells (neurons) in the brain and at least that number of glial cells (glia) The cellular units of signal transmission are generally considered to be the neurons Although many glia, especially the astrocyte glia, are also directly involved in signaling. Figure 2.1. Nerve cell (left) and astrocyte glial cell (right).\nAnimal Nervous Systems # \u0026hellip; have been undergoing evolutionary refinement for hundreds of millions of years, and millions of years were required for the complexity of the human brain to develop among the primates.\nSponges # Have been around for around a half a billion years Do not contain nerve cells, thus no nervous system Hydra # Aquatic animals Simple nervous system: loosely connected network of a small number of cells, enabling simple signal communication Typically smaller than an inch Caenorhabditis elegans # Tiny nematode (roundworm) Since the 1970s has been widely studied by biologists interested in molecular, cellular, and developmental biology About 1 millimeter in length Relatively simple organismal structure, at least for an animal that is able to manifest some degree of behavioral sophistication Can navigate through soil environment using olfactory and thermal cues Simple nervous system Only 302 neurons Location, connectivity, and developmental history of each have been determined by researchers. That is, the complete wiring diagram of the C. elegans nervous system is known. Insects # Have complex brains that execute complex behaviors Fruit flies (Drosophila) has been extensively studied by biologists for more than a century Has been a focus of neurobiological research for several decades now Brain is \u0026gt;0.5 mm in width + contains around 150,000 neurons. Jellyfish # Distant cousins of hydra Possess relatively simple neural networks Figure 2.2. Compass jellyfish, Chrysaora hysoscella, from 1904 book Kunstformen der Natur (Artforms of Nature) by Ernst Haeckel.\nPlanaria # Flatworms found in both aquatic and terrestrial environments More complex in structure than nematodes Typically several millimeters long The planarian nervous system contains.. Extended network of interconnected neurons Two clusters of neurons at the head end of the worm (Fig. 2.3). Some neurobiologists consider these clusters to represent a primitive brain. Figure 2.3. Planarian nervous system.\nContemporary Vertebrate # Hundreds of millions of years of evolutionary variation and selection have led to these brains The basic structure of all vertebrate animal brains is similar \u0026ndash; can be represented as shown in Figure 2.4. Develops in the embryo when a tubular structure (the neural tube) folds in and then closes off and expands at one end (on the left side in the diagram in Fig. 2.4). The interior spaces of the tube will become the ventricles (fluid-filled internal spaces) in the mature brain. Figure 2.4. Basic plan of the vertebrate brain\nThe three regions expand in the mature brain to contain millions of cells organized into distinct anatomical structures The forebrain is dominated by the cerebrum The midbrain is dominated by the optic tectum The hindbrain is dominated by the medulla + cerebellum Progressing from older fish, amphibians, and reptiles to (evolutionarily) more recent birds and mammals, the size of the cerebrum increases relative to the rest of the brain (see Figs. 2.5 and 2.6). Figure 2.5. Brains of a fish (left) and a bird (right). Many mammalian brains are distinct in that the structure of the cerebrum has bumps and groves (rather than being smooth)\nThe bumps and grooves are called gyri (singular: gyrus) and sulci (singular: sulcus) Due to cerebrum being a folded structure No folding: Mice, rats, and squirrels Folding: Capybaras (the world’s largest rodents), dogs, cats, and primates Figure 2.6. Brain of a mouse (a mammal); the cerebrum has covered over the midbrain (compare with Fig. 2.5).\nBrain Features # Cerebral Cortex # Cerebral cortex: the outer layer (cortex in Latin means bark, or outer layer).\nHuman\u0026rsquo;s cerebral cortex Sheet of neural tissue, around 3 millimeters thick (about 1/8 inch) Highly folded so that its large size can fit inside the skull Surface area of ~ 2.5 square feet (newspaper) Eight lobes (two on each hemisphere) compose the cerebral cortex: Frontal Parietal Occipital Temporal Figure 2.7. Human brain: top view (dorsal, left) and side view (lateral, right) Regions # Most prominent landmark grooves: Longitudinal Fissure: Divides the right and left cerebral hemispheres; Most prominent Central Sulcus: Separates the frontal lobe from the parietal lobe Lateral Fissure: Separates the temporal lobe from the frontal and parietal lobes Corpus callosum: bundle of approximately 200 million nerve fibers connecting the right and left cerebral hemispheres Diencephalon: located between the base of the cerebral cortex and the midbrain, consists largely of the thalamus and hypothalamus Brainstem: Composed of the medulla, pons, and midbrain Some definitions also include all or part of the diencephalon Other definitions include the cerebellum. Figure 2.8. Human brain: underside (ventral, left) and split (down longitudinal fissure \u0026ndash; medial, right) External Protection # Andreas Vesalius (1514-1564) Physician who lived and taught in Italy Wrote book featuring some of the first, quality human anatomy drawings, published in 1543 in De Humani Corporis Fabrica \u0026ndash; On the Fabric of the Human Body Meninges: Three layers between skull and brain: Dura mater: Skin-like sheet of tissue that covers the brain; from the Latin words meaning \u0026ldquo;hard or tough mother.\u0026rdquo; Arachnoid: Delicate layer of tissue covering the brain below dura; from the Latin meaning \u0026ldquo;like a spider web.\u0026rdquo; Pia mater: Third delicate layer; Latin for \u0026ldquo;soft or tender mother.\u0026rdquo; Meningitis: Condition caused by inflammation (i.e through infection) of meninges; extremely serious occurrence because of its close proximity to the brain. Figure 2.9. Open human skull with dura intact (left) and with dura peeled back (right);\nFrom Andreas Vesalius’s De Humani Corporis Fabrica (1543).\nSub-arachnoid space: \u0026hellip;between the arachnoid and pia layers; contains\u0026hellip; Cerebrospinal fluid: liquid that cushions the brain inside the skull + transports soluble substances throughout the central nervous system Fiber Pathways # Vesalius, as well as other early anatomists, noted that the brain was highly interconnected via fiber pathways Notable connections between Brain with the sensory organs: the eyes, ears, nose, and tongue Brain with the heart, and lungs, and digestive system Spinal cord (which was contiguous with the brain) with muscles throughout the body Seemed likely there was extensive communication between the brain and the rest of the body. Figure 2.10. Autonomic and cranial nerve fibers connecting the brain and body (left), and nerve fibers connecting the spinal cord and the body (right), from Andreas Vesalius’s De Humani Corporis Fabrica (1543).\nRené Descarte\u0026rsquo;s (1596-1650) Research # Early thinker on signaling in the nervous system; studied\u0026hellip; How the human body worked How we are able to perceive the world How the actions of the body are related to the subjective mental experiences of mind Among his early writings, when he was in his thirties, were essays on the world (Le Monde) and on man (L’Homme) Worried about publication in part because these essays addressed big questions about the nature of reality, perception, and mind. At the same time (1630s) Galileo Galilei (1564-1642) was being tried and sentenced by the Catholic Inquisition for heresy as a result of his writings on related topics of grand scope. Figure 2.11. From L’Homme de René Descartes (1664): a person’s foot gets close to the fire, a signal is sent from the foot to the head. From the head another signal goes back to the foot and generates movement, causing the person to pull away from the fire. One theory was that this action was driven by fluid pressure of some kind, e.x. liquid or air. The fluid would be heated by the fire, resulting in increased pressure that forces it through channels up to the head and then back again. Now we know that nervous system signalling involves the movement of charged particles In Descartes’s time, electricity was just beginning to be understood. 1600s: Electricity was introduced into the English lexicon; Its origin being the Greek word for amber \u0026ndash; elektron \u0026ndash; the fossilized tree resin known for its properties of attraction: rub a piece of amber with a cloth, and the amber will attract light materials, such as hair or small pieces of paper. 1700s: Scientific luminaries were researching and writing about electrical phenomena. Descartes was very interested in trying to understand perception \u0026ndash; how is it we are able to sense the world, and how is it that physical sensations lead to mental experiences? It seemed the eye captures light which signals somehow to the brain\u0026ndash; but then what? How can the physical sensations of light lead to our mental experience of the world—to visual perception? Still an outstanding question now, four centuries later. Figure 2.12. Dissection of muscles surrounding the eyeball\nFigure 2.13. Visual perception and the action of pointing a finger\nElectricity # Figure 2.14. Two figures from Galvani’s 1791 publication, depicting experimental devices and arrangements, together with dissected frog legs. Luigi Galvani (1737-1798) Found found legs severed from dead frogs could twitch when electrically stimulated Hypothesized that muscles move as a result of internal electrical forces that can be triggered by external electrical stimulation (left) Published in 1791: De viribus electricitatis in motu musculari, commentarius. \u0026ndash; \u0026ldquo;Commentary on the effects of electricity on muscular motion.\u0026rdquo; Galvani’s nephew, Giovanni Aldini (1762-1834), continued his uncle’s work and contributed to increased public attention to connections between electricity and life. Around the same time, fellow Italian Alessandro Volta (1745-1827) invented the first battery. Near the end of the 1800s, the volt was named after him. In the mid-1800s, Scottish physicist James Clerk Maxwell (1831-1879), building on the work of Hans Christian Oersted (1777-1851), André Ampére (1775-1836), Michael Faraday (1791-1867), and others, derived mathematical relations that provided a unified description of electricity and magnetism. Important because\u0026hellip; Light could now be understood as a propagating wave of electromagnetic energy Electricity was became increasingly central to all of physics. As electricity became more a focus of experimental investigation, Galvani’s suggestion that neural signaling was electrical in nature continued to catch on. 1850: the German physician and physicist Hermann von Helmhotz (1821-1894) measured the speed of an electrical signal moving along a frog\u0026rsquo;s nerve fiber, finding it to be about $100 \\text{ kph}$ A few years later, Walt Whitman (1819-1892) wrote a famous poetic line: \u0026ldquo;I sing the body electric.\u0026rdquo; Nerve Fibers # Nerve cells have a basic construction and biochemistry similar to all cells for all of life on Earth. Examples: Chromosomes containing genetic information, coded into the molecular structure of DNA. Ribosomes where synthesis of proteins and their transport to desired locations within the cell. Mitochondria which are dedicated to the generation of cellular energy. All this is enclosed by a cell membrane: the structure that forms the boundary of the cell. Within the cell there is also elaborate molecular scaffolding: composed of protein polymers called microfilaments and microtubules. Far from being a bag of disorganized fluid protoplasm, a living cell is a highly ordered system of vast complexity.\nNerve cells (neurons) have dendrites and axons, specialized for the communication of signals from cell-to-cell Nerve fibers (the thread-like structures connecting the brain with various parts of the body, Observed by Vesalius and Descartes in their dissections) consist mainly of bundles of axons. Figure 2.15. Nerve cell, with axon and dendrites. Microscopic analyses of the brain revealed it to consist of densely packed neurons and glial cells. The elaborate interconnectivity of nerve cells in the brain was gorgeously illustrated in the drawings of two great pioneers of neuroscience, Camillo Golgi (1843-1926) and Santiago Ramon y Cajal (1852-1934), working in their respective countries of Italy and Spain at the beginning of the 20th century Both were using a technique for staining neurons developed by Golgi in 1873, called\u0026hellip; Figure 2.16. Human cerebellar neurons drawn by Golgi (L) and neurons from human cerebral cortex drawn by Ramon (R)\nLa Reazione Nera # While working as a physician in a large 500+ patient psychiatric asylum, Golgi, developed, in his \u0026ldquo;spare time\u0026rdquo;, his famous stain la reazione nera \u0026ndash; the black reaction.\nMade neurons, together with all their dendrites and axons, eminently visible under the microscope. Here is the recipe, in brief, of his technique: The dark crystals of silver chromate stain the neurons completely, rendering even very minute structures exquisitely visible. (In some variations of the procedure, glial cells are also stained.) If every neuron in a small piece of brain tissue was stained in this way, the result would be a glob of dark-colored mess, so densely packed with silver chromate that it would be useless to look at in hopes of seeing individual cells. Woefully inefficient: ~1% of neurons are stained! These 1% are stained well while the others aren’t stained at all. Still not known why only some neurons are stained! Appear to be a random subset May have to do with being recently (in)active\u0026ndash; or something else about the recent state of the cell’s physiology 20th Century # Brains and nervous systems were understood as sophisticated networks connecting sensory information with movement, having evolved in the animal kingdom to facilitate survival while moving around in complex and challenging environments. Signaling in the nervous system was electrical in nature: it involved the movement of charged particles These charged particles were likely to be atomic in nature. The brain was recognized as somehow central to the functioning of the mind—mental experience and consciousness. Developments in the sciences of physics and chemistry made it increasingly attractive to try to understand the phenomena of life at as microscopic a level as possible—that of cells and molecules. There were many mysteries, many questions—but the future of the life sciences looked bright:\nNerve currents sparkle.\nA trillion nodes resonate.\nThe mind engages.\nChemistry and Life # To understand how nerve cells work \u0026ndash; how they generate signals and pass information from one cell to another \u0026ndash; it is essential to introduce a few basic concepts of chemistry\nMatter: described in terms of its chemical elements, understood as atoms, which composed of protons, neutrons, and electrons Chemistry: scientific field which investigates the conditions of how elemental constituents (atoms) interact to form larger entities called molecules. Alchemy # What chemistry involved from Also concerned with the nature and transformation of matter Involved with processes i.e. extraction, conversion, fermentation, distillation E.x. metalworkers who \u0026lsquo;magically\u0026rsquo; extracted metals from rock, and physicians who prepared extracts and essences from plants for healing the body and mind Some alchemists sought the Philosopher’s Stone: a legendary substance rumored to transformation common metals (e.x iron, lead) into precious metals (e.x. gold and silver) More importantly, there was a sect of alchemy concerned with investigation the psyche; a transformation of one’s self and the human psychological condition \u0026ndash; a psychotherapy or vision quest In this context, the Philosopher’s Stone is understood as a means for self-transformation; the method the alchemical practitioner achieves an integrated wholeness. Chemistry Progression # Alchemy evolved into chemistry throughout the 17th and 18th centuries, becoming devoid of psyche Robert Boyle (1627-1691) and Isaac Newton (1642-1727) considered themselves alchemists, and both sought the Philosopher’s Stone But Boyle and Newton also made contributions to a new conception of the cosmos, one independent of mind or magic. By the end of the 1700s the transition was nearly complete Joseph Priestley (1733-1804) published works on electricity, carbonated water, and gases Priestley’s primary vocation was as an educator and minister He was a dissenter from the Church of England and a supporter of the American and French Revolutions Laboratory destroyed by mob, fled in 1794 to rural Pennsylvania Antoine Lavoisier (1743-1794) published what some consider the first modern text on chemistry \u0026ndash; Traité Elémentaire de Chimie, Elements of Chemistry in a New Systematic Order; a repository containing all current Discoveries. Developed chemical nomenclature still in use today Created a comprehensive listing of all the chemical elements known in his time. Arrested during the French Revolution and executed by guillotine in 1794 Periodic Table # 1869: Russian chemist Dmitri Mendeleev (1834-1907) organized known chemical elements; created the periodic table\nOne of the great achievements of the human intellect, representing a large amount of information in a very compact form Predicted the existence of several not-yet-discovered chemical elements through gaps in organizational scheme - E.x 3 elements, discovered later, are now called gallium, germanium, and scandium Chemical elements are represented by abbrvs: H : hydrogen He : helium Li : lithium C : carbon N : nitrogen O : oxygen Na : sodium Mg : magnesium P : phosphorus S : sulfur Cl : chlorine K : potassium Ca : calcium Fe : iron U : uranium Bk : berkelium Identity of an element is determined by the number of protons (a type of positively charged subatomic particle) within its nucleus Figure 3.1. Basic form of the modern periodic table of the chemical elements. What are the chemical elements that make up living matter?\nCarbon is the primary structural atom for building the large molecules making up living organisms, But it\u0026rsquo;s not the most numerous \u0026ndash; that\u0026rsquo;s water, which composes around 65% of humans Water is essential for life. CHNOPS 10 other elements also present \u0026amp; essential for healthy human life: magnesium, manganese, iron, cobalt, nickel, copper, zinc, selenium, molybdenum, and iodine These are found in much smaller amounts It is interesting to speculate as to what other forms of life might be possible Life without water, i.e silicone 10 Elements of Human Life: # By count By Mass Mass% hydrogen oxygen 65% oxygen carbon 18.5% carbon hydrogen 9.5% nitrogen nitrogen 3.2% calcium calcium 1.5% phosphorus phosphorus 1.0% sulfur potassium 0.4% sodium sulfur 0.3% potassium sodium 0.2% chlorine chlorine 0.2% Atoms have a neutral electric charge The positive nucleus is balanced by the negative charge of electrons in orbital clouds surrounding the nucleus. Ions: Charged atoms Hydrophilic: the electric charge on an ion will attract polar water molecules to gather around it Formed when atoms either gain or lose one or more electrons Thus, ions have either a net negative or positive charge Cation Positively charged \u0026ndash; easily give up electrons Far left side of periodic table E.x sodium, potassium, calcium Anion Negatively charged \u0026ndash; tend to take on electrons Far right side, except for the last column E.x. chlorine Noble (inert) Gases Right-most column of periodic table Neutral; Have outer orbitals that are completely filled with electrons so they tend to neither gain nor lose electrons Chemical interactions between elemental atoms depend on the gaining, losing, or sharing of electrons, these elements don\u0026rsquo;t inter/re-act with anything Because of their lack of reactivity, not involved in any known way as part of the life process. Molecules # Water # Water is the canvas upon which life is painted, the landscape or stage upon which the molecular drama of life is played out.\nWater is an example of a molecule: a stable configuration of atoms held together in a particular geometric shape by the sharing of electrons between atoms Water = H$_{2}$O = H-O-H Each hydrogen atom contributes one electron and the oxygen atom contributes two electrons for mutual sharing. This sharing of electrons between atoms is called a covalent (chemical) bond Shared electrons form a \u0026lsquo;glue\u0026rsquo; that holds the atoms together. Simple enough that electrons can be shown, but molecules are more commonly depicted as “ball” structures or “space filling” structures. Beyond Water # The molecules from which living organisms are built and the molecules that interact with living organisms are often more complex than the simple water molecule\nFluoxetine Substance synthesized by chemists, marketed as a treatment for the mood disorder called depression. Associated with the brand name Prozac Drawn to the right in a diagrammatic way that has been developed in organic chemistry to represent molecules Understanding Organic Molecules # Molecules produced by life (organic molecules, organic from organism) are composed largely of carbon and hydrogen and may contain dozens or hundreds or thousands of atoms Largely from these four elements \u0026ndash; carbon, hydrogen, oxygen, and nitrogen \u0026ndash; are built the basic structures of an enormous number of biologically interesting molecules: Hydrogen Has one electron to share, so can form only one chemical bond at a time. Has no possibility of forming the scaffold for a molecule Can only be stuck around the edges Carbon Form the scaffold that defines the overall shape of the molecule Has four electrons available for sharing, that is, each carbon atom can form four bonds and be covalently joined to up to four other atoms This allows carbon to form the structural framework for molecules that can become very large Oxygen Oxygen has two electrons to share and so can form two bonds. Nitrogen Nitrogen has three electrons to share and thus is able to form three bonds Example: Ethyl Alcohol (Ethanol) # Primary psychoactive component of alcoholic beverages Small organic molecule composed of 2 carbon atoms (C), 1 oxygen atom (O), and 6 hydrogen atoms (H) joined together by chemical bonds Covalent chemical bonds, the sharing of electrons between atoms, are represented by lines connecting the atoms Each carbon has four chemical bonds, reflecting four electrons to share; each hydrogen has one chemical bond, and the oxygen has two chemical bonds The particular molecular shape produced by this geometric arrangement of atoms confers upon ethanol its peculiar properties. Molecular structure diagrams show covalent bonds as lines. If there is no letter at the end of a line, assume a carbon atom is there. Any bonds not shown explicitly are assumed to be hydrogen, enough hydrogens so the total number of bonds per carbon equals four. Hydrogen atoms forming bonds with other elements are explicitly drawn in. Here are some other relatively simple organic molecules \u0026ndash; relatively simple in that they are built from linear chains of carbon and hydrogen, and nothing else Organic molecules composed solely of carbon and hydrogen are called hydrocarbons. The simplest combination of carbon and hydrogen consists of a single carbon atom bonded to four hydrogen atoms This molecule is called methane It is a gas that is combustible (burnable) in the presence of oxygen In fact, it is so-called natural gas, obtained from fossil fuel deposits in the earth and shipped into our homes by pipeline to be burned in stoves and furnaces The combination of two carbons and six hydrogens is called ethane; it is also a combustible gas Three carbons and eight hydrogens make up propane, also a combustible gas that easily liquefies under pressure and thus can be more easily stored and transported in tanks Four carbon atoms and ten hydrogen atoms form butane, an even more easily liquefied combustible gas Five carbons make pentane and six carbons make hexane, both combustible liquids Seven carbons make heptane and eight carbons make octane, also both combustible liquids As the number of carbons grows larger, the liquids develop an oily consistency and get progressively thicker. When the number of carbon atoms reaches twenty or more, the resulting substance is a waxy solid. These molecules are fossils \u0026ndash; molecular remnants originating with living organisms millions of years, or hundreds of millions of years, ago Geological processes have transformed the once living material into hydrocarbons Crude oil \u0026ndash; petroleum (Latin petra = rock, oleum = oil) \u0026ndash; is composed of a mixture of all these molecules and many more. Petroleum refineries separate crude oil into its molecular components, making them available to be used as fuels and other materials in the modern industrial world All these molecules are combustible \u0026ndash; that is, in the presence of oxygen they can burn and release energy as the covalent bonds connecting the carbons and hydrogens are broken. Fossil fuel Complete combustion will break all of the carbon-carbon and carbon \u0026ndash; -hydrogen bonds and convert the hydrocarbon into a mixture of carbon dioxide (CO) and water. Carbon, as well as some other atoms, also has the capacity to participate in bonds with other atoms in which it shares more than one electron For example, in the ethylene molecule each carbon contributes two (rather than one) electrons to the carbon-carbon bond, forming what is called a double bond, drawn with a double line connecting the atoms. Ethylene is also a combustible gas, often used in welding However, the most widespread use of ethylene is to make polyethylene plastics by linking many ethylene molecules together in very long chains (polymers) of various shapes and forms. The arrangement of carbons in a hydrocarbon molecule is not always linear The chains of carbons may be branched, such as in this branched octane molecule. The chain of carbons may even fold back upon itself to form a closed ring Here is an example of that, the cyclohexane molecule. Note that in all cases each carbon atom still forms four covalent chemical bonds and each hydrogen atom forms one covalent bond. There is another cyclic arrangement of six carbons that occurs commonly in nature The benzene molecule contains six carbons ina ring structure together with six hydrogens (rather than twelve hydrogens, as in cyclohexane). The existence of such a combination of six carbons and six hydrogens was puzzling until Friedrich Kekulé (1829-1896) suggested a novel structure in 1865 for benzene. It is now appreciated that, rather than an alteration of single and double bonds, all the carbon-carbon bonds in benzene are equivalent and are sort of intermediate between single and double bonds in strength The shared electrons are best described as existing in molecular orbitals that encompass the entire ring structure To represent this, the structural diagram for benzene is sometimes drawn witha circle inside the hexagon of carbons While benzene itself does not occur in living organisms (it is a highly toxic and combustible liquid), the ring structure of benzene does occur widely as part of larger structures of many biological molecules Let’s look at several of these that have relevance to the human nervous system. Here are two famous molecules found in the brain, the neurotransmitters dopamine and serotonin. While not the most abundant neurotransmitters in the human brain, these two molecules are perhaps the best known of the neurotransmitters, due to their frequent mention in the news media If you were to randomly stop people on the street and ask them to name a neurotransmitter, if they had any idea at all as to what you were asking, they would likely answer with the name of one of these two molecules. Notice that within the diagrams for these molecules the benzene ring structure occurs Also notice that each of these molecules consists of a bunch of carbon atoms, several nitrogen and oxygen atoms, and a bunch of hydrogen atoms around the periphery of the molecule, all joined by chemical bonds so that a particular geometric shape results Thus, these molecules are appreciated to be just a bunch of atoms (mostly carbon and hydrogen) connected together by the sharing of electrons (covalent chemical bonds) to form a particular geometric shape It is their unique shapes that determine the properties of particular molecules within, say, the human nervous system. If you spend time drawing a few molecular structures like those of dopamine and serotonin, you very quickly appreciate that mostly what you are doing is drawing lots of Cs (representing carbon atoms) and Hs (representing hydrogen atoms). There may be a few other atoms, like the several oxygen and nitrogen atoms in dopamine and serotonin, but mostly there are Cs and Hs, lots and lots of Cs and Hs. There can be so many Cs and Hs that it becomes difficult to notice the most important things about the molecule, such as the overall shape and what kinds of other atoms are part of the structure Thus, chemists have developed a shorthand language for depicting the structures of organic molecules In this shorthand language, the structures of dopamine and serotonin look like this: Here we are simply not drawing the Cs and most of the Hs. We are drawing only the bonds connecting the various atoms together. Although the six carbon atoms in a benzene-ring structure are diagrammed with alternating single and double bonds, the shared electrons are best described as existing in molecular orbitals that encompass the entire ring structure. So now, at last, we can articulate the rules for drawing and interpreting molecular structure diagrams Covalent bonds are drawn as lines If there is no letter explicitly shown at the end of a line, then it is assumed there is a carbon atom in that location Thus, each line (bond) has a carbon atom at either end, unless another atom is explicitly drawn Atoms other than carbon are indicated: N for nitrogen, O for oxygen, P for phosphorus, F for fluorine, and so forth What about the hydrogen atoms Because we know that carbon forms four bonds, any bonds not shown explicitly are assumed to be with hydrogen, enough hydrogens so that the total number of bonds per carbon equals four Hydrogen atoms forming bonds with other elements (such as nitrogen and oxygen) are explicitly drawn in. One of the things we can easily do with this shorthand notation for molecular structure is compare similarities in shape between molecules For example, one can see that these molecules \u0026ndash; dopamine and norepinephrine, which are neurotransmitters; amphetamine, methamphetamine, and ephedrine, which are psychoactive drugs; and phenylalanine, an amino acid \u0026ndash; all share a basic similarity of shape, something about their gestalt, or form. In fact, they can all be chemically converted from one to another in straightforward ways, given the (R) conditions\nAnd because of their similar molecular shapes, there are connections between what these molecules do in living organisms. The three molecules below \u0026ndash; tryptophan, an amino acid; serotonin, a neurotransmitter; and psilocin, a psychoactive molecule from Psilocybe mushrooms \u0026ndash; all look somewhat similar to one another and different from the molecules pictured above. Finally, here is another famous molecule \u0026ndash; caffeine. One sees that it, too, is simply a combination of rings of carbon and nitrogen atoms, together with a couple of oxygen atoms, but witha distinctly different shape compared with the other molecules drawn above\nAgain, these differences in shape determine the different functions these molecules have in living organisms. Returning to water, a key reason that water is so important in living systems is that it is so very effective at dissolving things Why is this so The answer lies in a property of water known as polarity In the water molecule (H$_2$O), the hydrogen atoms, being from the (L) side of the periodic table, are prone to giving up their electron and becoming positively charged The oxygen, being from the (R) side of the periodic table, is prone to picking up electrons and becoming negatively charged The result of these tendencies is that when the electrons forming the covalent bonds in water are distributed in the molecular orbitals describing the bonds, they essentially spend more time in the vicinity of the oxygen and less time in the vicinity of the hydrogens. So the oxygen atom in water becomes slightly negatively charged and the hydrogen atoms in water become slightly positively charged This is what is meant by polarity Polar means separation, and in this case there is a separation of charge between different parts of the water molecule Water is a polar molecule. Water’s polarity causes the molecules to loosely stick to one another, a phenomenon called hydrogen bonding \u0026ndash; the slightly negative oxygen atom of one water molecule is attracted to the slightly positive hydrogen atom of another (Opposite electric charges are attracted to one another.) This results in a matrix of water molecules held together in a loose way by hydrogen bonds, depicted here as dashed lines. Hydrogen bonding is noncovalent \u0026ndash; it does not involve sharing of electrons The water molecules are able to easily slip and slide past one another, producing the wateriness of liquid water When water is heated the energy of the heat causes the molecules to jitter and shake more vigorously At the boiling point, the jittering becomes vigorous enough to overcome the hydrogen bonding between water molecules, and the H$_2$O molecules escape as gaseous water \u0026ndash; steam Cool the water, and the molecular vibration lessens until, at the freezing point, the water molecules lock into a rigid matrix interconnected by hydrogen bonds \u0026ndash; ice The slight expansion of water when it freezes is due to the rigid matrix of hydrogen bonds taking slightly more space than when the molecules of water can slip and slide past one another in their liquid state. The polarity of water also accounts for water’s amazing ability to dissolve many things, including ions This can be illustrated by considering what is called table salt, sodium chloride (NaCl). A sodium atom (Na), being from the (L) side of the periodic table, very readily gives up an electron to form a positively charged sodium cation (Na). A chlorine atom, being from the right side of the periodic table, will very readily take on an electron to form a negatively charged chloride anion (CI). Crystalline table salt, sodium chloride, can be described as an array of alternating sodium and chloride ions, held together by the electrical attraction of their respective positive and negative charges. In the absence of water, NaCl is an extremely stable structure However, introduce even a small amount of water and the NaCl will begin to dissolve, that is, fall apart into the water. This illustrates water’s extraordinary ability to dissolve ions. Because the charged ions are attracted to the opposite-charged portions of the polar water molecule, water molecules can slip into the otherwise very stable matrix of sodium and chloride forming the salt crystal As the salt crystal dissolves, the cations and anions become surrounded with polar water molecules, attracted to the ionic charges A saltwater solution is formed Water is very effective at dissolving all kinds of atoms and molecules that have a net electrical charge (such as ions) or, indeed, any polar molecule, because the positive and negative separation of charge in a polar molecule will be attracted to the positive and negative separation of charge in the water molecule. This allows us to introduce a very important concept into our description of solubility Water, a polar molecule, dissolves (or loves to hang out with) other things that have polarity or charge We call things that like to hang out with water hydrophilic (Greek hydro = water, philos = loving). Hydrophilic substances like to be around water, and water likes to be around them At the other extreme of solubility in water, consider the hydrocarbons These molecules consist only of carbons and hydrogens linked by covalent bonds When electrons are shared among carbon and hydrogen atoms, the electrons are essentially equally likely to be found in proximity to either type of atom Thus, there is no significant polarity, or separation of charge, in a hydrocarbon molecule and so nothing with which a water molecule can form a hydrogen bond Thus, hydrocarbons don’t dissolve in water This is illustrated by the well-known adage that oil and water don’t mix Substances such as hydrocarbons that don’t dissolve in water \u0026ndash; that don’t like to hang out with water \u0026ndash; are called hydrophobic (Greek phobos = fearing). Hydrophilic things like to hang out with other hydrophilic things, and hydrophobic things like to hang out with other hydrophobic things. 4 Fundamental Biological Molecules # Okay, so where is all this chemistry stuff taking us Let’s return to the cell Cells are the fundamental organizational units for all known living organisms There are some characteristics shared by all cells, from bacterial cells to cells in the human brain Among these shared features are a boundary membrane (composed of phospholipid bilayer), genetic material (composed of nucleic acids), ribosomal structures (for protein synthesis), protein receptors, pumps, and channels within the cell membrane, and so on The component materials, the building blocks of a cell, are molecules, and the machinery of life, that which characterizes a cell as living rather than nonliving, is understood to be the chemical processes taking place within Some of the molecules from which cells are constructed are quite large, consisting of thousands of atoms held together by covalent bonds Below four fundamental types of biological molecules are described: lipids, proteins, carbohydrates, and nucleic acids. Lipids # Primarily hydrogen and carbon atoms in long chains; roles include energy storage, signaling within and between cells, precursors for neurotransmitters and hormones, and formation of membranes. Phospholipids Includes phosphatidylcholine The phosphate group at the head is hydrophilic: will bind to water. The hydrocarbon lipid tails are hydrophobic: won’t bind to water. The fats or lipids (Greek lipos = fat) are medium-size molecules composed primarily of carbon and hydrogen atoms in long chains, generally sixteen to twenty-four carbon atoms long Often there are a few oxygen atoms at one end of the chain of carbons The roles of these molecules within cells are diverse and include energy storage (lots of energy is contained in the carbon-carbon and carbon-hydrogen bonds within the molecule), signaling within and between cells, precursor molecules the cell uses to make certain neurotransmitters and hormones, and formation of membranes enclosing cells and its interior structures. A fatty acid is a kind of lipid molecule consisting of a hydrocarbon chain with a carboxylic acid group (-COOH) at one end Palmitic acid is a sixteen-carbon fatty acid It is called a saturated fatty acid because all the carbons are fully bonded with hydrogen atoms; there are no double bonds Palmitic acid is very common in plants and animals. It draws its name from palm trees and is a major component in palm oils. Another example of a lipid molecule is oleic acid, an eighteencarbon fatty acid Its name comes from oleum, Latin for “oil.” The chemical composition of olive oil is more than 50 percent oleic acid. Oleic acid contains one double bond, and thus is an unsaturated (monounsaturated) fatty acid The double bond is located nine carbons in from the end of the hydrocarbon chain, a so-called omega-9 fatty acid. Because lipids are composed primarily of carbon and hydrogen atoms, they are largely hydrophobic in nature, preferring the company of other lipids rather than that of water Lipophilic (lipid loving) is synonymous with hydrophobic And conversely, lipophobic (lipid fearing) means the same as hydrophilic Oil and water don’t mix. Of great importance in living organisms are the phospholipids. These lipids are composed of two carbon-hydrogen chains (each chain generally sixteen to twenty-four carbons in length), joined together at one end by a group of atoms containing, in addition to the ubiquitous carbons and hydrogens, atoms of oxygen, phosphorus, and perhaps nitrogen These latter atoms form bonds having polarity and sometimes even electric charge Thus, phospholipids have a highly hydrophilic portion (the polar or electrically charged phosphorus-containing “head group”) and a highly hydrophobic portion (the two long nonpolar hydrocarbon chains, or “tail groups”). Among the most abundant phospholipids in the cells of animals and plants are the phosphatidylcholines, an example of which is shown here: The two hydrocarbon lipid tails, here both eighteen carbons long, may be of varying lengths The head group contains a phosphorus atom, multiple oxygen atoms, and a nitrogen atom that carries a positive charge The positive charge comes from an electron deficit arising from the nitrogen having four bonds with other atoms, rather than its customary three This configuration is called a quaternary amine. The peculiar structure of phospholipids, with their distinct hydrophilic and hydrophobic portions, gives rise to a remarkable phenomenon when these lipids are present in an aqueous (water) environment When surrounded by water, phospholipids will cluster together such that the hydrophobic/lipophilic tail groups associate with one another and the hydrophilic/lipophobic head groups associate with one another Moreover, the hydrophilic head groups want to be in contact with the water environment and the hydrophobic tail groups want to be protected from contact with the water environment To accomplish this, they form a double layer of phospholipid molecules (Fig 3.2): the hydrophilic head groups form the exterior surfaces (in contact with one another and in contact with the water environment), and the hydrophobic tail groups are inside the layers (in contact with one another and shielded from contact with the water environment by the hydrophilic heads). Figure 3.2. Diagram of phospholipid bilayer: the head groups, represented as small circles, are to the outside of the bilayer, and the hydrocarbon tails are inside. There would be water above and below the bilayer. Phospholipid bilayers form sheets in three dimensions that can fold to form enclosed surfaces separating two aqueous environments, such as the inside and outside of a cell Indeed, phospholipid bilayers constitute the membranes forming the boundary layers around all cells for all of life on Earth The cell membranes of bacteria, of plants, of mushrooms, and of brain neurons all have the same fundamental structure: a bilayer of phospholipid molecules It is one of the most beautiful and elegant structures in the known universe! These phospholipid bilayer membranes are very tiny structures, with thicknesses of only about 5 nanometers (5 x 10-9 meters, or five billionths of a meter). The phospholipid bilayer membranes of cells contain a diversity of protein molecules that serve a variety of functions vital to cells In nerve cells, among the various membrane proteins are ion channels, ion pumps, neurotransmitter receptors, and neurotransmitter reuptake transporters In an actual biological membrane, the density of membrane proteins can be quite high, and the proteins will have a variety of wild and crazy shapes (Fig 3.3). The molecules that make up a phospholipid bilayer are constantly jiggling as a result of thermal vibration, and there can be quite a lot of mobility of the proteins and other molecules within a biological mem- brane In many ways, these membranes are more like fluids in their properties than they are like solids. Protiens # Proteins are made of amino acids, which are molecule that contain both an amine group (-NH$_2$) and a carboxylic acid group (-COOH).\nProteins are large molecules built from amino acids linked into long chains by covalent chemical bonds, called peptide bonds\nSo, first, what is an amino acid? In organic chemistry, an amino acid is a molecule that contains both an amine group (-NH2) and a carboxylic acid group (-COOH). The amino acids used as protein building blocks by all life on Earth are characterized by having amine and acid groups linked to the same carbon atom; these are termed alpha-amino acids Here R represents a portion of the molecule containing other atoms Different amino acids are characterized by having different R groups The simplest amino acid has R = H This amino acid is called glycine The next simplest has R = -CH3. This is alanine Another amino acid, called phenylalanine, has an R group consisting of a carbon attached toa benzene ring Life on Earth uses twenty different amino acids (characterized by twenty different R groups) as the molecular building blocks of proteins. Figure 3.3. Phospholipid bilayer forming a biological membrane\nMembrane proteins are here depicted as amorphous, potato-like structures In actuality, proteins have a great deal of internal structure. Here is a representation of two amino acid molecules, with all the atoms drawn out explicitly: The R groups, R$_1$ and R$_2$, may be the same or different If amino acid molecules are simply mixed together in water and shaken up, nothing happens \u0026ndash; they do not spontaneously join together to form chains. However, in the appropriate environment inside living cells, amino acids can join together with peptide bonds to form a chain of amino acids called a polypeptide. Here is the molecular structure diagram of how two amino acids can join together into a dipeptide by the formation of a covalent bond between them This joining does not happen spontaneously but only under specific catalytic conditions found within the ribosomes of cells Ribosomes are structures inside cells that are built from proteins and nucleic acids They are the sites of protein synthesis within cells, with enzymatic activity of the ribosome facilitating, or catalyzing, the formation of covalent bonds between amino acids The diagram above shows that an -H and an -OH are removed in the formation of a peptide bond They will produce one molecule of water, HzO, which floats away You can also see that the two ends of the resulting polypeptide structure have an amine group and an acid group available to form additional peptide bonds Thus, it is possible to form very long chains of amino acids, covalently linked by peptide bonds Ribosomes provide the conditions for this to take place. When a bunch of amino acids link together to form a polypeptide chain, they fold around and form a shape Similar to water flowing downhill, the polypeptide chain folds in such a way so as to arrive at a lowest-energy configuration, determined by the myriad attractions and repulsions among the component atoms Thus, the chain of amino acids develops a unique three-dimensional shape, and it is this shape that will help determine how it functions in the cell Any chain of amino acids is called a polypeptide; if it is more than about forty amino acids long, then it is called a protein The threshold number for defining when a polypeptide becomes a protein is somewhat arbitrary Some might say thirty, some fifty. Figure 3.4 is a diagram of a small protein called myoglobin It is a chain of 153 amino acids that binds, stores, and transports oxygen molecules within animal muscles In this drawing, myoglobin is depicted as a ribbonlike structure representing the polypeptide chain and overall shape of its folding The chain of amino acids spontaneously folds into a stable configuration characterized by an energy minimum The result is a unique three-dimensional structure for the protein In addition to its 153 covalently linked amino acids, myoglobin contains a component called heme (not shown in this diagram), a planar molecule embedded within the protein’s structure that functions to bind a molecule of oxygen The diameter of myoglobin is only about two nanometers; even though proteins are considered relatively large molecules, they are still very tiny indeed. Figure 3.4. Myoglobin molecule.\nIn describing proteins, several descriptive levels of structure have been defined What is called the primary structure of the protein is the linear sequence of amino acids forming the protein \u0026ndash; a list of the component amino acids in the order they occur in the polypeptide chain. The secondary structure describes the interactions of nearby amino acids to produce patterns of local folding within the protein Nearby amino acids may interact with one another via hydrogen bonding and other sorts of electrical attraction and repulsion This may give rise to distinct varieties of local folding within the protein The most famous example of protein secondary structure is the alpha helix, first described by the great chemist Linus Pauling (1901-1994). In Figure 3.4, alpha helices are represented by twisted sections of ribbon. The tertiary structure is the overall shape of the entire protein molecule, created by all the electrical and geometric properties of the constituent amino acids guiding the folding of the chain of amino acids into a unique three-dimensional form. Many functional proteins in living organisms are composed of a complex of more than one polypeptide subunit, with each subunit generally consisting of hundreds of amino acids Such an arrangement is termed the quaternary structure of the protein For example, hemoglobin \u0026ndash; which binds and transports oxygen in our blood \u0026ndash; is composed of four polypeptide subunits And ionotropic receptors for the neurotransmitter GABA (see Chapter 6) are composed of five subunit polypeptides. Carbohydrates # Built from atoms of carbon, hydrogen, and oxygen, which store substantial amounts of energy in their bonds. Small molecules include sugars such as glucose Large molecules include glycogen, cellulose, and starches. Starches consist of polymers of hundreds or thousands of glucose molecules linked by covalent bonds. Our third category of biological molecule is that of carbohydrates The name comes from a conjunction of carbon (carbo) and water (hydrate), and carbohydrates are built from atoms of carbon, hydrogen, and oxygen, covalently joined to form molecules Some carbohydrates are small molecules, such as the sugars glucose, fructose, and ribose. And some carbohydrates are enormous molecules, such as glycogen, starches, and cellulose Starches consist of polymers of hundreds or thousands of glucose molecules linked by covalent bonds into very long chains As with the fats, substantial amounts of energy are stored in chemical bonds joining the carbon, hydrogen, and oxygen atoms, and carbohydrates serve as sources of fuel for living organisms. Nucleic Acids # The fourth and final category of biological molecule to be introduced here is that of the nucleic acids, represented by DNA (deoxyribonucleic acid) and RNA (ribonucleic acid). DNA and RNA are by far the largest molecules in living organisms, containing many thousands, even millions, of atoms and serving as the repositories for the information required for constructing a living cell \u0026ndash; the genetic or hereditary information The DNA double helix is composed of two very long chains of four component nucleotides \u0026ndash; adenine (A), cytosine (C), guanine (G), and thymine (T) \u0026ndash; coupled with deoxyribose sugars and phosphate (phosphorus and oxygen) groups Each long chain is held together by covalent bonds between the sugars and phosphates The two chains wind around one another in a helical form and are held together by noncovalent hydrogen bonds between nucleotides Adenine forms hydrogen bonds with thymine, and guanine forms hydrogen bonds with cytosine. The discovery of the double-helical structure of DNA and its ramifications is one of the great sagas of twentieth-century science The field of molecular biology and the modern biotechnology industry were among the offspring of this discovery Also, among the ramifications, I believe, was the strengthening of confidence that the known chemistry of atoms and molecules will be able to account for all the phenomena of life, including even that of mind and consciousness. So, let the tale betold\u0026hellip;\nMolecules and cells, atoms play their varied roles.\nTheater of life!\n"},{"id":66,"href":"/mcb-c61/2/","title":"2: DNA, Synapses, and Neuroanatomy","section":"MCB C61","content":" \\(\\) 4: Genes and the History of Molecular Biology # Overview of History # Mid-1850s: Charles Darwin proposes that all life is deeply related and that there must be some underlying source of variation that is associated with how information is stored and passed on from one generation to the next. Gregor Mendel investigates inheritance in pea plants, which leads to the idea that information needed to build an organism is packaged into a gene \u0026ndash; some abstract kind of units associated with specific traits, segregated and sorted in reproduction 1920s: Based on groundwork laid by Max Planck in 1900 and Albert Einstein in 1905, a large group of physicists begin to develop Quantum Mechanics. 1932: Niels Bohr speaks to a group of clinicians in Copenhagen, Denmark, about the limits of investigating structure and function of living organisms at the subcellular level; he speculates that such study could kill the cell. 1935: Max Delbrück publishes “On the nature of gene mutation and gene structure,” in which he proposes that genes are likely to be large molecules. In 1937 he moves to the United States and begins working with bacteria and viruses (called “bacteriophages”) as a way to investigate the physical properties of heredity. Viruses are simple\u0026ndash;consisting of protein, nucleic acid, and lipid\u0026ndash;yet carry genes to replicate themselves 1944: Erwin Schrödinger publishes What Is Life?, dealing with deep questions concerning the molecular basis of life and drawing attention to Delbrück’s idea that genes are likely to be large molecules. His book inspires a signiﬁcant number of physicists to research the physical basis of life, a ﬁeld of study that eventually becomes called molecular biology. Oswald Avery and colleagues publish a report that describes experiments with pneumococcal bacteria demonstrating that DNA can carry genetic information from one cell to another. The report concludes “a nucleic acid of the deoxyribose type is the fundamental unit of the transforming principle” (not protein). His work was essentially ignored by the scientiﬁc community, which considered DNA too “stupid” a molecule to carry genetic information. 1952: Alfred Hershey and Martha Chase infect bacteria with T2 phages containing radioactive sulfur atoms (incorporated into protein) and radioactive phosphorous atoms (incorporated into DNA) and showed that it is the radioactive DNA that is transferred to the bacteria during viral infection (more below) 1953: Francis Crick and James Watson propose their famous double-helical structure for the DNA molecule. Realized that genes where sequences of (A,T,C,G)s in DNA and that there was some genetic code between this DNA sequence and the ammino acids in proteins The Hershey-Chase Blender Experiment # Sought to answer the question: Are Genes Made of Protein or DNA? Steps involved: Bacteriophage virus T2 is grown on two different media: one containing radioactive sulfur atoms and one containing radioactive phosphorus atoms. Most proteins contain sulfur atoms, while DNA does not; DNA contains phosphorus atoms, while proteins do not. The viruses are allowed to infect bacteria in a solution with it\u0026rsquo;s genes, but not given enough time to replicate. The cells are agitated (bended) enough to dislodge the virus particles from the surface of the bacteria. Spinning in a centrifuge causes the larger, heavier bacteria to settle into a pellet at the bottom of the tube, while the smaller, lighter virus particles remain in the supernatant liquid. Radioactivity remains in the supernatant, and thus protein stays with the original phages after the infection. Radioactivity is in the pellet, and thus DNA is transferred to the bacteria during the infection; thus, Genes are made of DNA. 5: How Neurons Generate Signals # Diffusion Things tend to spread out from high to low areas of concentration With ions, they diffuse as a function attempting to equalize the concentration difference along with the electrostatic attractions/repulsions Ion channels proteins: Are often gated, sometimes allowing certain substances through Passive; substances move across membrane down concentration gradient so they do not require energy (unlike pumps) This leads to concentration differences of the major ions for neural function Na+, Cl-, Ca++ tend to be more concentrated on the outside of the neuron K+ tends to be more concentrated on the inside This produces a resting potential of -65 millivolts (negative inside) \u0026ndash; when no signals are being sent Hyperpolarization: The membrane potential, wrt the resting, becomes more negative when a K+ or Cl- channel open Depolarization: The membrane potential becomes more positive when a Na+ or Ca+ enter Axon hillock: where all dendrites meet, where depolarization first occurs Spatial and temporal summation of neuronal input: superposition of signals from dendrites occur Action potential: The amount of energy needed for a neuron to fire Research done by Alan Hodgkin, Andrew Huxley Voltage-gated ion channels open and close to create this phenomena Action potential propagation along axon with Na+ entering and K+ exiting Sodium-potassium pumps Na+ out of neuron and K+ into neuron to \u0026lsquo;recharge\u0026rsquo; the neuron This requires energy in the form of ATP 1 ATP =\u0026gt; 3 Na+ out / 2 K+ in 25% of our energy is consumption by human brain Refractory period is the 2-3 ms cooldown where it can\u0026rsquo;t fire again Myelin coats axons, forming nodes of Ranvier Speeds up propagation via Saltatory conduction Allows the AP to \u0026lsquo;jump\u0026rsquo;, increasing up signal speed from 10 m/s to 100 m/s Only in vertebrae animals Made up of 40% phospholipid, 30% cholesterol, 30% protein Forming myelin Oligodendrocytes operates in the central nervous system, working on various axons simultaneously Schwann cells operate in peripheral nervous system 6: Synapses, Neurotransmitters, and Receptors # Electrical synapse Gap junction: where ions flow from one to another Connexons: channel Connexins: subunit Chemical synapse: Default synapse people reference Dendritic spine increase the surface area, act as input channel AP propagates down through VG Na+ and K+ channels which trigger the VG Ca++ channels Synaptic vesicle then fuse to membrane-to-membrane to terminals which\u0026hellip; Involve neurotransmitters (e.x. serotonin, dopamine) which release into the synaptic cleft Synaptic cleft is ~20nm between in and output Pre-synaptic neuron: axon terminal Post-synaptic neuron: dendritic spine, dendrite, cell body Reuptake transporter reabsorb the neurotransmitter and repackage/resynthesize them Otto Loewi discovered this concept of chemical neurotransmitters Done by triggering the Vagus nerve to slow down a heart in water, then setting another new heart in the same water and finding it slowed down as well Called this Vagusstoff, now known as Acetylcholine Two most common neurotransmitters (NTs) Glutamate: primary excitatory (more positive, gain of ions) NT in human brain GABA: primary inhibitory NT in human brain Glutamic acid decarboxylase is an enzyme than can convert glutamate to GABA Two common receptors Ionotropic receptor / Ligand-gated channel One type of NT receptor protein NT opens the channel, allowing ions through (Ionotropic) Ligand = small molecule that binds to a larger Ionotropic glutamate receptors Channels for Na+ and Ca++ Produces EPSP: excitatory post-synaptic potential Ionotropic GABA receptors Channels for Cl- Produce IPSP: inhibitory post-synaptic potential Metabotropic receptor, GPCR: G-Protein coupled receptor NT trigger the GPCR which activates G-protein which interact with\u0026hellip; Effector Enzyme makes cAMP which can interact with.. Protein Kinase which activate substrate proteins causing cellular effects This is very complex intracellular messaging 7: Neuroanatomy and Excitability # CNS: Brain, Spinal Cord PNS: Everything else; e.x. sensory systems, muscles/movement, autonomic Cranial nerves: 12 nerves that connect the (periphery) senses to the brain Four biggies: olfactory, optic, auditory/vestibular, and vagus nerve Autonomic nervous system: Regulate automatic processes (i.e breathing, heart beat) Sympathetic: Stimulatory, \u0026lsquo;fight or flight\u0026rsquo;, effects: Increase heart rate, blood pressure Dilates lung airways, pupil Constricts bladder Decreases intestinal mortality NT: norepinephrine Parasympathetic: Inhibitory effects (opposite of those above) NT: acetylcholine Drug molecules: molecules that have an impact on the physiology of the body Agonist: Chemical that triggers NT receptor Sympathomimetic Parasympathomimetic Antagonist: Chemical that blocks NT receptor Sympatholytic Parasympatholytic Monoamine Neurotransmitters: nerve fibers emerge from small clusters of cells in the brainstem yet innervate the whole brain Acetylcholine: Used in the neuromuscular junction (NMJ) Found in relatively small proportionally in brain in the Basal Forebrain Nuclei \u0026amp; Midbrain Pontine Nuclei Serotonin: ~100-200k found in the raphe nuclei Made from tryptophan (amino acid) Dopamine: found in the ventral tegmentum \u0026amp; substantia nigra Norepinephrine: found in the locus coeruleus Peptide neurotransmitters: Made up of chains of ammino acids joined by peptide bonds E.x. endorphins (opioids peptides), substance P Seizure: run-away neural activity in the brain Characterized by aura, muscle convulsions/spasms, amnesia, loss of consciousness Causes of seizures: Idiopathic \u0026ndash; underlying cause is not known Genetic and developmental components Associations / factors \u0026ndash; all vary widely person-to-person Physical trauma Infection, fever, tumor Emotional stress Sleep deprivation Drug use/withdrawal Intense sensory stimuli Medication: all involve reducing neural excitability Inhibit voltage-gated sodium/calcium/potassium channels Activate GABA receptors Block glutamate receptors Prevalence: 1% ~ 3 million people have it; 30% continue even with medication Surgical treatment: Severing of corpus collosum (rare) Excision of epileptogenic brain tissue Brain electrical stimulation "},{"id":67,"href":"/mcb-c61/3/","title":"3: Pharmacology; Neurodevelopment","section":"MCB C61","content":" \\(\\) 8: Poison, Medicine, and Pharmacology # Pharmakon: medicine and poison at the same time Everything is a poison. The difference between a poison and a remedy depends on the dose. \u0026ndash; Paracelsus\nBotulinum Toxin Strongest toxin known Disrupts acetylcholine transmitters of neuromuscular junctions Human LD$_{50}$ = lethal dose for 50%: 2 ng / kg E.x. lethal dose for 50kg is 100ng Therapeutic (Botox) dosage: ~5ng Therapeutic Index (TI): lethal dose / therapeutic dose E.x. 100/5 = 20 (unitlest) Brain The brain needs lots of energy; makes up ~25% of the body\u0026rsquo;s energy intake at rest The brain receives robust blood flow via the circulatory system Blood-brain barrier (BBB): Everywhere, except the brain and spinal cord, have pores in the blood vessel walls (allowing medium-large molecules system) Walls are composed of phospholipid bilayer biological membrane \u0026ndash; thus most of the thickness is hydrophobic Ways to pass Transport proteins: glucose sugar, some amino acids (too big for channels) Diffusion: Sufficiently lipophilic / hydrophobic molecules \u0026ndash; how non-endogenous molecules pass Natural poisons # Tetrodotoxin (TTX) # Blocks voltage-gated sodium channels Peripheral nerves unable to generate normal action potential Leads to numbness, muscle weakness, paralysis Can lead to death through respiratory paralysis Does not get into the brain No CNS effects! In skin and organs of puffer/blow fish, octopus, newts, fugu All unrelated animals; there are symbiotic bacteria that cause this resistance TTX resistance: how do these animals (and their predators) not die? Genetic variance can lead to an amino acid change in the sodium channel Only 1/1800 amino acids are sufficient to be resistant Saxitoxin # Also called Paralytic Shellfish Poisoning (PSP) Found in dinoflagellates and other protists Causes numbness, muscular weakness, paralysis Resistance results from mutations in the Sodium channel Mutation changing a single amino acid from glutamic acid to aspartic acid (one carbon difference!) causes 1000x decrease in STX sensitivity Batrachotoxins # Discovered in skins of tiny amazonian frogs Used for blow darts to hunt birds Also found in birds which eat a certain beadle Thought that inside the beadle live symbiotic organisms that produce the molecule Paralytic effect Rather than blocking VG sodium channels, these trigger the receptors (open) which causes a similar effect Similar to local anesthetics (i.e cocaine, benzocaine, lidocaine, procaine) 2 Acetylcholine Receptors # Ionotropic (nicotinic ACh receptor) Occurs in brain, neuromuscular junctions Agonists: nicotine Antagonist: tubocurarine GPCR (muscarinic ACh receptor) Occurs in brain, parasympathetic NS Agonists: Muscarine Antagonist: Atropine (parasympatholytic) 9: Psychoactive Drugs # Psychoactive drugs: top 5 # Caffeine: tea leaves, coffee, chocolate Molecule blocks adenosine receptors (antagonist) Ethyl alcohol: beer, wine From fermentation of sugar by yeast Enhances GABA action at receptors Antagonist at some glutamate receptors Blocks presynaptic uptake of adenosine Most popular sedative-hypnotic drugs; those that are calming at lower doses and can lead to death TI is low; ~10 for ethanol Also: Barbiturates (TI \u0026lt;10; used in lethal injection), benzodiazepines (Xanax, Valium), general anesthetics (gases; global loss of sensation for surgeries) All enhance inhibitory action of Cl- GABA receptor in CNS Nicotine: tobacco plant Agonist at nicotinic AChRs Found in basal forebrain nuclei CNS effects: stimulation and relaxation PNS effects: partial sympathominetic via facilitation of norepinephrine release Arecoline: betel (areca) nut Similar to nuts/dates Popular in Asia Commonly mixed with betel lea, slaked lime (Calcium hydroxide), spices, and, in India, tobacco Slaked lime enhances psychoactive effects Major psychoactive is Arecoline; muscarinic AChR (GPCR) agonist PNS: Activates parasympathetic NS (parasympathomimetic) CNS: stimulant/relaxant effects, like nicotine Cannabinoids: cannabis Effects Analgesic Anti-inflammatory Muscle relaxant Appetite stimulant Antiemetic Reduces intraocular pressure Anti-seizure Sedative Hypnotic Unique botanical chemistry involving cannabinoids (THC, CBD) Endogenous ligand; Endocannabinoids: Mediate retrograde signal (backwards!) 1. Anandamide 2. 2-AG Can cause psychosis Erythroxylum coca (cocaine) From leaf to prevent insects from munching (similar to nicotine and caffeine) Acts at norepinephrine (Locus coeruleus) / dopamine (ventral tegmentum, substantia nigra) receptors to prevent reuptake PNS: Norepinephrine; sympathomimetic CNS: stimulant; wakefulness, stamina, appetite suppressant, euphoria Toxicities: sympathetic + CNS overstimulation, addiction Local anesthetic Only occurs at high concentration of substance Disrupts Na+ reuptake Amphetamine (adderall) and other stimulants (ritalin, provigil, etc) Act on norepinephrine / dopamine, but make receptors \u0026rsquo;leakey' Similar effects to cocaine at high doses Medical use: treatment of ADHD Opium, opium poppy (Papaver somniferum), morphine, opioids Acute effects Analgesia: reduces pain Anxiolytic: disrupts anxiety Sedation: calming Cough suppression Decreased intestinal motility Pupil constriction Friedrich Wilhelm Sertürner was interested in learning what was inside pharmacy products Extracted opium, founding morphine (~1803) Later the other primary opiod, codeine, was extracted as well Semi-synthetic opioids: synthetic modification of opiates from opium Heroin from morphine; 2-3x more potent Synthetic opioids: not directly related to morphine in chemical structure Fentanyl; 100x more potent Wildnil; 10,000x \u0026ndash; used to kill large animals Toxicity Depression of respiratory control centers in brainstem Potential OD potential (low TI) High addictive potential Opioid receptors Agonists at opioid receptors (GPCRs: mu, delta, kappa) Naloxone (Narcan) is an antagonist which blocks and reverses the effects of opioid agonists; used for ODs Endorphins are the neurotransmitter involved Psychedelics (\u0026lsquo;mind manifesting\u0026rsquo;), hallucinogens Leads to heightened awareness of internal/external perception Acts on 5-HT$_{2A}$ serotonin receptor agonists; found all over brain Impacts on brain functionality, synaptic neuroplasticity Can be used to treat anxiety, depression, addiction, PTSD LSD Discovered by Albert Hofmann Most popular psychedelic Psilocybe mushrooms Contains Psilocybin Maria Sabina was the first shaman to spread the word to Gordon Wasson DMT Naturally found; can be converted from tryptophan (amino acid) Used in ayahuasca; used in tribal rituals Peyote cactus Contains Mescaline molecule Discovered by Arthur Heffter 10: Neural Development and Neuroplasticity # Neuroplasticity: Structure and function of synapses always is being modified Occurs at all times, prevalent in prenatal development Increases by\u0026hellip; Increase amount of NT released Open gates for longer Increase number of pre/post synaptic vesicles Decrease reuptake transporters Embryonic Development Process Starts with an egg full of embryonic stem cells Grows into embryonic disc by 2 weeks Neural tube (start of NS) starts to develop by week 3 Controlled by genes Human genome 23 chromosomes (from parents haploid cells) 3 billion base pairs (ATCGs) 21,000 distinct genes coding for proteins \u0026lt; 2% of human genome code translated for function protein 80% of the remaining 98% is transcribed into RNA Cell differentiation is controlled by gene transcription: turning on/off of genes at different times Done via transcription factors and regulatory RNA E.x brain cells start as stem cells, grow into some nervous system neural progenitor cell, then can change into various types of neurons/gilial cells through axon and dendrite branching + forming connections (synaptogenesis) Synaptogenesis involves the axon growing tip (growth cone) extending it\u0026rsquo;s \u0026lsquo;feelers\u0026rsquo; (filopodia) outwards Through staining we can see cell\u0026rsquo;s cytoskeleton (made of microtubules and microfilaments) which are very active in this process Roger Sperry speculate there was some feedback so they knew where to go through chemical signals (Chemoaffinity hypothesis) Now recognized as neurotrophins: proteins produced by the body important for cell growth and survival Guidance Factors Initiated by neurotrophin \u0026lsquo;Steered\u0026rsquo; by contact and soluble factors Ephrin receptor interaction can cause either an attraction or repulsive Microtubule is driven outwards while microfilaments communicate "},{"id":68,"href":"/mcb-c61/4/","title":"4: Sensory Perception","section":"MCB C61","content":" \\(\\) 11: Sensory Perception # Sensation # Detection and capacity to respond to some signal; Property of all living organism\nMicroorganisms can detect and respond to physical stimuli Chemotaxis (reaction to chemicals, i.e sugar) has been observed in ecoli through movement by \u0026lsquo;swimming\u0026rsquo; with runs and tumbles Phototaxis: Moving toward light (e.x. bacteria) Phototropism: Bending towards light (e.x. plant) Perception # The experience of said a signal; constructive process occurring with our nervous system\nDepends on\u0026hellip; ..what is \u0026lsquo;out there\u0026rsquo; (objective; profound assumption) ..processing that occurs in our brain/mind ..physics of our sensory receptors (can also be out of our range of sensation) Visible light is just the range (~400 - 700 nm) of light we can sense on the electromagnetic spectrum These colors or not objective and is a product of our human body No reason to assume aliens would perceive them similarly Ultraviolet is below (shorter) our range, infrared is above (longer) Increasing wavelength, decreasing energy These are clearly there, but we are unable to experience them (perceive them) Naive realism: Common sense theory of perception; what we experience is what there is - Illusions can easily disprove this Animals # Karl von Frisch asked how to honeybees see flowers colors? Colors one sugar-sample blue; bees routinely came back to it Only possible if they could see color; impossible if they say black-and-white Discovered that bees communicated information to hive mates Received Nobel Prize in 1973 for said research Only prize awarded thus far for behavioral work Some flowers also have a UV \u0026rsquo;landing-guide\u0026rsquo; Pit vipers can view infrared \u0026lsquo;Images\u0026rsquo; similar to our eyes in their nostril Enables them to hunt in complete darkness Animals emit heat which can be seen by them Light # Can be thought of as vibrating wave; posses wavelength, velocity, energy (amplitude), and polarization (the plane in which wave is propagating)\nLight from sun is unpolarized and varies with position in sky Bees, ants, beetles, and other insects are able to perceive this Enables them to navigate their environment form this Polarized filters only allow certain planes through Water also acts as a filter Audition # We can hear~ 20-20,000 hertz (vibrations per second) Elephants, pigeons, dolphins, whales are able able to hear low frequency noises \u0026lt; 10 Hz Bats, dolphins, whales, moths can hear \u0026gt;50,000 Hz Use as a bio-sonar to practice echolocation Sonar = sound navigation and ranging Electroreception # Ability to sense electric fields Occurs passively (only detecting, not producing) in sharks Shown in study in 60s by showing sharks find/eat buried fish that they can otherwise no sense (smell) Fish have some field due to their nervous system, creating a field Field is especially strong in water Can sense this to feel it\u0026rsquo;s surroundings \u0026ndash; active electroreception Platypus also has this capacity Bill has electroreceptors Used to detect bioelectric fields of invertebrate prey Also passive Magnetic field detection Earth produces a magnetic field everywhere Birds, Fish, sea turtles, bees, and many other animals posses this, use to navigate Pigeons are taken from home then released Overcast days inhibit a birds ability to orient themselves (versus a nice, sunny day) We don\u0026rsquo;t really know how! 12: Nose and Smell # Olfaction # Our ability to sense volatile molecules, resulting in smell\nOlfactory receptor cells are found in the nasal cavity, extending nerve fibers containing olfactory receptor proteins Proteins are GPCRs Fish have ~ 100 gene types Mammals have ~ 1000 gene types Mice have ~ 1300 gene types Humans have ~ 350 gene types (with ~ 650 nonfunctional pseudogenes) We can still discriminate ~100,000 odors Cilia are the \u0026lsquo;fingers\u0026rsquo; that extend out, increasing the surface area They arise from a mucus layer which helps molecules stick Signals are then generated and sent through the olfactory bulb in the brain One path goes to thalamus, orbitofrontal cortex Standard route Involved in identification Second path goes to amygdala, temporal cortex, hypothalamus (limbic system) Associated with emotional response Causes innate reaction Olfactory stem cells replace dying cells often - We smell a lot of chemicals that may harm/cause error in our existing receptor cells Aromatic Odor # The flavor component of plants; the \u0026rsquo;essence\u0026rsquo; of the plant (from \u0026rsquo;esse\u0026rsquo;, to be)\nEssential oil (i.e perfumes, spices) are concentrates (often through distillation) are made up of numerous aromatic molecules Small changes in the composition of these molecules results in radically different sensations Stick slightly differently to GPCRs receptors Thiols indicate sulfur Stinky! Found in skunks and \u0026lsquo;asparagus smell\u0026rsquo; in urine Not everyone can smell this though, there are genetic differences Anosmia \u0026ndash; Specific: deficit in specific gene change in olfactory GPCR, the case here Anosmia \u0026ndash; General: deficit occurs at a higher level of perception Pheromones # Chemicals used for intraspecies social communication\nFound in insects and some vertibraes Rodents have an vomeronasal organ (VMO) and pathway in vertebrate animals for pheromone detection Used for territorial marking, sex, social status, identity, mate attraction Some of these chemicals are found in nature as well It appears us humans have a vestigial VMO that\u0026rsquo;s not active Menstrual synchrony in women is thought to be mediated by subtle aromatics Sexual attraction is speculated to be partially mediated by human leukocyte antigen genes Tears carry several molecules as well that may contribute as well 13: Tongue and Taste # Taste buds are made up of.. Microvilli, adjacent to cilia, that increase surface area Stem cells gustatory cells to replenish cells Taste receptor cells fall into these types with the associated proteins: Salt: ion channel sensitive to Na+ Sour: acidic, ion channel sensitive to H+ Bitter: Group of ~30 different GPCR receptors Variety of molecules are bitter, typically poisonous Sweet: GPCR heterodimer receptor; sucrose/fructose/glucose/stevioside (~300x sweeter than sucrose!) Umami: \u0026lsquo;Savory\u0026rsquo;; glutamate Gustatory neural pathways are analogous to their olfaction counterparts.. (Identification) Tongue to brainstem to thalamus to insula + somatosensory cortex (Reaction/emotion) Tongue to brainstem to hypothalamus + amygdala Capsicum annum: Chili pepper Capsaicin: Molecular component associated with \u0026lsquo;hotness\u0026rsquo; Same receptor protein (TRPV1 receptor; Ca++ channel) is also activated with heat All over our skin as well as tongue TRPA1 activated by mustard, horseradish, wasabi How pit vipers see heat (infared)! Wow TRPV1 activated by (crushed) garlic Menthol: \u0026lsquo;Cool\u0026rsquo; mint sensation; Opposite than capsicum Activates Ca++ channels to cause cell depolarization Menthol receptor (TRPM8) also activated by cold temperatures These TRP ion channels are tetramer (4 proteins) subunits surrounding a Ca++ channel Flavor is the amalgamation of taste, smell, pungency, texture, and other qualities (metallic, fatty) 14: Eyes and Vision # The human eye, like all vertebrate eyes, contain the\u0026hellip; Retina: Photosensitive material Photoreceptor cells: Rods: Ideal for dim light, nighttime Most responsive at 498 nm wavelength Most concentrated around fovea, with none at the fovea itself; \u0026lsquo;donut\u0026rsquo; Cones: Ideal for colorful, bright light Multiple types; S at 420nm, M at 530nm, L at 560nm The amalgamation of these 3 enables color Most concentrated at the fovea, and drop off rapidly Retinal achromatopsia: loss of color perception, due to loss of cones Retinal (vitamin A) absorbs light of the right wavelength, changes via Photoisomerization from cis (bent) to trans (straight), altering the amino acid, activating a GPCR receptor to hyperpolarize Retinal needs beta-carotene (from carrots, etc.) Retina: bipolar cells, ganglion cells Retina: horizontal cells, amacrine cells Fovea: Where the light is directly focused, in the center Latin \u0026ldquo;pit\u0026rdquo; Where our vision is best Blind spot: Where all the axons come bundled together No rods nor cones Receptive field: the region of space from which a stimulus elicits a neural response From the retina, the signal goes to the diencephalon containing the thalamus where the Lateral Geniculate Nucleus (LGN) parses the information, then onto the visual cortex (occipital, posterior parietal, posterior temporal lobes) Visual cortex (V1-5) are areas maps to the visual world Basically, nearly every cortex is connected bidirectionally to every other cortex Disordrs Scotoma: Dark region, blind spot in certain region; due to lesion in V1 - Hemianopia: blindness of one hemisphere Cortical achromatopsia: Loss of color, sometimes confined to only a certain region; Lesion in V4 Superior colliculus: Loss of ability to see color; Lesion in V5 Prosopagnosia: difficulty in recognizing faces; due to fusiform gyrus; spectrum condition Agnosia: difficulty in recognizing objects "},{"id":69,"href":"/mcb-c61/5/","title":"5: Connectivity","section":"MCB C61","content":" \\(\\) 16: Skin, Touch, and Movement # Somatosensory receptors occur in the skin Dorsal-root ganglion (DRG) is a somatosensory neuron that carries \u0026rsquo;touch\u0026rsquo; information Somatosensory cortex is where information is processed Located in postcentral gyrus Somatosensory body map pairs region in cortex and location on body Discovered by Wilder Penfield Size in cortex isn\u0026rsquo;t 1:1 with size (index finger has an extra-large area) \u0026ldquo;Distortion\u0026rdquo; reflects sensory sensitivity E.x. difficult to sense two points of contact (on back); Two-point discrimination test Finger amputation in monkey increased cortical response for adjacent fingers (somatosensory map reorganization) Phantom limb\u0026rsquo;s may occur when one loose\u0026rsquo;s their arm and \u0026lsquo;feels\u0026rsquo; said arm when receives information from cortical-adjacent areas Primary somatosensory cortex (S1) lesions lead to loss of sensation in specific region Posterior somatosensory cortex (S2, S3, etc) lesions lead to neglect syndrome: one is aware, but unable to focus on a body region (ex. one doesn\u0026rsquo;t dress their left-side of their body) Primary motor cortex (M1) Located anterior to the postcentral gyrus Contains a motor-body map Contralateral; left-right, right-left Supplementary motor / premotor areas: Involves planning of movement; lesions cause apraxias, impairment in organizing movements (to solve problems) Mirror neurons activate during movement and during observation of movement! Vast interconnectivity between posterior sensory areas and prefrontal motor areas Cerebellum Involved in timing and coordination of movement More nerve cells here than the rest of the brain Frontal-parietal lesion Causes paralysis of right half of body Lead to loss of sensation, \u0026lsquo;weirdness\u0026rsquo; on right side If on the right side, they become denial! Anosognosia, loss of knowledge of one\u0026rsquo;s disease Exhibits a kind of exaggeration of psychological defenses Hypothesis: Left hemisphere controls defense mechanisms and is kept in check by the right hemisphere 17: Imaging the Brain # Static/structural brain imaging: # Popularized by Andreas Vesalius (1514-1564) in De Humani Corporis Fabrica (1543)\nX-ray imaging Discovered by William Rontgen in 1895 First Nobel Prize in Physics (1901) Xrays are higher energy than UV and natural light, passing through skin/tissue more-so than bone Lesions show as change in contrast Computed axial tomography (CAT, CT) Began in 1970s 16A Magnetic resonance imaging (MRI) Looks similar to CAT scan Operates by Nuclear Magnetic Resonance (NMR) to measure/alter the nuclear spin of atoms in molecules Magnetic field strength: Tesla = 10,000 Gauss Earth\u0026rsquo;s magnetic field = 0.5 gauss No known toxic effects on the body Dynamic/functional brain imaging # Electroencephalography (EEG) Developed in 1920s by Hans Berger Give\u0026rsquo;s you a \u0026lsquo;summed\u0026rsquo; signal of the brain Seizure causes erratic behavior Wilder Penfield, epileptogenic tissue Electrocorticography (ECoG) Temporary implanted electrodes on brain itself Allows higher frequency range + higher resolution Magnetoencephalography (MEG) Magnetic fields are far weaker, so a stronger detector is needed Very expensive and can only operate in special, shielded room fMRI Hemoglobin: Protein in our red blood; has spin-flip energy depending on oxygen contents BOLD signal: Blood Oxygen Level Dependent Signal Represents changes in (de)oxygenated hemoglobin from blood flow and cell metabolism Correlated with neural activity Positron emission tomography (PET) Radioactive isotope injected into blood; Florine-18 (HL: 2hr), Oxygen-15 (HL: 2min), Carbon-11 (HL: 20min) These unstable elements emit positrons (anti-electron): the antimatter particle corresponding to the electron Matter-antimatter collision results in annihilation: complete conversion of mass to energy 2 gamma-ray photons released when positron-electron collision occur Enables 3d reconstruction of where most energy is occurring (glucose intake, directly) Only difference/change in brain activity is shown “dark energy” of the brain is the abnormally large background noise Ernest Lawrence invented, at Berkeley, the Cyclotron in the 1930s Accelerates atoms to very high speeds Won Nobel Prize in 1939 in Physics Transuranium elements 104-118 found by Russians Live for a fraction of a second Able to create unstable elements, required in PET scans 18: Connectivity, Language, and Meaning # Aphasia: Broca’s; Left frontal lobe Production aphasia for spoken and written language Mouth/tongue etc are all uneffected Wernicke’s: Comprehension aphasia for spoken and written language Prevent analysis at a higher level Part of auditory cortex Cortical layers and connectivity Hemispheric asymmetry Wada test: Bartbiturate sedative-hypnotic injected into right/left hemisphere via right/left catheter threaded from femoral artery in thigh Asked to count after injection Right causes slight pause when it hits Left causes speaking to stop when it hits Language lateralization and handedness Right handers: LH 97%; RH 3% Non-right handers: LH 70%; RH 15%; Both 15% Linguistic mirror neurons Sound activate the auditory cortex, A1 Sounds that are clearly language activate A1, Wernicke\u0026rsquo;s Language with meaning activates A1, Wernicke\u0026rsquo;s, Broca\u0026rsquo;s Language occurs all across the brain Origin of intentionality isn\u0026rsquo;t real clear Prefrontal motor areas generate movement "},{"id":70,"href":"/ap/huge/","title":"AP Human Geography","section":"AP Notes","content":" The content of these notes are solid, but formatting is not since they\u0026rsquo;re exported from Notion. 🗺️ Unit 1 — Thinking Geographically # Developing Understanding\nThis first unit sets the foundation for the course by teaching students how geographers approach the study of places. Students are encouraged to reflect on the “why of where” to better understand geographic perspectives. Many other high school courses ask students to read and analyze data, but for this course, students also apply a spatial perspective when reading and analyzing qualitative and quantitative data. Students learn the ways information from data sources such as maps, tables, charts, satellite images, and infographics informs policy decisions such as voting redistricting or expanding transportation networks. They also learn about how people influence and are influenced by their environment; the resulting impact on topography, natural resources, and climate; and the differences between and consequences of environmental determinism and possibilism. Finally, students are introduced to the language of geography, learning discipline-specific terminology and applying that language to contemporary, real-world scenarios so they can better study population processes and patterns in the next unit.\nBIG IDEA 1 Patterns and Spatial Organization (PSO)\nWhy do geographers study relationships and patterns among and between places? BIG IDEA 2 Impacts and Interactions (IMP) How do geographers use maps to help them discover patterns and relationships in the world? BIG IDEA 3 Spatial Processes and Societal Change (SPS) How do geographers use a spatial perspective to analyze complex issues and relationships? 1.1 Introduction to Maps # IMP 1A Geographers use maps and data to depict relationships of time, space, and scale.\nLearning Objective: Identify types of maps, the types of information presented in maps, and different kinds of spatial patterns and relationships portrayed in maps. Essential Knowledge: Types of maps include reference maps and thematic maps reference maps Serve to display general features of an area Topographic highway, atlas, etc thematic maps Server to display single type of information Types: graduated circle size of circle conveys quantitative statistic Example dot display pattern, distribution, dispersion of data in an area Example choropleth display an average value of data in an area Example Types of spatial patterns represented on maps include absolute and relative distance and direction, clustering, dispersal, and elevation. maps are a special form of model that depicts information in two dimensions and usually on paper. A model is a simplified generalization of something in real life relative distance (scale) Scale: The ratio between the size of an area on a map and he actual size of that same area on the earth\u0026rsquo;s surface Scale gives a frame of reference Representation Factor (RF): 1 inch : 250,000 feet Verbal: One inch represents 250,000 feet Visual |——————|——————| 0 250,000\u0026rsquo; 500,000' direction absolute direction: north south, compass relative direction: left right, forward backwards, up down based on perspective in certain location clustering + dispersal where groups are (not) centered around density vs concentration: Density is the amount of an object within a certain area concentration is how these objects are distributed. Box A the distribution is dispersed. There is a clear cluster of stars in the upper left corner for Box C. Box B has concentrations in the left and right sides Box D has a cluster (though not as clustered as C) in the middle left and one outlier. elevation Hight from set point, typically water level All maps are selective in information; map projections inevitably distort spatial relationships in shape, area, distance, and direction Maps are 1D representations (projection) of a 2D environment, so they\u0026rsquo;ll inherently have distortion Mercator shape: fairly accurate area: distorts area near north and south pole direction: turns curves into straight lines Goode\u0026rsquo;s shape: accurate area: accurate direction: accurate hard to understand Goode\u0026rsquo;s shape: everything slightly distorted outwards from 0\u0026rsquo; 0' area: accurate direction: accurate 1.2 Geographic Data # IMP 1B Geographers use maps and data to depict relationships of time, space, and scale.\nLearning Objective: Identify different methods of geographic data collection. Essential Knowledge: Data may be gathered in the field by organizations or by individuals. Geospatial technologies include geographic information systems (GIS), satellite navigation systems, remote sensing, and online mapping and visualization. Remote sensing: Record area from a distance People have used cameras with airplanes, kites, hot-air balloons, etc. for many years Nowadays use satellites Useful for any area that would be otherwise be difficult to travel record geographic information systems (GIS) merge mapping software with a database to overlay various data layers on a basic map grid Spatial information can come from written accounts in the form of field observations, media reports, travel narratives, policy documents, personal interviews, landscape analysis, and photographic interpretation. 1.3 The Power of Geographic Data # IMP 1C Geographers use maps and data to depict relationships of time, space, and scale.\nLearning Objective: Explain the geographical effects of decisions made using geographical information. Essential Knowledge: Geospatial and geographical data, including data and satellite imagery, are used at all scales for personal, business and organizational, and governmental decision making purposes. Used to measure climate change pollution spreads of fires military survalliance google maps / personal GPS delivering your packages 1.4 Spatial Concepts # PSO-1 Geographers analyze relationships among and between places to reveal important spatial patterns.\nLearning Objective: Define major geographic concepts that illustrate spatial relationships. Essential Knowledge: Spatial concepts include absolute and relative location, space, place, flows, distance decay, time-space compression, and pattern. location + place (same thing) actual position on earth space area that is occupied by something can refer to physical and cultural objects on the surface of earth relative space is concerned with where something is in relation to something else and changes constantly as interrelationships between people, places, and things change absolute space is a measurable area with definite boundaries site physical location of a place situation location of a place based on its relation to other places flows + patterns trends of relationships(?) distance decay The declining degree of acceptance of an idea or innovation with increasing time and distance from its point of origin or source. Example: The number of phone calls made decreases with distance. Greater number of migrants settled at the edge of the country closer to the country of origin, compared to the number settled on the opposite edge of the country. The diminishing evidence of cultural traits by a group of people, if the explanation clearly shows a link to the fact that due to migration there is less contact between the migrants and their home country. Explanatory factor behind distance decay relationship (e.g., travel cost, information availability). time-space compression (decrease in friction of distance) the increasing sense of connectivity that seems to be bringing people closer together even thought their distances are the same. Space time compression is the solution to distance decay because technology (internet,cell phones) is allowing us to communicate more across longer distances. 1.5 Human–Environmental Interaction # PSO-1 Geographers analyze relationships among and between places to reveal important spatial patterns.\nLearning Objective: Explain how major geographic concepts illustrate spatial relationships Essential Knowledge: Concepts of nature and society include sustainability, natural resources, and land use. sustainability the use of the earths renewable and nonrenewable natural resources in ways that ensure resource availability in the future 3 Pillars Environmental Having Conservation, Nonrenewable and renewable resources and Preservation Social Humans need shelter food and clothing to survive so make resources meet those needs Economic Having natural resources; supply and demand Humans take resources that aren\u0026rsquo;t always needed or they take advantage of the resources that we have. But there is a lot of supply and demand and humans don\u0026rsquo;t know how to balance the resources. natural resources Resources that come directly from Earth Need to be used in moderation if they aren\u0026rsquo;t renewable land use to be sustainable, land has to be used efficiently to get enough value but not worked too hard to the point where it looses nutrients Theories regarding the interaction of the natural environment with human societies have evolved from environmental determinism to possibilism. Environmental determinism : The idea that physical environment caused social development Possibilism: (Replaces Environmental Determinism) The idea that environment may limit some human actions, but people have the ability to adapt to their environment 1.6 Scales of Analysis # PSO-1 Geographers analyze relationships among and between places to reveal important spatial patterns.\nLearning Objective: Define scales of analysis used by geographers. Essential Knowledge: Scales of analysis is how information is clustered global — worldwide regional — groups of states, e.g. North America national — single state, e.g. Wisconsin local — city or town, e.g. Middleton Scale is what infomation is shown Learning Objective: Explain what scales of analysis reveal. Essential Knowledge: Patterns and processes at different scales reveal variations in, and different interpretations of, data. To understand individual, local, regional, national, and global interrelationships, geographers compare and contrasts different scale views. Downsides to large scale 1.7 Regional Analysis # SPS 1 Geographers analyze complex issues and relationships with a distinctively spatial perspective\nLearning Objective: Describe different ways that geographers define regions. Essential Knowledge: Regions are defined on the basis of one or more unifying characteristics or on patterns of activity. A region is an area characterized by similarity or by cohesiveness that sets it apart from other areas. Regions allow us to generalize about a common characteristic so we can better group them A region is an area on the earth identified by two common characteristics: physical and political geography. Physical regions are features such as deserts, mountains, and lakes. Political regions by establishing political boundaries like the borders of countries. Some regions are based on culture (language or religion), while physical geography defines others. Types of regions include formal, functional, and perceptual/vernacular Formal (Uniform) Region with high level of consistency in a certain cultural or physical attribute E.g. Dairying region of North America Political boundaries Tropical Regions Functional (Nodal) A region with a node, sometimes a hearth, surrounded by interconnecting linkages. Usually connections relate to trade, communication, transportation, etc. E.g. Cell towers Newspaper Circulation School District Metropolitan Area Perceptual (Vernacular) A region defined by feelings and prejudices that may or may not be true. A construct of one\u0026rsquo;s mental map E.g. Bible belt South Regional boundaries are transitional and often contested and overlapping. Regions of the world can and do overlap such as the areas of Southeast Asia and Asia. Regions also have transitional boundaries like between North Africa and Sub-Saharan Africa Geographers apply regional analysis at local, national, and global scales There is not total agreement, however, among geographers on how all regions are defined. One geographer may place Chad in the region of North Africa, and another would classify Chad as part of Central Africa. Geographers will also use two different terms to describe the same area; the Middle East and Southwest Asia, for example. 👣 Unit 2 — Population and Migration Patterns and Processes # Developing Understanding\nThis unit addresses the patterns associated with human populations. Populations may increase or decrease as a result of a combination of natural changes (births and deaths) and migration patterns (emigration and immigration). Students examine population distributions at different scales—local, national, regional, and global. Population pyramids demonstrate age-sex structures, revealing the growth or decline of generations and allowing geographers to predict economic needs based on reproductive and aging patterns. Students learn about factors that influence changes in population as well as the long- and short-term effects of those population changes on a place’s economy, culture, and politics. For example, environmental degradation and natural hazards may prompt population redistribution at various scales, which in turn creates new pressures on the environment and on cultural, economic, and political institutions. The study of migration patterns allows students to examine factors contributing to voluntary and forced relocation and the impact of these migrating populations on existing settlements. Combined, the concepts and theories encountered in this unit help students develop connections and transfer their learning in upcoming units to course topics such as cultural patterns, the political organization of space, food production issues, natural resource use, and urban systems.\nBIG IDEA 1 Patterns and Spatial Organization (PSO)\nHow does where and how people live impact global cultural, political, and economic patterns? BIG IDEA 2 Impacts and Interactions (IMP) How does the interplay of environmental, economic, cultural, and political factors influence changes in population? BIG IDEA 3 Spatial Processes and Societal Change (SPS) How do changes in population affect a place’s economy, culture, and politics? 2.1 Population Distribution # PSO-1 Understanding where and how people live is essential to understanding global cultural, political, and economic patterns.\nLearning Objective: Identify the factors that influence the distribution of human populations at different scales. Essential Knowledge: Physical factors (e.g., climate, landforms, water bodies) and human factors (e.g., culture, economics, history, politics) influence the distribution of population. physical: humans avoid areas that are dry too dry to plant food lack water for crops + regular consumption wet + hot combination of rain and heat deplete nutrients from soil typically near equator cold + high altitude to rough for easy transportation to cold for crops places considered too harsh for occupancy have diminished over time people like to go to low-lying areas with fertile soil and temperate climate near a river or ocean is good too cultural: economics history politics Factors that illustrate patterns of population distribution vary according to the scale of analysis. Scale of analysis is the level of detail that a map goes into Demography: study of characteristics of human population, varies according to scale 75% of population live on 5% of land 50% of population live in urban areas Land inhabited called ecumene most extreme cases occur at small scales countries vary in size so aren\u0026rsquo;t great for seeing details, but they\u0026rsquo;re better than state Learning Objective: Define methods geographers use to calculate population density. Essential Knowledge: The three methods for calculating population density are arithmetic, physiological, and agricultural. Artihmetic / crude - all people / all land Physiological - all people / agricultural land Agricultural - all farmers / agricultural land Learning Objective: Explain the differences between and the impact of methods used to calculate population density. Essential Knowledge: The method used to calculate population density reveals different information about the pressure the population exerts on the land. High agricultural density implies that farmers aren\u0026rsquo;t extracting the most value from there land (better farmers need less farmers to fully use there land) Low agricultural density implies farmers are very efficient and probably developed High physiological density implies that there is either little farmland or that agricultural land is being used by more and may reach its output limit sooner than a country that has a lower density High arithmetic density implies there are a lot of people in a small area, aka urbanization 2.2 Consequences of Population Distribution # PSO-2 Understanding where and how people live is essential to understanding global cultural, political, and economic patterns.\nLearning Objective: Explain how population distribution and density affect society and the environment. Essential Knowledge: Population distribution and density affects political, economic, and social processes, including the provision of services such as medical care. redistricting / gerrymandering Results provision of services such as medical care those closer to medical services are more likely be able to use them Population distribution and density affect the environment and natural resources; this is known as carrying capacity carrying capacity — how many people an area can support on a sustained basis sustainability 2.3 Population Composition # PSO-2 Understanding where and how people live is essential to understanding global cultural, political, and economic patterns.\nLearning Objective: Describe elements of population composition used by geographers. Essential Knowledge: Patterns of age structure and sex ratio vary across different regions and may be mapped and analyzed at different scales most extreme cases occur at small scales countries vary in size so aren\u0026rsquo;t great for seeing details, but they\u0026rsquo;re better than state age structure older in retirement areas younger in military / college towns helps predict how much money is needed for social services old and young (dependent) require more money to support sex ratio more male in military training camps Learning Objective: Explain ways that geographers depict and analyze population composition. Essential Knowledge: Population pyramids are used to assess population growth and decline and to predict markets for goods and services. a representation of a country’s population displayed by age and gender groups on a bar graph. Normally shows the % of the total pop in 5-year age brackets with youngest at base of pyramid and oldest at the top. The length of the bar represents the % of total pop in that group. Males on left, females on right. Young population (bulge at bottom) Economic Risks Potential for job shortage Shift in workforce and jobs that target young people Social Increasing demand on support for youth Preschools, parks, etc. Health and Education Have to be prepared for childhood diseases Increase spending on family planning programs Middle (bulge in middle) Economic: Not enough upcoming workers Possible automation People work longer (later retirement) Aging population (bulge at top) Economic Transition to tertiary jobs Income tax burden falls on a shrinking workforce Over 65 expect long term expensive healthcare Strain on pension system with fewer paying in Capital flow from aging countries shifting global economic power Social Risks Migration needed to satisfy labor needs 2.4 Population Dynamics # IMP-2 Changes in population are due to mortality, fertility, and migration, which are influenced by the interplay of environmental, economic, cultural, and political factors.\nLearning Objective: Explain factors that account for contemporary and historical trends in population growth and decline. Essential Knowledge: Demographic factors that determine a population’s growth and decline are fertility, mortality, and migration. fertility crude birth rate (CBR) — total live births in a year for every 1,000 people alive in society infant mortality rate (IMR) — annual number of deaths of infants under 1 year of age compared with number of live births Decrease due to (life expectancy too) total fertility rate (TFR) — average number of children a woman will have throughout her childbearing years (15-49) mortality crude death rate (CDR) — total number of deaths in a year for every 1,000 people alive in society life expectancy — number of years expected for a newborn to live migration Geographers use the rate of natural increase and population-doubling time to explain population growth and decline. Natural increase rate (NIR) — Percentage by which a population grows in a year CBR - CDR = NIR Doubling Time — Number of years needed to double the population (assuming a steady rate of growth), is affected by NIR Social, cultural, political, and economic factors influence fertility, mortality, and migration rates. Social + cultural norms when to get married role of genders political laws anti/pro abortion one child policy (China) 2.5 The Demographic Transition Model # IMP-2 Changes in population are due to mortality, fertility, and migration, which are influenced by the interplay of environmental, economic, cultural, and political factors.\nLearning Objective: Explain theories of population growth and decline. Essential Knowledge: The demographic transition model can be used to explain population change over time. explains the rising and falling of NIR over time in a country no country has ever reverted back to a previous stage Stage 1: Hunter-gatherers, scarcity of food Stage 2: Increase in healthcare, still lots of kids Stage 3: Rural to urban, less kids (due to less space / resources in urban area) Stage 4: Developed, stuff balances out Stage 5: Replacement rate isn\u0026rsquo;t enough to sustain population Reasons for population drop off / decrease The epidemiological transition explains causes of changing death rates. stage 1 — prestillence and famine (high CDR): principal cause of death: infectious and parasitic diseases stage 2 — receding pandemic (rapidly declining CDR) factors such as improved sanitation, nutrition, and medicine stage 3 — degenerative disease (moderately declining CDR) decrease in deaths by infectious diseases, increase in deaths by chronic disorders associated with aging stage 4 — delayed degenerative diseases (low but increasing CDR) death by cardiovascular diseases and cancer delayed because of modern medicine (possible) stage 5 evolution: infectious disease microbes adapt around drugs, new strains form poverty: infectious diseases are more prevalent in poor areas due to unsanitary conditions and inability to afford medicine / treatment increased connections: advancements in transportation, e.g. air travel, increases contact as well as urbanization death rates are high during first, the drop off til they level around stage 4 this is due to increase in education and healthcare, as well as contraceptives Jobs Economic Activities Primary: Production of raw materials or natural resource extraction (e.g., agriculture, mining, energy, timber, fishing) Secondary: Processing or refining of natural resources (e.g., manufacturing finished goods, industry, building construction, assembly, factory work, value-added, blue collar) Tertiary: Provision of services (e.g., healthcare, technology, communications, financial, wholesale and retail trade, transportation, personal, professional, business services, white collar) How these patterns change as courtiers develop 2.6 Malthusian Theory # IMP-2 Changes in population are due to mortality, fertility, and migration, which are influenced by the interplay of environmental, economic, cultural, and political factors.\nLearning Objective: Explain theories of population growth and decline. Essential Knowledge: Malthusian theory and its critiques are used to analyze population change and its consequences. Thomas Malthus - proposed in his Essay on the Principle of Population 1798, that the population grows faster than the food supply. He claimed that while population expanded at a geometric or exponential rate, food supply increased arithmetically or linearly. However, the continued evolution of agriculture has continued to provide the world with an adequate amount of food. The problem now is distribution of food, not the actual production of it. Also, the birth rates declined sharply in the latter part of the 20th century, thus the world population expanded to only 6 billion compared to Malthus’s predicted 10. Neo-Malthusians claim that more LDC’s are in stage 2 of the demographic transition than ever before in history, thus putting a larger strain on the food supply. They also modified Malthus’s theory by stating that the population growth is out-stripping not just food production, but a wide variety of resources, such as oil, natural gas, etc. Critics of Malthus claim that population growth stimulates new technology and that as strain is put on any resource, the inventive human being will simply develop an alternative method once it is economically feasible. 2.7 Population Policies # SPS-2 Changes in population have long- and short-term effects on a place’s economy, culture, and politics.\nLearning Objective: Explain the intent and effects of various population and immigration policies on population size and composition. Essential Knowledge: Types of population policies include those that promote or discourage population growth, such as pronatalist, antinatalist, and immigration policies. countries fearing overpopulation may enact Antinatilist policies, e.g. China\u0026rsquo;s child limit countries fearing dipping below the replacement level, to assure the population continues to replace itself, may enact pronatilist policies, e.g. Scandinavian country\u0026rsquo;s proactive ads, or policy encourage immigration (migration in to the country) 2.8 Women and Demographic Change # SPS-2 Changes in population have long- and short-term effects on a place’s economy, culture, and politics.\nLearning Objective: Explain how the changing role of females has demographic consequences in different parts of the world. Essential Knowledge: Changing social values and access to education, employment, health care, and contraception have reduced fertility rates in most parts of the world. social: women have a say in whether or not they want a child economic: women joining the workforce leaves less time for relationships political roles: women in government can enact laws that better represent what women may need healthcare: higher chance of mother and child surviving contraception less likely for \u0026lsquo;accidents\u0026rsquo; Social status change Changing social, economic, and political roles for females have influenced patterns of fertility, mortality, and migration, as illustrated by Ravenstein’s laws of migration. Ravenstein\u0026rsquo;s Laws: Most migrants move only a short distance. Migration proceeds step by step. There is a process of absorption, whereby people immediately surrounding a rapidly growing town move into it and the gaps they leave are filled by migrants from more distant areas, and so on until the attractive force is spent. Migrants going long distances generally go by preference to one of the great centres of commerce or industry. Each current of migration produces a compensating counter-current. Natives of towns are less migratory than those of rural areas Females are more migratory than males within the kingdom of their birth, but males more frequently venture beyond. Most migrants are adults: families rarely migrate out of their country of birth. Large towns grow more by migration than by natural increase. Migration increases in volume as industries and commerce develop and transport improves. The major direction of migration is from the agricultural areas to the centres of industry and commerce. The major causes of migration are economic. Ravensteins laws aren\u0026rsquo;t scientific Are too specificied E.g. short distance occurs in Africa where most migrations is due to wars, while most migration from China is to the US (long) 2.9 Aging Populations # SPS-2 Changes in population have long- and short-term effects on a place’s economy, culture, and politics.\nLearning Objective: Explain the causes and consequences of an aging population. Essential Knowledge: Population aging is determined by birth and death rates and life expectancy. An aging population has political, social, and economic consequences, including the dependency ratio. dependency ratio: ratio of citizens under 15 or 65 and older to those between age 15 and 65 Gives us an idea of how many workers are needed to support the dependent population If the population is aging: Government spending for adult daycare, nursing homes, and home care social services will increase Government spending for education, child welfare, and health services will decrease Why population is aging Reduced Fertility Improved education of women, more women working, delays in starting families Children are an economic liability in MDCs, too expensive to have several, societal norms (1–2 children) Birth control: cost, availability, accessibility, acceptance, quality More urban societies: less need for children to work on farms Government and private pensions reduce “children as pension” Increased Life Expectancy Improved health care (e.g., medicine, facilities, research/knowledge, personnel, technologies, accessibility) Improved lifestyle (e.g., knowledge of health risks, improved diets, technology, nutrition and exercise) Improved food security/availability Less conflict (e.g., less crime, fewer wars) Improved work conditions (e.g., less physically demanding labor, better safety standards) Improved public health (e.g., sanitation, water supply, housing, standard of living) Improved financial security for elderly (e.g., pensions, care facilities) Improved safety standards (e.g., sports, transportation, building codes) Out-migration of Youth Out-migration of youth for better lifestyle (e.g., jobs, security) 2.1 Causes of Migration # IMP-2 Changes in population are due to mortality, fertility, and migration, which are influenced by the interplay of environmental, economic, cultural, and political factors.\nLearning Objective: Explain how different causal factors encourage migration. Essential Knowledge: Migration is commonly divided into push factors and pull factors. push factors — bad stuff in current location famine war no jobs disease violence hate crime overcrowding pull factors — good stuff somewhere else better jobs lower taxes better climate better schools/social services more room low crime Push/pull factors and intervening opportunities/obstacles can be cultural, demographic, economic, environmental, or political. Intervening obstacle — a feature that hinders migration cultural similar culture to home demographic people tend to like to be around similar people (age, race) economic better jobs environmental distance Distance Decay: Says that migrants try to minimize the friction of distance migrants will be more inclined to move to locations closer to them; they will be less interested in moving longer distances has been on the decline Intervening Opportunity\u0026rsquo;s: Idea that migrants will choose a location closer rather than farther if all other factors are the same related to other reasons (time and money go up farther you have to travel) climate (push + pull) oceans/water rugged terrain (intervening obstacles) political social services no war less corruption age 2.1 Forced and Voluntary Migration # IMP-2 Changes in population are due to mortality, fertility, and migration, which are influenced by the interplay of environmental, economic, cultural, and political factors.\nLearning Objective: Describe types of forced and voluntary migration. Essential Knowledge: Forced migrations include slavery and events that produce refugees, internally displaced persons, and asylum seekers types of migration: Immigrant: person entering another country with intention of living there Emigrant: person who is leaving one country with the intention of living in a different country refugee: A person who flees, is displaced, or is forced to leave his or her home country, often due to religion, ethnicity, race, or political belief Reasons IDP — Internally Displaced Person: A person who is forced out of the home region due to war, political, or social unrest, environmental problems, etc. does not cross any international boarders Types of voluntary migrations include transnational, transhumance, internal, chain, step, guest worker, and rural-to-urban. transnational migration across one or more nation transhumance movement between mountains and lowlands, typically practiced by farmers Cyclic Movement: movement that has a closed route repeated annually or seasonally internal permanent move w/in the same country. Interregional migration move from one region to another within country. e.g. from Middleton to North WI boonies Intraregional migration move within one region within one country. e.g. moving apartments in downtown Madison chain migration migration of people to a certain location because family members (or other contacts), typically of the same nationality, previously migrated there Examples must clearly establish a link/transfer of knowledge between the first group of migrants and subsequent groups OR it should be clear that subsequent migrants are from areas of close proximity to the source area of the early migrants, and that they are migrating to the same destination area. step migration to a distant destination that occurs in stages hard to measure / verify e.g. from farm to nearby village to town to city guest worker legal immigrant who has work visa, usually short term typically take unskilled labor jobs risk of them overstaying typically citizens of poor countries who temporarily obtain dangerous low-paying jobs in MDC’s that the permanent citizens refuse to accept. rural-to-urban migration flow going frow rural to urban areas In LDC’s, the migration trend recently has been rural to urban. In MDC’s, the migration trend has been urban to suburban Counterurbanization — net migration from urban to rural areas. This has been a trend in MDC’s, as improved technologies enable people to live farther from their places of employment and still enjoy all the amenities the city offers. However, in the U.S., counterurbanization has stopped because of poor economic conditions in the rural areas. Once again, the trend is from non-metropolitan to metropolitan areas, only now it is characterized by a move into the suburbs rather than the inner city. 2.1 Effects of Migration # IMP-2 Changes in population are due to mortality, fertility, and migration, which are influenced by the interplay of environmental, economic, cultural, and political factors.\nLearning Objective: Explain historical and contemporary geographic effects of migration. Essential Knowledge: Migration has political, economic, and cultural effects. cultural visually see change in landscape Chinatown, mosques, etc. political potentially controversial hate crimes economic typically beneficial fill unskilled labor jobs Migration transition identified by Wilbur Zelinsky consists of changes in a society comparable to the demographic transition. Stage 1 consists of little migration Stage 2 involves international migration Stages 3 and 4 are characterized by internal migration. brain drain may occur due to human capital theory of migration (states that smart people are the ones to leave and seek better job opportunities) smartest/most talented people leave a country, leaving only stupid people benefitial to both countries in-country gets talented labor and capital out-country gets often gets remittances (payments sent from individuals) Quotas — maximum limits on the number of people who could immigrate to the U.S. from a country in 1 year — may encourage illegal immigrants some countries rely on immigration from other countries to stay above the replacement level of 2.1 to assure the population stays even support those who are dependent (under 15, over 65) Normally shows the % of the total pop in 5-year age brackets with youngest at base of pyramid and oldest at the top. The length of the bar represents the % of total pop in that group. ⚱️ Unit 3 - Cultural Patterns and Processes # Developing Understanding\nThe main focus of this unit is on cultural patterns and processes that create recognized cultural identities. Students consider the physical environment to determine the effects of geographical location and available resources on cultural practices. Visuals representing artifacts, mentifacts and sociofacts all shed light on cultural landscapes and how they change over time. Practice in analyzing images of different places at different times for evidence of their ethnicity, language, religion, gender roles and attitudes, and other cultural attributes builds students’ understanding of cultural patterns and processes. This unit also considers from a temporal and spatial perspective how culture spreads, through traditional forces such as colonialism and imperialism and through contemporary influences such as social media. Rather than emphasize the details of cultural practices associated with specific languages and religions, this unit instead focuses on the distribution of cultural practices and on the causes and effects of their diffusion. For example, students might study the distribution of Chinese versus English languages or the diffusion patterns of religions such as Hinduism and Islam, at local, national, or global scales. An understanding of the diffusion of cultural practices provides a foundation for the study of political patterns and processes in the next unit\nBIG IDEA 1 Patterns and Spatial Organization (PSO)\nHow does where people live and what resources they have access to impact their cultural practices? BIG IDEA 2 Impacts and Interactions (IMP) How does the interaction of people contribute to the spread of cultural practices? BIG IDEA 3 Spatial Processes and Societal Change (SPS) How and why do cultural ideas, practices, and innovations change or disappear over time? 3.1 Introduction to Culture # PSO-3 Cultural practices vary across geographical locations because of physical geography and available resources.\nLearning Objective: Define the characteristics, attitudes, and traits that influence geographers when they study culture. Essential Knowledge: Culture comprises the shared practices, technologies, attitudes, and behaviors transmitted by a society. shared practices teaching style events holidays technologies what side of the road we drive on 120/240 volt outlet attitudes how we treat different genders behaviors how we greet one another Social customs originates at a hearth, a center of innovation. Folk customs tend to have anonymous sources, from unknown dates, through multiple hearths Folk music tells stories or conveys information about daily activities. Isolation promotes cultural diversity as a group’s unique customs develop over several centuries. Therefore, folk culture varies widely from place to place at one time. Since most folk culture deals in some way with the lives and habits of its people, the physical environment in which the people act has a tremendous impact on the culture. pop culture generally has a known originator, normally from MDC’s, and results from more leisure time and more capital. Pop music is written by specific individuals for the purpose of being sold to a large number of people. Cultural traits include such things as food preferences, architecture, and land use. Food: the food we eat daily and the food we experience while traveling around the world all depends upon our location often passed throughout generations brings us together; sharable food in US reflects patterns of migration and what we\u0026rsquo;re capable of growing food also shows if there\u0026rsquo;s any (religious) taboos Low meat consumption in Africa and S Asia due to large Hindu population, which don\u0026rsquo;t eat beef because they believe the cow to be sacred Muslim and Judaism prohibited from eating pork Somali clans restrict the consumption of fish Land use: location determines what foods are grown locally and readily available in a particular region of the world certain states are known for certain foods; Jersey tomatoes, Florida oranges, Georgia peaches Architectural trait reflection of our built land scape can explain what was going on at a given time and what resources were available Cultural relativism and ethnocentrism are different attitudes toward cultural difference. relativism looking at culture objectively / holistically believing nothing is right or wrong ethnocentrism viewing other cultures through your own believing that your culture is the norm / superior pride in heritage, devaluation of other groups 3.2 Cultural Landscapes # PSO-3 Cultural practices vary across geographical locations because of physical geography and available resources.\nLearning Objective: Describe the characteristics of cultural landscapes. Essential Knowledge: Cultural landscapes are combinations physical features, agricultural and practices, religious and linguistic characteristics, evidence of sequent occupancy, and other expressions of culture including traditional and postmodern architecture and land-use patterns. Toponyms reflect leaders / people of power / religious figures / in immigrant\u0026rsquo;s native language Religious impact Religious architecture sacred spaces religious symbols Architecture traditional architecture built for elements that impact the area being built on built with materials (abundantly) available post-modern architecture is designed to descriptive / feats of design Sequent Occupance early societies leave their cultural imprint on a place old fashion architecture old-street\u0026rsquo;s names land surveys — how land was parceled / divided square pattern system long-lot (french) housing reflects cultural identities of those who lived there and environmental constraints \u0026ldquo;The cultural landscape is fashioned from a natural landscape by a cultural group. Culture is the agent, the natural area is the medium, the cultural landscape is the result.\u0026rdquo; - Carl Sauer\nGentrification Positive Impacts Negative Impacts How govt can slow down + rpevent Learning Objective: Explain how landscape features and land and resource use reflect cultural beliefs and identities Essential Knowledge: Attitudes toward ethnicity and gender, including the role of women in the workforce; ethnic neighborhoods; and indigenous communities and lands help shape the use of space in a given society. positive attitudes encourage chain migration can lead to subsections of area with high concentration of a particular ethnicity China town little Mexico generally, women are becoming more equal but there\u0026rsquo;s still progress to be made 3.3 Cultural Patterns # PSO-3 Cultural practices vary across geographical locations because of physical geography and available resources.\nLearning Objective: Cultural practices vary across geographical locations because of physical geography and available resources. Essential Knowledge: Regional patterns of language, religion, and ethnicity contribute to a sense of place, enhance placemaking, and shape the global cultural landscape. language dialects phrases ethnicity immigrants shape neighborhoods china town sense of place link Bilingualism Language, ethnicity, and religion are factors in creating centripetal and centrifugal forces. countries need a stronger centripetal force than centrifugal force to stay intact forces and produce regionalism or dissimilarity between people in the same country centripetal exist in a place and don\u0026rsquo;t move — typically people don\u0026rsquo;t wanna or can\u0026rsquo;t leave while push/pull are actively working centripetal forces — together Centripetal forces unify a state (provide stability, strengthen, bind together, create solidarity). culture regligious acceptance ethnic unity and tolerance social equity economic equity just and fair legal system nationalism history — common heritage centrifugal forces — apart (repel) Centrifugal forces divide a state (lead to balkanization/devolution, disrupt internal order, destabilize, weaken). cultural diversity religious differences language ethnic conflict social injustice nationalism legal restrictions physical features economic stratification “White flight” is the rapid fleeing of whites from the cities as black families emigrate out of the ghettos, or as the ghetto expands. It was encouraged by blockbusting. blockbusting- the real estate practice of scaring whites into selling their homes at low prices by telling them that blacks would soon be moving in and causing property values to fall. The real estate agents then turned around and sold the homes at extremely high prices to blacks that were emigrating from the inner city. Apartheid-the physical separation of different races into different geographic areas, i.e. South Africa. The apartheid laws were repealed in 1991 in South Africa, but many years will be needed to erase the legacy of such racist policies. 3.4 Types of Diffusion # IMP**-3 The in**teraction of people contributes to the spread of cultural practices.\nLearning Objective: Define the types of diffusion. Essential Knowledge: Relocation and expansion — including contagious, hierarchical, and stimulus expansion — are types of diffusion Relocation — hearth moves act of people physically moving takes time, typically slow not everyone is going to absorb it term: person who uses a term moves to a new location and continues to use the term in the new location, OR a form of media, in which a term is used, is relocated to a new place and the term is used in the new location. Example: Spread of Christianity, when people moved and brought it with them expansion — hearth stays same place contagious diseases rapid spread term: an individual uses, or individuals use, the new word and then acquaintances (or those in close proximity to them) begin to use the word as well Example: Hinduism spreading throughout the Indian subcontinent hierarchical intent is to spread selectively term: celebrities start to use the new word and then it spreads to others down the social hierarchy OR people in large cities start to use the word and then the word eventually gets to smaller places or media markets, OR Reverse Hierarchical: minority use of the term spreads up the social ladder to majority group(s). spread by choice, often wealth stimulus innovative idea diffuses from its hearth outward, but the original idea is changed by the new adopters Example: Different Menu items from McDonalds around the world. 3.5 Historical Causes of Diffusion # SPS-3 Cultural ideas, practices, and innovations change or disappear over time.\nLearning Objective: Explain how historical processes impact current cultural patterns. Essential Knowledge: Interactions between and among cultural traits and larger global forces can lead to new forms of cultural expression; for example, creolization and lingua franca. lingua franca a language that is informally agreed upon as the language of business and trade Reasons for US being lingua franca / dominant Historical Globalization creolization mix of languages that form its own language native to some group of people vocab from different language pidgin no ones native language usually result of trading between groups of people who don\u0026rsquo;t share the same language Colonialism, imperialism, and trade helped to shape patterns and practices of culture. Colonialism + imperialism The widespread diffusion of English is thanks, in large part, to the colonial practices of the British trade link 3.6 Contemporary Causes of Diffusion # SPS-3 Cultural ideas, practices, and innovations change or disappear over time.\nLearning Objective: Explain how historical processes impact current cultural patterns. Essential Knowledge: Cultural ideas and practices are socially constructed and change through both small-scale and large-scale processes such as urbanization and globalization. These processes come to bear on culture through media, technological change, politics, economics, and social relationships. sense of place is the special perception we have of a certain place based on our feelings, emotion, and associations with that place. also called distinctive culture Placelessness is he loss of a place\u0026rsquo;s unique favor and identity because of standardizing influence of popular culture and globalization Communication technologies, such as the internet and the time-space convergence, are reshaping and accelerating interactions among people; changing cultural practices, as in the increasing use of English and the loss of indigenous languages; and creating cultural convergence and divergence. the internet has united us more pros easier to share + spread culture english is language of the internet, universal cons culture can be easily judged without full context english is prereq and reducing usage of other languages 3.7 Diffusion of Religion and Language # IMP**-3 The** interaction of people contributes to the spread of cultural practices.\nLearning Objective: Explain what factors lead to the diffusion of universalizing and ethnic religions. Essential Knowledge: Language Language families, languages, dialects, world religions, ethnic cultures, and gender roles diffuse from cultural hearths. Diffusion of language families, including Indo-European, and religious patterns and distributions can be visually represented on maps, in charts and toponyms, and in other representations. Religion Hinduism Hearth: Pakistan in ~2300 BCE Because Hinduism is an ethnic religion it primarily diffused via relocation diffusion to India and Nepal Judaism Hearth: Isreal/Palestine in ~1800 BCE Due to persecution from many countries, Judaism has diffused across many countries but is most prominent in Isreal and the United States now. Christianity Hearth: Jerusalem on 1 AD Christianity largely spread due to conquest throughout much of the Roman Empire, and again later on through colonialism. Now it’s the most practiced religion and is most influential in Europe, the Americas, South Africa, and Australia Islam Hearth: Arabian Peninsula/Saudi Arabia in ~600 CE Spread via conquest and trade, concentrated primarily in the Middle East, North Africa, Southwest Asia, and some portions of Southeast Asia Buddhism Hearth: Nepal in ~500 BCE Missionaries and trade helped diffuse Buddhism, and it’s not found in Southeast and East Asia, India, Sri Lanka, and Tibet Religion can identify, unit, or divide a group of people RELIGION IS ARGUABLY THE MOST VOLITALE OF ALL HUMAN RELATIONS AND THE SOURCE OF MOST VIOLENCE THROUGHOUT HISTORY. EUnion forbid any religious symbols such as crucifixes, crosses, etc. on public school walls and calls them a violation of religious and educational freedom Religion is nearly always suppressed in communist countries. Leaders believe that religion has a tendency to upset stability and therefore ban it altogether, though often they just concrete the people’s religious adherence instead of destroying it. Religions have distinct places of origin from which they diffused to other locations through different processes. Practices and belief systems impacted how widespread the religion diffused. Romans came and pushed out Jews, forcing them to diffuse Many religions spread via trade routes Christian countries tended to have many trade routes, so it spread to a range of areas Islamic countries didn\u0026rsquo;t trade very far outside Africa and Asia (a bit to Europe) Buddhism barely diffused from Asia Universalizing religions, including Christianity, Islam, Buddhism, and Sikhism, are spread through expansion and relocation diffusion. Anyone can become a member of a universalizing religion universalizing religion stories often attempt to explain the mystical and there calendar\u0026rsquo;s main purpose in calendars is to commemorate events in the founder’s life, thus the seasons or weather are not central to the structure Christianity, Buddhism, and Islam are the three main Buddhism and Islam are the universalizing religions that place the most emphasis on identifying shrines/holy places. In universalizing religions, the holy places are generally locations at which memorable events happened in the founder’s life, such as Mecca is in Islam because it is Muhammad’s birthplace. Holy places in ethnic religions are often physical features that are closely tied to the religion (For example, in Hindu one of the most important rituals is the bathing of oneself in the Ganges River.) Members actively proselytize, or seek new converts by sending missionaries through the world to spread their beliefs Excluding Hinduism, this shows diffusion of universalizing religions Ethnic religions, including Hinduism and Judaism, are generally found near the hearth or spread through relocation diffusion. Only really spread from generation to generation Ethnic religious creation stories tend to deal with the physical environment and natural events and typically organize their calendars around the seasons, other natural events, or the physical geography Ethnic religions rarely diffuse, and when they do, it is to a small extent Judaism is an exception in that it has diffused widely throughout the years, mainly because its people have had to flee persecution from many areas in the world. Traditional religions subgroup of ethnic practiced by small groups, typically within a village or tribe 3.8 Effects of Diffusion # SPS-3 The interaction of people contributes to the spread of cultural practices.\nLearning Objective: Explain how the process of diffusion results in changes to the cultural landscape Essential Knowledge: Acculturation, assimilation, syncretism, and multiculturalism are effects of the diffusion of culture. Acculturation process of adopting some of the values, customs, and behaviors of the host culture immigrants may adopt the language and a few other customs of the host group but will retain many distinctive customs assimilation Assimilation is the process whereby individuals or groups of differing ethnic heritage are totally absorbed into the dominant culture of a society. The process of assimilating involves taking on the traits of the dominant culture to such a degree that the assimilating group becomes socially indistinguishable from other members of the society. as we become more assimilated, languages become lost syncretism blending of cultures and ideas from different places multiculturalism grouping of various cultures in a certain area can lead to loss of cultural uniqueness, languages, and general \u0026ldquo;sameness\u0026rdquo; — link 🏛️Unit 4 - Political Patterns and Processes # Developing Understanding\nThis unit addresses the political organization of the world. Building on knowledge of populations and cultural patterns learned in previous units, students examine the contemporary political map and the impact of territoriality on political power and on issues of identity for peoples. Students also look at the different types of political boundaries, how they function, and their scale, as they consider both internal and international boundaries. The interplay of political and cultural influences may cause tensions over boundaries to arise, such as sovereign states making claims on what other states consider to be international waters. Students also examine forms of government and how forces such as devolution may alter the functioning of political units and cause changes to established political boundaries. Separatist and independence movements that challenge the sovereignty of political states may arise from economic and nationalistic forces, as seen in Scotland, Northern Ireland, and Spain. The influence of supranational organizations such as the United Nations or European Union and their role in global affairs presents another challenge to nationalist sovereignty. Student understanding of cultural patterns and processes helps inform their understanding of the consequences of centrifugal and centripetal forces.\nBIG IDEA 1Patterns and Spatial Organization (PSO)\nHow do historical and current events influence political structures around the world? BIG IDEA 2 Impacts and Interactions (IMP) How are balances of power reflected in political boundaries and government power structures? BIG IDEA 3Spatial Processes and Societal Change (SPS) How can political, economic, cultural, or technological changes challenge state sovereignty? 4.1 Introduction to Political Geography # PSO-4 The political organization of space results from historical and current processes, events, and ideas.\nLearning Objective: For world political maps: a. Define the different types of political entities. b. Identify a contemporary example of political entities. Essential Knowledge: Independent states are the primary building blocks of the world political map. Types of political entities include nations, nation-states, stateless nations, multinational states, multistate nations, and autonomous and semiautonomous regions, such as American Indian reservations. state: a country and not a political subdivision within the united states, such as Nevada or Maine has sovereignty, boundaries, and a permanent population nations: unified group of people with a common culture Navajo, Roma nation-states: States in which over 90 percent of the population are the same specific culture or group of people A politically organized area in which nation and state occupy the same space. can act as a centripetal factor to those of the same ethnicity as majority Japan, Iceland, Armenia, Bangladesh, Lesotho stateless nations A nationality that is not represented by a state. multinational states states made up of a two or more ethnic groups United states, Canada, China, Russia, India, Brazil multistate nations country with two or more nationalities within its borders (a nation that exists in multiple states) Kurds, French, Basque Buffer state States that are allowed to exist by neighboring states (to help relieve tension between the neighboring states). Mongolia between China and Russia autonomous a group of people or territory are self-governing, thus not under the control of a higher level of government semiautonomous regions a group of people that have some level of automity, but are still controlled by another entity 4.2 Political Processes # PSO-4 Explain the processes that have shaped contemporary political geography.\nLearning Objective: Explain the processes that have shaped contemporary political geography. Essential Knowledge: The concepts of sovereignty, nation-states, and self-determination shape the contemporary world sovereignty internationally recognized exercise of a country\u0026rsquo;s power over its people and territory nation-states URL self-determination the concept that ethnicities have the right to govern themselves can lead to irredentism Colonialism, imperialism, independence movements, and devolution along national lines have influenced contemporary political boundaries. Colonialism when a group of people impose a set of formal controls by the mother country over its colonies or outside territories Colonizers colonize because of Gold — seek monitary gains at (most of the time) any expense God — want to spread there own religion Glory — clout Effects still present today Social unrest typically speak langauge of colonizers French in Northern Africa, English (British) in Southern Africa boundaries expand that aren\u0026rsquo;t physically connected to mother country imperialism the use of military, cultural domination, and or economic sanctions to gain control of a country and its resources boundaries expand from mother country independence movements can result in section breaking off devolution Quizlet the transition of power from the central government to regional governments in a state is done by Altering of a constitution Experiments on new governmental body Internal Division (Ethnocultural, Economic, or Spatial) results in Creation of an independent state Calls for Autonomy can result in section breaking off 4.3 Political Power and Territoriality # PSO-4 Explain the processes that have shaped contemporary political geography.\nLearning Objective: Describe the concepts of political power and territoriality as used by geographers Essential Knowledge: Political power is expressed geographically as control over people, land, and resources, as illustrated by neocolonialism, shatter belts, and choke points. neocolonialism Refers to the economic control that MDCs are sometimes believed to have over LDCs. Through organizations such as the IMF, the MDCs are able to dictate precisely what LDCs economic policies are, or are able to use their economic subsidies to put LDCs industries out of business. shatter belts an area of instability between regions with opposing political and cultural values choke points A geographical land feature such as a valley or water way narrowing causing a decrease in forces making their way through. Territoriality is the connection of people, their culture, and their economic systems to the land. boundaries are set to connect people with same/similar culture and that want to have an economic relation with one anther Stages of Economic Growth and Core Periphery Model (Core-Periphery) Core Periphery Stages of each Reasons for economic location Core-periphery leads to uneven spatial distribution of economic, political, or cultural power 4.4 Defining Political Boundaries # IMP-4.Political boundaries and divisions of governance, between states and within them, reflect balances of power that have been negotiated or imposed.\nLearning Objective: Define types of political boundaries used by geographers. Essential Knowledge: Types of political boundaries (barriers)include relic, superimposed, subsequent, antecedent, geometric, and consequent boundaries. relic boundary that used to exist, but is no longer active / present You can often still see effects of relic boundary even if they aren\u0026rsquo;t there e.g. great wall of China, East and West Germany superimposed boundary imposed by an outside force may not reflect cultural landscape e.g. treaties, Africa during Colonial era subsequent corresponds to group that is there, often regardless of cultural divide boundary set after the settlements of different groups meet often correspond to ecumene wall impacts consequent boundaries A boundary line that coincides with some cultural divide, such as religion or language. E.g. India antecedent pre-existing; most commonly physical features such as rivers, bays, mountains, desserts can potentially be removed with technology road build across dessert whole through mountain geometric straight lines US-Canada boarder because they aren\u0026rsquo;t visible, can lead to conflict Shapes compact round easy defense and communication prorupted or protruded round with a large extension increases access to resources or water/ports elongated long, narrow difficult communication between areas fragmented two or more areas separated by another country or body of water difficult communication perforated totally surrounds another country can abuse country w trade taxes/tariffs landlocked 4.5 The Function of Political Boundaries # IMP-4. Political boundaries and divisions of governance, between states and within them, reflect balances of power that have been negotiated or imposed.\nLearning Objective: Explain the nature and function of international and internal boundaries. Essential Knowledge: Boundaries are defined, delimited, demarcated, and administered to establish limits of sovereignty, but they are often contested. defined Treaty or legal document delimited Drawn on map in agreement demarcated VISUALLY MARKED walls, posts, fence administered the enforcement \u0026amp; maintaining of a boundary by government who can cross? what goods can cross? demarcated? Political boundaries often coincide with cultural, national, or economic divisions. However, some boundaries are created by demilitarized zones or policy, such as the Berlin Conference. most of the time are made to not piss people / countries off if made by a supranational org Land and maritime boundaries and international agreements can influence national or regional identity and encourage or discourage international or internal interactions and disputes over resources. The United Nations Convention on the Law of the Sea defines the rights and responsibilities of nations in the use of international waters, established territorial seas, and exclusive economic zones. UN came up with these zones EEZ is important because you can tax ships that travel and make money Includes islands too which has put pressure / created conflict at some islands South China Sea UN came in because otherwise it was hard to figure out who owned what area and lead to conflicts 4.6 Internal Boundaries # IMP-4.Political boundaries and divisions of governance, between states and within them, reflect balances of power that have been negotiated or imposed.\nLearning Objective: Explain the nature and function of international and internal boundaries. Essential Knowledge: Voting districts, redistricting, and gerrymandering affect election results at various scales. Voting districts subsections in states specifically for voting redistricting occurs once in every 10 years in US after census goal is to be drawn fairer and group like people often results in gerrymandering gerrymandering redistricting in such a way that it favors a political party 4.7 Forms of Governance # IMP-4.Political boundaries and divisions of governance, between states and within them, reflect balances of power that have been negotiated or imposed.\nLearning Objective: Define federal and unitary states. Essential Knowledge: Forms of governance include unitary states and federal states unitary states most power lies in centralized govt areas far away from central power aren\u0026rsquo;t represented equally can pull a country together federal states most power lies in local govt country is split into states / provinces, typically to group like-people can better represent areas Green = Federal Learning Objective: Explain how federal and unitary states affect spatial organization. Essential Knowledge: Unitary states tend to have a more top-down, centralized form of governance, while federal states have more locally based, dispersed power centers. unitary states focus on central government or big city federal states multiple nodal points stateless compact around a center nodal point small boarders 4.8 Defining Devolutionary Factors # SPS-4.Political, economic, cultural, or technological changes can challenge state sovereignty.\nLearning Objective: Define factors that lead to the devolution of states Essential Knowledge: Factors that can lead to the devolution of states include the division of groups by physical geography, ethnic separatism, ethnic cleansing, terrorism, economic and social problems, and irredentism. physical geography if state is physically separated by land, water, etc. ethnic separatism rise of ethnic groups in a state that want there own statehood ethnic cleansing not genocide genocide = killing people ethnic cleansing is moving certain people out political justification Heartland-rimland theory justified eu colonization during 19th century by claiming EU was the heartland and the surrounding territories comprised of the rimland the heartland was well positioned to dominate the world because of the immense size of its mass. since Russia formed the major part of the heartland, Mackinder (creator of theory) influenced politicians of the day to try to limit Russia\u0026rsquo;s expansion by colonizing territories near Russia tl;dr — Politicians used some crappy justification that aligned with the biases to further there agenda domino thoery once a country became communist, the neighboring countries around it were likely to also become communist terrorism goal is to intimidate or coerce a govt to do the terrorists political or social objective serves to pull country apart economic richer areas can want to split off so that they don\u0026rsquo;t have to pay a majority in taxes to people they don\u0026rsquo;t know/relate to social problems due to differences between cultural groups irredentism the goal of a group of people to want to unit with another group of people who share cultural elements with, but are divided by national boundaries can result in civil wars 4.9 Challenges to Sovereignty # SPS-4.Political, economic, cultural, or technological changes can challenge state sovereignty.\nLearning Objective: Explain how political, economic, cultural, and technological changes challenge state sovereignty. political superimposed boarders can lead to people upset economic area wanting less taxes because they\u0026rsquo;re paying an unequal amount terrorist groups attacking transportation / pipes cultural e.g. one group wanting an official language and another group wanting a different official language Essential Knowledge: Devolution occurs when states fragment into autonomous regions; subnational political territorial units, such as those within Spain, Belgium, Canada, and Nigeria; or when states disintegrate, as happened in Sudan and the former Soviet Union. autonomous regions; subnational political territorial units Link E.g. Spain, Belgium, Canada, and Nigeria disintegrate E.g. Sudan and the former Soviet Union Advances in communication technology have facilitated devolution, supranationalism, and democratization. devolution — link supranationalism easier to connect with similar people and want to join them Political, economic, and/or cultural cooperation among national states to promote shared objectives Tendency for states to give up political power to a higher authority in pursuit of common objectives (political, economic, military, environmental) Venture involving multiple national states (two or more, many, several) with a common goal democratization easier to see how much better it is outside your country with internet Global efforts to address transnational and environmental challenges and to create economies of scale, trade agreements, and military alliances help to further supranationalism. economies of scale trade agreements military alliances Supranational organizations—including the United Nations (UN), North Atlantic Treaty Organization (NATO), European Union (EU), Association of Southeast Asian Nations (ASEAN), Arctic Council, and African Union— can challenge state sovereignty by limiting the economic or political actions of member states. international group which the power and influence of member states transcend national boundaries or interest to share in decision making and vote on issues concerning the collective body member states give up some rights for common good of supranational organziation cooperation should resolve conflict sometimes used for collective defense can make economic stuff easier by opening boarders to member states set standards 4.1 Consequences of Centrifugal and Centripetal Forces # SPS-4.Political, economic, cultural, or technological changes can challenge state sovereignty.\nLearning Objective: Explain how the concepts of centrifugal and centripetal forces apply at the state scale. Essential Knowledge: Centrifugal forces may lead to failed states, uneven development, stateless nations, and ethnic nationalist movements. failed states political body that has disintegrated to a point where basic conditions and responsibilities of a sovereign govt no longer function most LDCs uneven development tend to be poor with corrupt govt stateless nations can serve as a centrifugal factor to state they\u0026rsquo;re in united inside there own area typically grouped by ethnic groups ethnic nationalist movements lead to separation movements riots against govt if not adequately represented Centripetal forces can lead to ethnonationalism, more equitable infrastructure development, and increased cultural cohesion. ethnonationalism nationalism of people with common background / language unites people more equitable infrastructure development increased cultural cohesion 🚜Unit 5 — Agriculture and Rural Land-Use Patterns and Processes # Developing Understanding\nThis unit examines the origins of agriculture and its subsequent diffusion. Students learn about the ways agricultural practices have changed over time as a result of technological innovations, such as equipment mechanization and improvements in transportation that create global markets. In addition, they examine the consequences of agricultural practices such as the use of high-yield seeds and chemicals, revisiting the human–environmental relationships studied in Unit 1. Course emphasis on spatial patterns is evident in this unit as students consider the differences in what foods or resources are produced and where they are produced. These agricultural production regions are impacted by economic and technological forces that increase the size of agricultural operations and the carrying capacity of the land. This has in turn created a global system of agriculture and the interdependence of regions of agricultural consumption and production. Student understanding of this global system of agriculture based on government cooperation lays the foundation for a deeper understanding of economic development in the final unit of the course.\nBIG IDEA 1Patterns and Spatial Organization (PSO)\nHow do a people’s culture and the resources available to them influence how they grow food? BIG IDEA 2 Impacts and Interactions (IMP) How does what people produce and consume vary in different locations? BIG IDEA 3Spatial Processes and Societal Change (SPS) What kind of cultural changes and technological advances have impacted the way people grow and consume food? 5.1 Introduction to Agriculture # PSO 5 Availability of resources and cultural practices influence agricultural practices and land-use patterns.\nLearning Objective: Explain the connection between physical geography and agricultural practices. Essential Knowledge: Agricultural practices are influenced by the physical environment and climatic conditions, such as the Mediterranean climate and tropical climates. During the First Agricultural Revolution: Sarted around 11,000 BC Unknown origins / how it happened (before recording of history) Humans radically changed their behavior: Went from hunting/gather life style to settling in to single areas and cultivating the land, planting crops, and raising animals Agriculture is impacted by: Land — cheap, near market + transportation, good quality soil Labor — cheap, skilled (enough), enough quantity Climate — have to meet requirements of crop, enough rain Mediterranean Agriculture: produces grapes and olives (cash crops), alongside citruses and figs — helps attract tourists, contributes to culture still produce cereals, especially wheat for pasta/bread requires warm year-round climate with lots of sunshine and boardering a sea horticulture: growing fruits, veggies, and flowers Intensive farming practices include market gardening, plantation agriculture, and mixed crop/livestock systems. Land: Small — land isn\u0026rsquo;t cheap so only small portions of high quality land is used High yield (food produced) to feed consumers Location: Closest to market Relies on lots of labor and tech (pesticides, fertilizer, etc.) (Oftentimes) variety of products produced — polyculture Examples: market gardening close to market small scale production of cash crops: fruits, vegetables, and flowers (apples, asparagus, cherries, lettuce, mushrooms, tomatoes) sold directly to local consumers. truck farming: truck means \u0026ldquo;barter\u0026rdquo; or \u0026ldquo;exchange\u0026rdquo;, not a physical truck plantation agriculture (cash crop, cashcrop) highly efficient tend to be established in or near the tropics produce a cash crop mixed crop/livestock systems Both animal and crops are farmed in the same area. Most common form of agriculture in US Crops, like maize and soybeans, are grown primarily to feed animals Utilizes crop rotation: cycles various crops and fields left to fallow (naturally grow over) to allow nutrient replenishing China, India, and SE Asia rely on this type of agriculure to double-crop Fit 2 years of harvest in 1 year Extensive farming practices include shifting cultivation, nomadic herding, and ranching. Land: Large Location: Farther from market — land isn\u0026rsquo;t cheap so it\u0026rsquo;s not that great and far away from market Examples: Shifting cultivation (slash-and-burn) vegetation is cut down and then ignited to make the ground more productive low yield / ineffective occurs in tropics More notes Nomadic herding (animal husbandry) based on herding domesticated animals. can result in desertification low yield, but only needs to support tribe/family Ranching commercial grazing of livestock over an extensive area practiced is semi-arid or arid land, where vegetation is too sparse or the soil to too poor to support crops prominent in later 19th century in the American West; on the decline due to low profit margins — more intensive to go into mono farming 5.2 Settlement Patterns and Survey Methods # PSO 5 Availability of resources and cultural practices influence agricultural practices and land-use patterns.\nLearning Objective: Identify different rural settlement patterns and methods of surveying rural settlements. Essential Knowledge: Specific agricultural practices shape different rural land-use patterns. Rural defined as \u0026lt; 2500 residents, and between 1 and 999 person per square mile 3 factors that affect the pattern of rural setllement: The kind of resource/feature that attracts people to the area (forests, farmlands, fields) The transportation method avaliable at the time of settlement (rivers, roads) Role of government policy, especially the land survey system (metes-and-bonds, long lot, rectangle, etc.) Rural settlement patterns are classified as clustered, dispersed, or linear. clustered A clustered rural settlement typically includes homes, barns, tool sheds, and other farm structures, plus personal services, such as religious structures and schools. dispersed characterized by farmers living on individual farms isolated from neighbors rather than alongside other farmers in the area. linear Linear rural settlements feature buildings clustered along a road, river, or dike to facilitate communications. nucleated a number of families live in close proximity to each other, with fields surrounding the collection of houses and farm buildings (e.g., Asian longhouse) Rural survey methods include metes and bounds, township and range, and long lot. Rectangular survey system Also called the Public Land Survey The system was used by the U.S. Land Office Survey to parcel land west of the Appalacian mountains. The system divides land into a series of rectangular parcels. metes and bounds A system of land surveying east of the Appalachian Mountains (EU) It is a system that relies on descriptions of land ownership and natural features such as streams or trees. township and range rectangular land division scheme designed by Thomas Jefferson grid system Intended to disperse settlers evenly across farmlands of the U.S. interior long lot Distinct regional approach to land surveying found in the Canadian Maritimes, parts of Quebec, Louisiana, and Texas designed to give everyone an equal type of land / soil (one person shouldnt get all poor land / land on a road or river / etc.) Land is divided into narrow parcels stretching back from rivers, roads, or canals. 5.3 Agricultural Origins and Diffusions # SPS 5 Agriculture has changed over time because of cultural diffusion and advances in technology.\nLearning Objective: Identify major centers of domestication of plant sand animals. Essential Knowledge: Early hearths of domestication of plants and animals arose in the Fertile Crescent and several other regions of the world, including the Indus River Valley, Southeast Asia, and Central America. Fertile Crescent (Mesopotamia) — 10,000 years ago 1200 years ago it became good for sedentary agriculture In the Middle East that includes most of Iraq (Known as Mesopotamia in the past), Syria, Lebanon, Israel, and the Nile River basin in Egypt. Huang He (Yellow) Valley — 10,000 years ago Flooding of rivers resulted in people settling near them and using them to farm Barley, wheat, lentils, and olives Diffused west to EU + Central Asia Experienced the Primary Revolution later than in the Fertile Crescent/Mesopotamia Nile River Valley — 8,000 years ago Used crop rotation with lagoons and cereals to reduce salt build up Settlements around Nile Indus River Valley — 4,000 years ago Extended from modern-day northeast Afghanistan to Pakistan and northwest India. Important innovations of this civilization include standardized weights and measures, seal carving, and metallurgy with copper, bronze, lead, and tin. Central America. Uncertain time period, but probably happened last Some scholars estimate 2000 BC, but may go up to the discovery of the Americas by the Europeans because some Native Tribes had not progressed to the First Agricultural Revolution by that time Beans, maize (corn), and cotton Diffused North and South Sub-Saharan Africa Sorghum, yams, milet, and rice 10,000 years ago Diffused south Learning Objective: Explain how plants and animals diffused globally. Essential Knowledge: Patterns of diffusion, such as the Columbian Exchange and the agricultural revolutions, resulted in the global spread of various plants and animals. 5.4 The Second Agricultural Revolution # SPS 5 Agriculture has changed over time because of cultural diffusion and advances in technology.\nLearning Objective: Explain the advances and impacts of the second agricultural revolution. Essential Knowledge: New technology and increased food production in the second agricultural revolution led to better diets, longer life expectancies, and more people available for work in factories. Page 105 in Barron\u0026rsquo;s Occurred in 1700\u0026rsquo;s through 1940\u0026rsquo;s (alongside industrial revolution) Advances: Motors, specifically tractors, which further advanced stuff in the 1930\u0026rsquo;s People used crop rotation instead of letting land grow over Where: Happened in Europe and North America Started with horse-drawn hoes in England Outcomes: Surplus of crops in England diffused through EU People ate healthier because more food was available at lower prices Allowed more people to move to cities which led to industrial revolution women needed less kids for farms Farming changed from family to commercial enterprise (agribusiness) that emphasized single crops and profits Vertical integration (contracts between farmer and purchaser) caused farm outputs to increase by 1990s 5.5 The Green Revolution # SPS 5 Agriculture has changed over time because of cultural diffusion and advances in technology.\nLearning Objective: Explain the consequences of the Green Revolution on food supply and the environment in the developing world. Essential Knowledge: The Green Revolution was characterized in agriculture by the use of high-yield seeds, increased use of chemicals, and mechanized farming Machines have replaced human labor New seeds, chemical pesticides, and fertilizers increased yield MDC often get newest tech first The Green Revolution had positive and negative consequences for both human populations and the environment. Started in mid-1970\u0026rsquo;s when scientists developed hybrid higher-yield seeds and new fertilizers to use alongside them. + New seeds and fertilizers diffused from core to periphery countries to help eradicate hunger + China, India, and SE Asia had rice harvests increase + Decrease land devoted to farms + Less expensive food + Reduce poverty + More consistent yield - Many poor farmers couldn\u0026rsquo;t afford new seeds - Africa couldn\u0026rsquo;t take advantage of seeds (there chief foods are millet, sorghum, yams, and cassavas - Increased irrigation, causing environmental damage - Focus on cash crops - Some soil has lost majority of nutrients due to over use - Biodiversity and native food crops have gone down, increased chance of blight 5.6 Agricultural Production Regions # PSO Availability of resources and cultural practices influence agricultural practices and land-use patterns.\nLearning Objective: Explain how economic forces influence agricultural practices Essential Knowledge: Agricultural production regions are defined by the extent to which they reflect subsistence or commercial practices (monocropping or monoculture). Subsistence vs Commercial: Subsistence is : Food grown for the farmer or farmer’s family/kin Food grown for local consumption for village/community market Food NOT grown for commercial purposes/sold for revenue Monocropping or Monoculture — cultivation of a single crop Occurs in mostly commercial farms in MDCs — US in 1950\u0026rsquo;s minimizes risks — climate, cost of inputs like labor or fertilizer, market demand, etc. maximize profits — choice of crop best suited for growing, potential pricing, etc. Supply and demands influences farmers to raise the crops that have high demands Governments disort market influence by subsidizing certain crops (rice in Japan, milk in US) Intensive and extensive farming practices are determined in part by land costs (bid-rent theory). bid-rent: geographical economic theory: refers to how the price and demand on real estate changes as the distance towards the Central Business District (CBD) increases. intensive agriculture: any kind of agriculture activity that involves effective and efficient use of labor on small plots of land to maximize crop yield extensive agriculture: an agricultural system characterized by low inputs of labor per unit land area 5.7 Spatial Organization of Agriculture # PSO Availability of resources and cultural practices influence agricultural practices and land-use patterns.\nLearning Objective: Explain how economic forces influence agricultural practices. Essential Knowledge: Large-scale commercial agricultural operations are replacing small family farms. Occured during 20th century due to larger profits Further notes Complex commodity chains link production and consumption of agricultural products. Von Thunen Technology has increased economies of scale in the agricultural sector and the carrying capacity of the land. More food leads to people spending less time trying to figure out how to eat and shifting to figuring out how to grow / be smarter Technology has lead to intensive farming that has optimized how we use land 5.8 Von Thünen (Thunen) Model # PSO Availability of resources and cultural practices influence agricultural practices and land-use patterns.\nLearning Objective: Describe how the von Thünen model is used to explain patterns of agricultural production at various scales Essential Knowledge: Von Thünen’s model helps to explain rural land use by emphasizing the importance of transportation costs associated with distance from the market; however, regions of specialty farming do not always conform to von Thünen’s concentric rings. The closer the land is to the city, the more expensive it Because perishable items, e.g. Milk, and difficult to transport items must be grown very closely to their market Milkshed is ring around market where dairy farming occurs Weberian theory Sort of not true with advances in transportation and that all land is able to support it\u0026rsquo;s designated task Forest resources (needed for fuel) could be grown and harvested further out than fruits/veggies from the market Market example with automobiles Grain could be harvested even further out because it could be grown, harvested, and stored easily and cheaply until needed Limitations Livestock then could be raised in the outer ring where cheap, larges pastures were pentiful 5.9 The Global System of Agriculture # PSO Availability of resources and cultural practices influence agricultural practices and land-use patterns.\nLearning Objective: Explain the interdependence among regions of agricultural production and consumption Essential Knowledge: Food and other agricultural products are part of a global supply chain. MDCs tend to have access to best tech and get far ahead compared to LDCs LDCs with land arable for certain foods may be funded by MDCs for a specific product that they (MDCs) can\u0026rsquo;t produce Some countries have become highly dependent on one or more export commodities. Country\u0026rsquo;s that have a monopoly on foods can exploit other countries that depend on them If there\u0026rsquo;s a pest+drought+other issue that stops production of producer country from making food, then all dependents are screwed too The main elements of global food distribution networks are affected by political relationships, infrastructure, and patterns of world trade. Tariffs can screw over trade agreements Country\u0026rsquo;s with more infrastructure are more likely to receive investment because its key to exporting goods Core-periphery model applies to agriculture 5.1 Consequences of Agricultural Practices # IMP Agricultural production and consumption patterns vary in different locations, presenting different environmental, social, economic, and cultural opportunities and challenges.\nLearning Objective: Explain how agricultural practices have environmental and societal consequences. Essential Knowledge: Environmental effects of agricultural land use include pollution, land cover change, desertification, soil salinization, and conservation efforts. pollution land cover change desertification overgrazing leads to land being perminent damage to land via erosion of unprotected topsoils soil salinization conservation efforts For rice farming For rice farming For mechanization / wheat farming Agricultural practices—including slash and burn, terraces, irrigation, deforestation, draining wetlands, shifting cultivation, and pastoral nomadism—alter the landscape. terraces creating an embankment (a terrance) at a right angle to sloping land in order to allow water to soak into the soil rather than move down the slope, taking the soil with it irrigation more efficient, developed to keep up with population demand deforestation plantation farming mainly specialize in 1-2 crops mostly in tropics, Latin America, Africa, some Asia Mostly produce prodcuts for sale is MDCs Intensive subsistence with wet rice dominant High agricultural density — lots of farmers, little land must produce enough food for family or small village wet rice: field prep: plow flooding: rain, river, or irrigation transplanting: rice seedling grow on dry land then moveod to flooded field to grow harvest: by hand double cropping: finishing 2 harvests in 1 year Intensive subsistence with wet rice NOT dominant climate prevents growing of rice in some regions where summer preciiptation is too low and/or winters are too harsh wheat, barley, millet, oats, corn, and some cash crops (cotton, flax, hemp) grown small land worked fulley commonly use crop rotation shifting cultivation based on growing crops in different fields on a rotating basis, e.g. Maya in the Yucatan grow maize by rotating fields (on a seven-year cycle) crops include rice, maize, manioc, millet, sorghum, yams, surgercane, veggies fields are cut and burned each year to enrich the soil with nutrients process called swidden or slash-and-burn seeds are planted in time for rainy season cultivated fields are used for two or three years until all nutrients are used critics it should be replaced by a more efficient means defenders say it is the most environmentally sound approach occupies 25% of world\u0026rsquo;s land, but practiced by just 5% of people used to be tradiationally done by village, now mostly private companies being replaced by logging, ranching, and cashcrops (monofarming) pastoral nomadism animals are herded in a seasonal migratory pattern 200+ million pastoralists in the world often cattle, goats, sheet, camels, and reindeer often in arid, marginal alnds — N Africa, Central Asia, Middle east when herds are moved from land to land, it\u0026rsquo;s called transhumance declining in popularity because modern tech can make better use of pastures mining, irrigation, petroleum Societal effects of agricultural practices include changing diets, role of women in agricultural production, and economic purpose. changing diets food security plays a large role in LDCs you can\u0026rsquo;t flourish if you\u0026rsquo;re physical and mental power is spent on getting food maps additional notes role of women in agricultural production economic purpose 5.1 Challenges of Contemporary Agriculture # IMP Agricultural production and consumption patterns vary in different locations, presenting different environmental, social, economic, and cultural opportunities and challenges.\nLearning Objective: Explain challenges and debates related to the changing nature of contemporary agriculture and food-production practices. Essential Knowledge: Agricultural innovations such as biotechnology, genetically modified organisms, and aquaculture have been accompanied by debates over sustainability, soil and water usage, reductions in biodiversity, and extensive fertilizer and pesticide use. biotechnology, genetically modified organisms, and aquaculture technological innovations have led to much higher yield debates over sustainability, soil and water usage, reductions in biodiversity, and extensive fertilizer and pesticide use pesticides kill off good bugs, possibly whipe out entire vital species by accident the majority of the latest tech is avaliable only the core countries, not periphery agricultural diversity has been on the decline, increased chance of blight New, or very old, disease comes back and, because of lack of genetic diversity, most crops highly susciptible Patterns of food production and consumption are influenced by movements relating to individual food choice, such as urban farming, community-supported agriculture (CSA), organic farming, value-added specialty crops, fair trade, local-food movements, and dietary shifts. urban farming community-supported agriculture (CSA) organic farming avoid using synthetic chemical fertilizers and GMOs aim to protect earth and produce safe, healthy food with \u0026ldquo;zero impact\u0026rdquo; on environment value-added specialty crops fair trade local-food movements dietary shifts Most typical citizen is an asian farmer who produces enough to get by MDCs people eat a lot more Cliamte affects what people eat MDCs this is less of an impact due to better tech making shipping faster LDCs can only really eat what is avaliable locally Cultural prefences still dictate diet \u0026lsquo;Fast-food\u0026rsquo; diet Meat consumption: MDCs get 1/3 of protein from meat LDCs get 1/10 of protein from meat, rely more on grains Challenges of feeding a global population include lack of food access, as in cases of food insecurity and food deserts; problems with distribution systems; adverse weather; and land use lost to suburbanization. food access (food insecurity and food deserts problems with distribution systems adverse weather land use lost to suburbanization The location of food-processing facilities and markets, economies of scale, distribution systems, and government policies all have economic effects on food-production practices. 5.1 Women in Agriculture # IMP Agricultural production and consumption patterns vary in different locations, presenting different environmental, social, economic, and cultural opportunities and challenges.\nLearning Objective: Explain geographic variations in female roles in food production and consumption. Essential Knowledge: The role of females in food production, distribution, and consumption varies in many places depending on the type of production involved. Historically, Men gathered the materials and women used these materials to manufacture household objects and maintained there house Obstacles to equality + empowerment Impact of empowerment effects of this women empowerment / equality on population growth effects of this women empowerment / equality on economic development effects of this women empowerment / equality on gender roles in the developing world. "},{"id":71,"href":"/ap/cmech/","title":"AP Physics C: Mechanics","section":"AP Notes","content":" This are very disjoint notes I took long ago. I would recommend using this for practice qs and perhaps equation review after you have a solid understanding of the chapters. My other notes are much more comprehensive, I swear! :) 1. Kinematics # Four Primary Equations # $$\\Delta x=\\frac{1}{2}(v_f-v_i)\\Delta t \\text{ \u0026ndash; no } a$$ $$v_f=v_i+a\\Delta t \\text{ \u0026ndash; no } x$$ $$\\Delta x=v_i \\Delta t+\\frac{1}{2}a \\Delta t^2 \\text{ \u0026ndash; no } v_f$$ $$\\Delta x=v_f \\Delta t-\\frac{1}{2}a \\Delta t^2 \\text{ \u0026ndash; no } v_i$$ $$v_f^2=v_i^2+2a \\Delta x \\text{ \u0026ndash; no } t$$ Used when .$a$cceration is constant Slope and Area # Top is .$x$, middle is .$v$, bottom is .$a$ Projectile Motion # Half of parabolic flight time: $$t_\\text{top}=\\frac{v_i*\\sin\\theta}{g}$$ Peak in .$y$ direction: $$y_\\text{max}=\\frac{v_i*\\sin^2\\theta}{2g}$$ Distance traveled in .$x$ direction: $$x_\\text{max}=\\frac{v_i*\\sin2\\theta}{g}$$ Desmos tools Projectile motion Displacement, Velocity, and Acceleration Practice # FRQs to study Graphs Changing Acceleration / Velocity Example Qs 1. 2. 3. 4. 5. 6. 7. 8. 2. Forces # .$N = \\text{kg m/s}^2$ \u0026ndash; a force of 1N causes a 1kg mass to accelerate at 1ms.$^{-2}$ Normal doesn\u0026rsquo;t always equal mg! Friction # Kinetic friction only acts when the force breaks past the static friction threshold The friction force is always the lesser of .$\\mu \\cdot N$ or the force it\u0026rsquo;s resisting $$F_s \\le \\mu_s \\cdot F_N$$ Spring # $$F_\\text{spring} = -kx$$ $$k=\\frac{\\Delta F}{\\Delta x}$$ Thus, the slope .$\\Delta y / \\Delta x$ of a force vs. distance is .$k$ Centripital # $$F_\\text{centripetal} = \\frac{mv^2}{r}$$\nGravity # $$mg\\sin(θ) = F_\\text{gx} \\text{ \u0026ndash; Acceleration down ramp w/no friction}$$ $$mg\\cos(θ) = F_\\text{gy} \\text{ \u0026ndash; Normal force when no other forces act in the y direction}$$\nPulleys + Atwoods # $$\\text{Acceleration of a Pulley} = \\frac{Mg}{m+M} = \\frac{Mg-\\mu mg}{m+M}$$\nDrag Force # Drag on x-axis: Drag on y-axis: Practice # FRQs to study Drag Multiple Bodies (pulleys, carts) Practice Qs 3. Energy # Work # $$W = \\int F\\ dx = \\vec F_\\parallel \\cdot x = +\\Delta KE = -\\Delta PE = \\int P dt$$\nForce parallel to distance traveled If force is opposing motion and acceleration changes, work stays the same Potential # $$F = -\\frac{dU}{dx}$$\nPotential Energy can only depend on position Negative relation with force indicates that the direction of the force is always towards lower PE Derivation Conservation # Consider the total work done by a force that acts on a particle as the particle moves around a closed path and returns to its starting point. If this total work is zero, we call the force a conservative force. If the total work for the round trip is not zero, we call the force a non-conservative force. Consider the work done by a force that acts on an object as the object moves from an initial position to a final position along any arbitrarily chosen path. If this work is the same for all such paths, we call the force a conservative force. If the work is not the same for all paths, we call the force a non-conservative force. $$\\Delta K + \\Delta U + \\Delta E_\\text{int}= W_\\text{ext}$$\nConservative — NO external forces Mechanical Energy conserved; ME = ME' Gravity, Spring Force Always have a potential energy associated with it Conservative force\u0026rsquo;s magnitude and direction only depend on the object\u0026rsquo;s location, not on how the object is moving Non-conservative — external force present Mechanical Energy lost; ME \u0026gt; ME' Friction, Air resistance (drag) Internal Energy Power # Rate at which work is done $$P=\\frac{dW}{dt}=\\frac{dKE}{dt}=\\frac{W}{t}=\\vec F \\cdot \\vec v$$\nSprings # $$W_\\text{spring}=\\int F_\\text{spring} dx= \\int (-kx)dx = -\\frac{1}{2}kx^2 $$ $$U_\\text{spring} = -W_\\text{spring}=\\frac{1}{2}kx^2$$\nSprings are most compressed in collisions when velocity of both objects are equal Therefore, we can treat the system as inelastic at that moment Steps Equilibrium # Neutral Equilibrium is where the Potential Energy of the object remains constant regardless of position. For example, a ball rolling on a level surface. Stable Equilibrium is where the Potential Energy of the object increases as the position of the object moves away from the equilibrium position and therefore the object naturally returns to the equilibrium position. For example, a water bottle being tipped to the side. Unstable Equilibrium is where the Potential Energy of the object decreases as the position of the object moves away from the equilibrium position and therefore the object naturally moves away from the equilibrium position. For example, a marker being tipped to the side. Practice # Practice Qs 4. Momentum # Collisions # Elastic — bounce off # KE conserved Momentum conserved (while no unbalanced ext forces) If the final velocity of an object is less than half of the initial velocity of the object (v_i/2), then the object it\u0026rsquo;s colliding with has more mass $$v_1+v_1\u0026rsquo;=v_2+v_2\u0026rsquo;$$ $$v_1\u0026rsquo;=\\frac{m_1-m_2}{m_1+m_2}v_1$$ $$v_2\u0026rsquo;=\\frac{2m_1}{m_1+m_2}v_2$$ Inelastic — Stick # KE lost Momentum conserved (while no unbalanced ext forces) Maximum speed when m \u0026laquo; M Impulse — Force and Time # $$\\vec J = \\int \\vec F dt = \\vec F_\\text{avg} \\Delta t= \\Delta \\vec p = m \\Delta \\vec v$$\nCenter of Mass # When only gravity is acting on a object that is thrown, it will spin (pivot) around the center of mass If you split an object along the center of mass line, both sides aren\u0026rsquo;t equal in mass unless density / form is the same for both. $$x_\\text{cm}=\\frac{\\Sigma(m_ix_i)}{\\Sigma(m)}=\\frac{\\int x \\lambda \\cdot dx}{\\Sigma M}$$ $$v_\\text{cm}=\\frac{\\Sigma(m_iv_i)}{\\Sigma(m)}$$ $$\\Sigma p=mv_\\text{cm}$$ $$\\Sigma F=ma_\\text{cm}$$ Practice # Practice Qs a. Integrate 0m to 4m 5. Rotation # Rotational Kinematics # Used when α is constant\n$$ \\Delta \\theta=\\frac{1}{2}(\\omega_f-\\omega_i)\\Delta t \\text{ \u0026ndash; no } \\alpha$$ $$ \\omega_f=\\omega_i+\\alpha\\Delta t \\text{ \u0026ndash; no } \\theta$$ $$\\ \\Delta \\theta=\\omega_i \\Delta t+\\frac{1}{2}\\alpha \\Delta t^2 \\text{ \u0026ndash; no } \\omega_f$$ $$\\ \\Delta \\theta=\\omega_f \\Delta t-\\frac{1}{2}\\alpha \\Delta t^2 \\text{ \u0026ndash; no } \\omega_i$$ $$\\omega_f^2=\\omega_i^2+2\\alpha \\Delta \\theta \\text{ \u0026ndash; no } t$$ Rotational Inertia # Pulleys with finishing with none+blue same time, then green, then red Rolling Down an Incline Rolling Down an Incline + Slipping Rolling Up an Incline Rolling Up an Incline Rolling Up an Incline + Slipping Should be Blue \u0026gt; Green \u0026gt; Red (0) Practice Qs # Practice Qs where green has lower moment of inertia, and red has larger moment of inertia Answer 9 10 11 12 13 "},{"id":72,"href":"/ap/stats/","title":"AP Statistics","section":"AP Notes","content":" The content of these notes are solid, but formatting is not since they\u0026rsquo;re exported from Notion. Unit 1: Exploring One-Variable Data # Types of Variables # Categorical variables assigns labels that place each individual into a particular group, called a category. Zip code. hair color Quantitative variables takes number values that are quantities—counts or measurements. Height, GPA Explanatory x on graph independent variable — what we\u0026rsquo;re changing measures an outcome of a study. Response y on graph (potentially) dependent variable — what we\u0026rsquo;re measuring may help predict or explain changes in a response variable Confounding Any factor that messes skews data Confounding occurs when two variables are associated in such a way that their effects on a response variable cannot be distinguished from each other. If you are asked to identify a possible confounding variable in a given setting, you are expected to explain how the variable you choose (1) is associated with the explanatory variable and (2) is associated with the response variable. Frequencies # A frequency table shows the number of individuals having each value. A relative frequency table shows the proportion or percent of individuals having each value. A marginal relative frequency gives the percent or proportion of individuals that have a specific value for one Categorical variable. What percent of people in the sample are environmental club members? What proportion of people in the sample never used a snowmobile? A joint relative frequency gives the percent or proportion of individuals that have a specific value for one Categorical variable and a specific value for another Categorical variable. We can compute marginal relative frequencies for the row totals to give the distribution of snowmobile use for all the individuals in the sample: We can compute marginal relative frequencies for the column totals to give the distribution of environmental club membership in the entire sample of 1526 park visitors A conditional relative frequency gives the percent or proportion of individuals that have a specific value for one Categorical variable among individuals who share the same value of another Categorical variable (the condition). What proportion of snowmobile renters in the sample are not environmental club members? What percent of environmental club members in the sample are snowmobile owners? Types of Graphs # Pie Chart — Categorical Need frequency value and corresponding label Doesn\u0026rsquo;t show sample size Bar Graph — Categorical Needs bar labels, axis names, units, vertical axis scale should start at 0 A side-by-side bar graph displays the distribution of a Categorical variable for each value of another Categorical variable. The bars are grouped together based on the values of one of the Categorical variables and placed side by side. A segmented bar graph displays the distribution of a Categorical variable as segments of a rectangle, with the area of each segment proportional to the percent of individuals in the corresponding category. Doesn\u0026rsquo;t show sample size, only proportions This can be fixed by using a mosaic plot which scales the width corresponding to size Dotplot — Quantitative A dot plot shows each data value as a dot above its location on a number line. Needs title, axis label, unit of measurement How to find percentile Percentile is the percent of people you\u0026rsquo;re better than, or percent of people that are worse than you Find how many points the decided point is ahead of, then divide by sample size The blue point is greater than 17 points, making it 17/20 —\u0026gt; in the 85% percentile Stemplot — Quantitative A stemplot shows each data value separated into two parts: a stem, which consists of all but the final digit, and a leaf, the final digit. The stems are ordered from lowest to highest and arranged in a vertical column. The leaves are arranged in increasing order out from the appropriate stems. Needs key and title Key should give context Key: 8|2 is a [context — student whose resting pulse rate] is 82 [beats per minute]\nHistogram — Quantitative A histogram shows each interval of values as a bar. The heights of the bars show the frequencies or relative frequencies of values in each interval. Needs title, axis label, unit of measurement Boxplot — Quantitative Describing Distributions (SOCS) + Context # Always be sure to include context when you are asked to describe a distribution. This means using the variable name, not just the units the variable is measured in. When comparing distributions of Quantitative data, it’s not enough just to list values for the center and variability of each distribution. You have to explicitly compare these values, using words like “greater than,” “less than,” or “about the same as.”\nShape (Skew) # A distribution is skewed to the right if the right side of the graph is much longer than the left side. A distribution is skewed to the left if the left side of the graph is much longer than the right side. The distribution of [context] is [skewed left/right/sym]\nOutlier # Gaps too Low outliers \u0026lt; Q1 − 1.5 × IQR High outliers \u0026gt; Q3 + 1.5 × IQR Doesn\u0026rsquo;t follow trend, large residual The [context — games played with 5 points / person with a height of 3\u0026rsquo;] appears to be an outlier\nCenter # Mean / average The mean is sensitive to extreme values in a distribution. These may be outliers, but a skewed distribution that has no outliers will also pull the mean toward its long tail. We say that the mean is not a resistant measure of center — Shouldn\u0026rsquo;t be used with skew or outliers Median Resistant — should be used with outliers and skew Spread / Variability # Range Not resistant — Shouldn\u0026rsquo;t be used with skew or outliers The data vary from [min] to [max] [context — points scored / heights] meaning it had a range of [max - min]\nStandard Deviation Measure of the typical distance of the values in a distribution from the mean. It should be used only when the mean is chosen as the measure of center. sx is not a resistant measure of variability — Shouldn\u0026rsquo;t be used with skew or outliers Larger values of sx indicate greater variation sx is always greater than or equal to The Interquartile Range (IQR) What The quartiles of a distribution divide the ordered data set into four groups having roughly the same number of values. To find the quartiles, arrange the data values from smallest to largest and find the median. The first quartile Q1 is the median of the data values that are to the left of the median in the ordered list. The third quartile Q3 is the median of the data values that are to the right of the median in the ordered list. IQR = Q3 - Q1 Resistant because they are not affected by a few extreme value — should be used with outliers Why is it important? # They might be inaccurate data values. Maybe someone recorded a value as 10.1 instead of 101. Perhaps a measuring device broke down. Or maybe someone gave a silly response, like the student in a class survey who claimed to study 30,000 minutes per night! Try to correct errors like these if possible. If you can’t, give summary statistics with and without the outlier. They can indicate a remarkable occurrence. For example, in a graph of net worth, Bill Gates is likely to be an outlier. They can heavily influence the values of some summary statistics, like the mean, range, and standard deviation. It can make it easier to see associations between variables An association exists when there is a difference in outcome for different inputs We can only find definitive associations for the sample, and we have to use test to find out if we can extrapolate this data to a larger sample For example, there may be an association between AP Stats students and not having post-HS plans and overall being less likely to go to University when compared to AP Calc students Five number summary (+ boxplot) # Min Q1 Median Q3 Max Standardized score (z-score) — the \u0026rsquo;test statistic' # Tells us how many standard deviations from the mean the value falls, and in what direction.\nValues larger than the mean have positive z-scores. Values smaller than the mean have negative z-scores.\nShape must be close to normal for z-scores to work\nNever say that a distribution of Quantitative data is Normal. Real-world data always show at least slight departures from a Normal distribution. The most you can say is that the distribution is “approximately Normal.” 68–95–99.7 Approximately 68% of the observations fall within σ of the mean μ Approximately 95% of the observations fall within σ 2 of the mean μ Approximately 99.7% of the observations fall within σ 3 of the mean μ Transforming Data # Multiplying / dividing by a constant (Units converted) Multiplies (divides) center and location (mean, five-number summary, percentiles) by b Multiplies (divides) measures of variability (range, IQR, standard deviation) by b Does not change the shape of the distribution Adding/subtracting constant Adds a to (subtracts a from) measures of center and location (mean, five-number summary, percentiles) Does not change measures of variability (range, IQR, standard deviation) Does not change the shape of the distribution Percentile # invNorm(.9, 0, 1) would find the z-score of 90% Good video Unit 2: Exploring Two-Variable Data # How to Describe a Scatterplot # Use CONTEXT Direction (association) Two variables have a positive association when above-average values of one variable tend to accompany above-average values of the other variable and when below-average values also tend to occur together. More [x-unit], more [y-unit] Two variables have a negative association when above-average values of one variable tend to accompany below-average values of the other variable. More [x-unit], less [y-unit] There is no association between two variables if knowing the value of one variable does not help us predict the value of the other variable. Form: # A scatterplot can show a linear form or a nonlinear form. The form is linear if the overall pattern follows a straight line. Otherwise, the form is nonlinear. Strength (correlation): # A scatterplot can show a weak, moderate, or strong association. An association is strong if the points don’t deviate much from the form identified. An association is weak if the points deviate quite a bit from the form identified. Correlation doesn’t imply causation. In many cases, two variables might have a strong correlation, but changes in one variable are very unlikely to cause changes in the other variable \u0026lsquo;r\u0026rsquo; — Correlation Coefficie It is only appropriate to use the correlation to describe strength and direction for a linear relationship Has same sign (positive or negative) as the slope r measures the direction and strength of the association, and does not measure form The correlation r is always a number between −1 and 1 (−1 ≤ r ≤ 1) The correlation r indicates the direction of a linear relationship by its sign: r \u0026gt; 0 for a positive association and r \u0026lt; 0 for a negative association. The extreme values r = −1 and r = 1 occur only in the case of a perfect linear relationship, when the points lie exactly along a straight line. If the linear relationship is strong, the correlation r will be close to 1 or −1. If the linear relationship is weak, the correlation r will be close to 0. Unusual features: # outliers that fall outside the overall pattern and distinct clusters of points — doesn\u0026rsquo;t follow trend large residual definition link influential point if you remove the point, then there would be substantial changes on slope, y-int, or r Interpreting # There is a [strength — fairly strong/weak], [direction — positive / negative] [form — (non)linear] relationship between [x var] and [y-var] with [any outliers + outlier point].\nResiduals # a = y-int, b = slope, s = s, R-sq = .$r^2$\nLeast-squares Regression Line (LSRL) # A regression line is a line that describes how a response variable y changes as an explanatory variable x changes. Made to reduce the residual Regression lines are expressed in the form where ŷ(pronounced “y-hat”) is the predicted value of y for a given value of x. Extrapolation is the use of a regression line for prediction far outside the interval of x values used to obtain the line. Such predictions are often not accurate. Don’t make predictions using values of x that are much larger or much smaller than those that actually appear in your data. Interpretation — NEEDS CONTEXT y-int y=b0+b1*x b0 For a [context of x-var] with a [x-unit] of 0, the predicted [y-var] is [b0]\nSlope b1 For every increase of 1 in [x-unit], the predicted [y-unit] increases by [b1] [y-units]\nResidual Difference between actual and predicted value Interpretation: The actual [y-var] for a [context] of [x-input \u0026amp; x-units] is [actual value (point we know) - predicted value] [lower/higher] than predicted by the LSRL.\nPlug in respective values to LSRL to get the predicted value Residual Plot a scatterplot that displays the residuals on the vertical axis and the explanatory variable on the horizontal axis. To determine whether the regression model is appropriate, look at the residual plot. If there is no leftover curved pattern in the residual plot, the regression model is appropriate. LSRL is good Interpretation: Because the residual plot does not show a clear pattern, the linear model is appropriate for the data\nIf there is a leftover curved pattern in the residual plot, consider using a regression model with a different form. LSRL is bad Interpretation: Because the residual plot shows a clear patter, the linear model is not appropriate for the data\nStandard deviation of residuals: .$s$ # The standard deviation of the residuals s measures the size of a typical residual. That is, s measures the typical distance between the actual y values and the predicted y values. Interpretation: The actual [y-var] [units] is typically around [s] away from the predicted by the least-squares regression line with x = [x-units]\nThe coefficient of determination: .$r^2$ # The coefficient of determination .$r^2$ measures the percent reduction in the sum of squared residuals when using the least-squares regression line to make predictions, rather than the mean value of y. In other words, .$r^2$ measures the percent of the variability in the response variable that is accounted for by the least-squares regression line. Interpretation: About [.$r^2$ in percent form]% of the variability for [y-var] is accounted for by the least-squares regression line with x = [x-var]\nUnit 3: Collecting Data # Chapter 4\nSampling # The population in a statistical study is the entire group of individuals we want information about. A census collects data from every individual in the population. A sample is a subset of individuals in the population from which we collect data. A sample survey is a study that collects data from a sample that is chosen to represent a specific population. Poor Sampling # Convenience sampling selects individuals from the population who are easy to reach. Voluntary response sampling allows people to choose to be in the sample by responding to a general invitation Bias: any difference between the sample result and the truth about the population that tends to occur in the same direction whenever you use the same sampling method The design of a statistical study shows bias if it is very likely to underestimate or very likely to overestimate the value you want to know. Bias is not just bad luck in one sample, it’s the result of a bad study design that will consistently miss the truth about the population in the same way If you’re asked to describe how the design of a sample survey leads to bias, you’re expected to do two things: Describe how the members of the sample might respond differently from the rest of the population Explain how this difference would lead to an underestimate or overestimate. Suppose you were asked to explain how using your statistics class as a sample to estimate the proportion of all high school students who own a graphing calculator could result in bias. You might respond, “This is a convenience sample . It would probably include a much higher proportion of students with a graphing calculator than in the population at large because a graphing calculator is required for the statistics class. So this method would probably lead to an overestimate of the actual population proportion.” Undercoverage occurs when some members of the population are less likely to be chosen or cannot be chosen in a sample. Most samples suffer from some degree of undercoverage. A sample survey of households, for example, will miss not only homeless people but also prison inmates and students in dormitories. Nonresponse Nonresponse occurs when an individual chosen for the sample can’t be contacted or refuses to participate Nonresponse leads to bias when the individuals who can’t be contacted or refuse to participate would respond differently from those who do participate. Consider a telephone survey that asks people how many hours of television they watch per day. People who are selected but are out of the house won’t be able to respond. Response Bias Response bias occurs when there is a systematic pattern of inaccurate answers to a survey question. The way questions are worded or the order in which they\u0026rsquo;re asked can lead to response bias Good Sampling # Simple random sample (SRS) Involves using a chance process to determine which members of a population are included in the sample . Gives each possible sample an equal chance of being selected A simple random sample (SRS) of size n is chosen in such a way that every group of n individuals in the population has an equal chance to be selected as the sample. For example, to choose a random sample of 6 students from a class of 30, start by writing each of the 30 names on a separate slip of paper, making sure the slips are all the same size. Then put the slips in a hat, mix them well, and pull out slips one at a time until you have identified 6 different students. Strata, Stratified random sample Strata are groups of individuals in a population who share characteristics thought to be associated with the variables being measured in a study. good for when sample sizes between population groups (stratas) are different Stratified random sampling selects a sample by choosing an SRS from each stratum and combining the SRSs into one overall sample. For example, in a study of sleep habits on school nights, the population of students in a large high school might be divided into freshman, sophomore, junior, and senior strata. After all, it is reasonable to think that freshmen have different sleep habits than seniors. The following activity illustrates the benefit of choosing appropriate strata. Clusters, Cluster sampling A cluster is a group of individuals in the population that are physically located near each other. Cluster sampling selects a sample by randomly choosing clusters and including each member of the selected clusters in the sample . Cluster sampling is often used for practical reasons, like saving time and money. Imagine a large high school that assigns students to homerooms alphabetically by last name, in groups of 25. Administrators want to survey 200 randomly selected students about a proposed schedule change. It would be difficult to track down an SRS of 200 students, so the administration opts for a cluster sample of homerooms. The principal (who knows some statistics) selects an SRS of 8 homerooms and gives the survey to all 25 students in each homeroom. Systematic Random Sampling In systematic random sampling, the researcher first randomly picks the first item or subject from the population . Then, the researcher will select each n\u0026rsquo;th subject from the list. The procedure involved in systematic random sampling is very easy and can be done manually. The results are representative of the population unless certain characteristics of the population are repeated for every n\u0026rsquo;th individual, which is highly unlikely. Experiments # Goal is to reduce bias and allow replication with the hopes that we find statistically significant results that we can infer/extrapolate to the population Four conditions of Experimental Design Comparison. Use a design that compares two or more treatments. Random assignment. Use chance to assign experimental units to treatments. Doing so helps create roughly equivalent groups of experimental units by balancing the effects of other variables among the treatment groups. Control. Keep other variables the same for all groups, especially variables that are likely to affect the response variable. Control helps avoid confounding and reduces variability in the response variable. Replication. (more than one experimental unit in each treatment group) — Use enough experimental units in each group so that any differences in the effects of the treatments can be distinguished from chance differences between the groups. Study: Observational Study: observes individuals and measures variables of interest but does not attempt to influence the responses. Observational Study vs experiment experiment (randomly) assigns treatments in studies the researcher has no interaction/input whatsoever data is observed and recorded naturally, scientists had no say can reduce bias from scientists no random assignment of subjects, but random sample can be taken therefore, we can make inferences about the population from which the individuals were chose, but not about cause and effect ( link) Vocab Experiment: deliberately imposes some treatment on individuals to measure their responses Response / Explanatory / Confounding Variables Placebo: A placebo is a treatment that has no active ingredient, but is otherwise like other treatments. The placebo effect describes the fact that some subjects in an experiment will respond favorably to any treatment, even an inactive treatment. Control: In an experiment, control means keeping other variables constant for all experimental units. Treatment: A specific condition applied to the individuals in an experiment Control group In an experiment, a control group is used to provide a baseline for comparing the effects of other treatments. Depending on the purpose of the experiment, a control group may be given an inactive treatment (placebo), an active treatment, or no treatment at all. Experimental unit: the object to which a treatment is randomly assigned Subjects: When the experimental units are human beings, they are often called subjects. Factor: In an experiment, a factor is a variable that is manipulated and may cause a change in the response variable. Levels: In an experiment, a factor is a variable that is manipulated and may cause a change in the response variable. The different values of a factor are called levels. Blinds In a double-blind experiment, neither the subjects nor those who interact with them and measure the response variable know which treatment a subject received. In a single-blind experiment, either the subjects don’t know which treatment they are receiving or the people who interact with them and measure the response variable don’t know which subjects are receiving which treatment Replication In an experiment, replication means using enough experimental units to distinguish a difference in the effects of the treatments from chance variation due to the random assignment. Sampling Variability Refers to the fact that different random samples of the same size from the same population produce different estimates. Larger random samples tend to produce estimates that are closer to the true population value than smaller random samples. In other words, estimates from larger samples are more precise. Statistically significant When the observed results of a study are too unusual to be explained by chance alone, the results are called statistically significant. Types of Experiments Block, Randomized block design A block is a group of experimental units that are known before the experiment to be similar in some way that is expected to affect the response to the treatments. In a randomized block design, the random assignment of experimental units to treatments is carried out separately within each block. Matched Pairs A matched pairs design is a common experimental design for comparing two treatments that uses blocks of size 2. In some matched pairs designs, two very similar experimental units are paired and the two treatments are randomly assigned within each pair. In others, each experimental unit receives both treatments in a random order. Random Assignment: In an experiment, random assignment means that the treatment / placebo is randomly given out Scope of Inference Inference: The process of drawing conclusions about a population based on samples, since we infer information about the population from what we know about the samples. Random Selection — sample units are selected randomly allows inference about the population from which the individuals were chosen groups may not be representative of population includes studies Random Assignment — experimental units are assigned to treatments using a chance process. allows inference about cause and effect. groups may differ between studies if not randomly assigned tend to average out all other uncontrollable factors so that they aren\u0026rsquo;t confounding with the treatment effects not studies Additional Requirements: The association is strong. The association is consistent. Larger values of the explanatory variable are associated w/ stronger responses. The alleged cause precedes the effect in time. Criteria for Establishing Causation When an Experiment CANNOT Be Done ● The association is strong. ● The association is consistent. ● Larger values of the explanatory variable are associated w/ stronger responses. ● The alleged cause precedes the effect in time. ● The alleged cause is plausible. Ethics All planned studies must be reviewed in advance by an institutional review board charged with protecting the safety and well-being of the subjects. All individuals who are subjects in a study must give their informed consent before data are collected. All individual data must be kept confidential. Only statistical summaries for groups of subjects may be made public. Unit 4: Probability, Random Variables, and Probability Distributions # Chapters 5 \u0026amp; 6\nProbability # LAW OF LARGE NUMBERS A law that states if we observe more and more repetitions of any chance process, the proportion of times a specific outcome occurs approaches its actual probability. We cannot accurately predict outcomes in the SHORT RUN; Order only emerges in the LONG RUN. No such thing as Law of Averages – Only Law of Large Numbers!!!!! Probibility The probability of any outcome of a chance process is a number between 0 and 1 that describes the proportion of times the outcome would occur in A VERY LONG SERIES OF REPETITIONS. Outcomes that will never ever occur Probability = 0 Outcomes that will always occur Probability = 1 Simulation The imitation of chance behavior, based on a model that accurately reflects the situation. The Simulation Process Describe how to use a chance device to imitate one trial (repetition) of the simulation. Tell what you will record at the end of each trial. Remember that every label needs to be the same length. In the golden ticket lottery example, the labels should be 01 to 95 (all two digits), not 1 to 95. When sampling without replacement, be sure to mention that repeated numbers should be ignored. Perform many trials of the simulation. Use the results of your simulation to answer the question of interest. Example Probability Model: A description of some chance process that consists of 2 parts: a list of all possible outcomes SAMPLE SPACE: The list of all possible outcomes. the probability for each outcome EVENT Any collection of outcomes from some chance process — A subset of the entire sample space. MUTUALLY EXCLUSIVE Two events A \u0026amp; B are mutually exclusive if they have no outcomes in common and so can never occur together – that is, if P(A and B) = 0. General Rules $$0 ≤ P(A) ≤ 1 \\text{ for any event }A$$ $$P(S) = 1 \\text{ if }S\\text{ is the sample space in a probability model}$$ $$P(A)=\\frac{\\text{Number of outcomes in event } A}{\\text{Total number of outcomes in sample space}}\\text{ in the case of EQUALLY LIKELY outcomes,}$$ $$\\text{Complement Rule: } P(A^c)=1-P(A); \\text{where }A^c=\\text{the event that A does not occur}$$ $$\\text{Addition rule for mutually exclusive events: }P(A \\text{ or } B)=P(A \\cup B) = P(A) + P(B)$$ $$\\text{General addition rule: } P(A \\text{ or } B) =P(A \\cup B)= P(A) + P(B) − P(A \\text{ and } B) $$ $$\\text{Dependent events: } P(A \\text{ and } B) = P(A ∩ B) = P(A) ⋅ P(B | A)$$ $$\\text{Independent events: } P(A \\text{ and } B) = P(A ∩ B) = P(A) ⋅ P(B)$$ $$\\text{Probability of A given B: } P(A|B)=\\frac{P(A \\text{ and } B)}{P(B)} $$ Conditional CONDITIONAL PROBABILITY Example The probability that one event happens given that another event is known to have happened. The conditional probability that event B happens GIVEN that event A has happened is denoted by P(B | A). INDEPENDENT EVENTS A and B are independent events if knowing whether or not one event has occurred does not change the probability that the other event will happen. In other words, events A and B are independent if $$P(A ∣ B) = P(A ∣ B ^c ) = P(A)$$ Alternatively, events A and B are independent if $$P(B ∣ A) = P(B ∣ A^c) = P(B)$$ Random Variables # DISCRETE RANDOM VARIABLE A random variable that has a countable number of outcomes with gaps. Examples: ACT Scores; # of Free-Throws Made, etc. CONTINUOUS RANDOM VARIABLE A random variable that has an infinite number of outcomes with no gaps. Examples: Temperature; Race Times; Heart Rate, etc. Mean Variance Standard Deviation Transforming — only works for independent! When MULTIPLYING (or DIVIDING) each value in a probability distribution by some number b, the ● mean is MULTIPLIED (or DIVIDED) by b ● variance is MULTIPLIED (or DIVIDED) by b^2 ● standard deviation is MULTIPLIED (or DIVIDED) by b When ADDING (or SUBTRACTING) the number a to each value in a probability distribution, the ● mean INCREASES (or DECREASES) by a ● variance STAYS THE SAME ● standard deviation STAYS THE SAME Combining — only works for independent! Suppose we add two normal distributions (X + Y) or we subtract two normal distributions (X – Y). The shape of the resulting distribution will be NORMAL and the mean and standard deviation can be calculated using the RULES. where \\row (no ^2) is standard deviation Difference between the binomial setting and the geometric setting Binomial Binomial Setting: Binary? Each observation falls into 1 of 2 categories: SUCCESS or FAILURE Independent? The n observations are all INDEPENDENT. (knowing one outcome of a trial has no effect on the other trials) Note: if sampling w/o replacement, you need to check the 10% condition Number? There is a fixed number n of TRIALS/OBSERVATIONS. Success? The probability of success, p, is SAME for each trial. BINOMIAL DISTRIBUTION If large counts is verified, then the binomial distribution\u0026rsquo;s shape is approximately normal The distribution of the count X of successes in the binomial setting with parameters n and p. n = # of Trials/Observations p = Probability of Success (per Trial) Possible Values for X = whole #s 0 to n Abbreviation = B (n, p) BINOMIAL COEFFICIENT The number of ways to arrange k successes among n trials; “Combinations” Binomial Probability Formula binomialPdf / Cdf also works Geometric: The Geometric Setting Binary? Each observation falls into 1 of 2 categories: SUCCESS or FAILURE Independent? The n observations are all INDEPENDENT. (knowing one outcome of a trial has no effect on the other trials). Trials? The goal is to count the number of trials until the FIRST SUCCESS Success? The probability of success, p, is SAME for each trial. Shape skewed right GEOMETRIC PROBABILITY FORMULA geometCdf / Pdf also works Unit 5: Sampling Distributions # Chapter 7 + 10\nSampling Distribution: # The sampling distribution of a statistic is the distribution of values taken by the statistic in all possible samples of the same size from the same population. Always say “the distribution of [blank],” being careful to distinguish the distribution of the population, the distribution of sample data, and the sampling distribution of a statistic. Sampling distribution of the sample proportion The sampling distribution of the sample proportion p hat describes the distribution of values taken by the sample proportion p hat in all possible samples of the same size from the same population. Conditions Random: The data must come from a well-designed RANDOM sample or a RANDOMIZED experiment. Normal: The sampling distribution is approximately NORMAL, meaning we can use a z-test statistic. Large Counts: (np ≥ 10) \u0026amp; (n (1 − p) ≥ 10 ) Independent: If 2 samples, both samples must be independent 10% Condition: When sampling w/o replacement to verify the use of our standard deviation ( 10n \u0026lt; N ) Sampling distribution of the sample mean The sampling distribution of the sample mean x describes the distribution of values taken by the sample mean x in all possible samples of the same size from the same population. Conditions Random: The data must come from a well-designed RANDOM sample or a RANDOMIZED experiment. Normal: The sampling distribution is approximately NORMAL, meaning we can use a z-test/t-test statistic. Either (or both) condition(s) must be met: CLT n ≥ 30 — n_diff ≥ 30 for paired data The central limit theorem (CLT) says that when n is larger than 30, the sampling distribution of the sample mean x is approximately Normal. Distribution shouldn\u0026rsquo;t be skewed values above the median are much more variable than the values below the median Independent: If 2 samples, both samples must be independent 10% Condition: When sampling w/o replacement to verify the use of our standard deviation ( 10n \u0026lt; N ) — 10*n_diff \u0026lt; N_diff for paired data Sampling Variability: Sampling variability refers to the fact that different random samples of the same size from the same population produce different values for a statistic. Parameter: A number computed from a population Statistic A number computed from a SAMPLE What makes a statistic a good estimator of a parameter? LOW BIAS (Randomization) High bias is usually because of a poor sampling design (lack of randomness). LOW VARIABILITY (Sample Size) Reduce the variability of a statistic is to INCREASING SAMPLE SIZE. Difference between proportions Difference between means Many students use “accurate” when they really mean “precise.” For example, a response that says “increasing the sample size will make an estimate more accurate” is incorrect Paired Data # Paired data result from recording two values of the same Quantitative variable for each individual or for each pair of similar individuals. 2 sets of data that are not independent from each other, then they\u0026rsquo;re paired Analyzing Paired Data: To analyze paired data, start by computing the difference for each pair. Then make a graph of the differences. Use the mean difference x and the standard deviation of the differences as summary statistic. Confidence Intervals # Point estimator: a statistic that provides an estimate of a population parameter. The value of that statistic from a sample is called a point estimate. Ideally our “best guess” at the value of an unknown parameter. Because the sample mean ¯x is an unbiased estimator of the population mean μ, we use the statistic ¯x as a point estimator ****of the parameter μ. The best guess for the value of μ is ¯x Unbiased Estimator: A statistic used to estimate a parameter is an unbiased estimator if the mean of its sampling distribution is equal to the value of the parameter being estimated. Confidence Level α The confidence level C% gives the overall success rate of the method used to calculate the confidence interval. Interpretation If we were to select many random samples from a population and construct a C% confidence interval using each sample, about C% of the intervals would capture the [parameter in context].\nConfidence interval When interpreting a confidence interval, make sure that you are describing the parameter and not the statistic. Interpretation “We are C% confident that the interval from [min] to [max] captures the true value of the [parameter in context].”\nMargin of error describes how far, at most, we expect the estimate to vary from the true population value. In a C% confidence interval, the distance between the point estimate and the true parameter value will be less than the margin of error in C% of all samples. How to decrease MOE The confidence level decreases. To obtain a smaller margin of error from the same data, you must be willing to accept less confidence. The sample size n increases. In general, increasing the sample size n reduces the margin of error for any fixed confidence level. Margin of error accounts for only the variability we expect from random sampling. It does not account for practical difficulties, such as undercoverage and nonresponse in a sample survey Critical Value The critical value is a multiplier that makes the interval wide enough to have the stated capture rate. Z-score when we know stdiv t-score when we don\u0026rsquo;t know stdiv Standard Error (SE) Proportions # Four-step process: State What parameter do you want to estimate and at what confidence level? 1 proportion \u0026ldquo;We wish to estimate the true proportion of all [parameter], with [C%] confidence.\u0026rdquo;\n2 proportions \u0026ldquo;We wish to estimate the true difference of the proportion of all [parameter 1] and all [parameter 2] for [context], with [C%] confidence.\u0026rdquo;\nPlan Identify the appropriate inference method \u0026ldquo;We\u0026rsquo;ll carry out a [ONE/TWO]-SAMPLE Z INTERVAL FOR A POPULATION PROPORTION (because we\u0026rsquo;re estimating a proportion and we know the standard deviation.)\u0026rdquo;\nCheck conditions Do Perform the calculations. \u0026ldquo;Because the conditions are true, we can do our calculations:\u0026rdquo; 1 proportion 2 proportions where z* is the critical value for the standard Normal curve with C% of its area between −z* and z*. Conclude Interpret your interval in the context of the problem. We are [C%] confident that the interval of [min] to [max] [units] captures the true proportion of all [context]\n2 sample difference If 0 is within confidence interval range: Because 0 is contained in the [C%] confidence interval it is plausible there is no difference between [parameter 1] and [parameter 2]. We do not have convincing evidence of a difference of proportions of [context]\nIf 0 is NOT within confidence interval range: Because 0 is not contained in the [C%] confidence interval it is plausible there is a difference between [parameter 1] and [parameter 2]. We have convincing evidence of a difference of proportions of [context]\nMeans # Four-step process State What parameter do you want to estimate and at what confidence level? 1 proportion We wish to estimate the true mean of all [parameter], with [C%] confidence.\n2 proportions We wish to estimate the true difference of the mean of all [parameter 1] and all [parameter 2] for [context], with [C%] confidence.\nPlan Identify the appropriate inference method If we know stdiv We\u0026rsquo;ll carry out a [ONE/TWO]-SAMPLE Z INTERVAL FOR A POPULATION MEAN(because we know the standard deviation.)\nIf we don\u0026rsquo;t know stdiv We\u0026rsquo;ll carry out a [ONE/TWO]-SAMPLE Z INTERVAL FOR A POPULATION MEAN(because and we don\u0026rsquo;t know the standard deviation.)\nCheck conditions Do Perform the calculations. z-score Because we know conditions are true, can do our calculations 1 mean 2 means with z* score and \\row t-score Because we know conditions are true, we can say that the t-distribution\u0026rsquo;s degree of freedom (df) is df = n - 1 and we\u0026rsquo;ll carry out a [one/two]-sample t interval for a population mean 1 mean 2 means Difference Conclude Interpret your interval in the context of the problem. We are [C%] confident that the interval of [min] to [max] [units] captures the true mean of all [context]\n2 sample difference If 0 is within confidence interval range: Because 0 is contained in the [C%] confidence interval it is plausible there is no difference between [parameter 1] and [parameter 2]. We do not have convincing evidence of a difference between means of [context]\nIf 0 is NOT within confidence interval range: Because 0 is not contained in the [C%] confidence interval it is plausible there is a difference between [parameter 1] and [parameter 2]. We have convincing evidence of a difference between means of [context]\nt-scores A t-distribution is specified by its degrees of freedom (df) calculated df = n - 1 The spread of the t-distribution is MORE than that of a standard Normal distribution. The t-distribution has MORE probability in the tails than the standard Normal distribution, since substituting the estimate sx for the parameter σ introduces MORE variation into the statistic. As degrees of freedom increases, the t-distribution becomes CLOSER to the standard Normal distribution, since sx estimates MORE accurately when the sample size is large. Choosing sample size # Sometimes, we want to choose our sample size (n) so that we may estimate a proportion within a particular margin of error. We must choose our sample size before we start sampling. Conservative Approach: Use p .5 , because it maximizes the margin of error. Better Approach if Possible: Make a guess about the value of p based on prior knowledge, common knowledge, previous studies, etc. $$Z^*\\sqrt{\\frac{p(1-p)}{n}} \\leq ME, \\text{and solve for n}$$ Sample Size for a Desired Margin of Error when Estimating μ To determine the sample size n that will yield a C% confidence interval for a population mean with a specified margin of error ME: Get a reasonable value for the population standard deviation σ from an earlier or pilot study. Find the critical value z* from a standard Normal curve for confidence level C%. Set the expression for the margin of error to be less than or equal to ME and solve for n: Tests # Shared Vocab # Significance Test A formal procedure for comparing OBSERVED DATA with a CLAIM whose truth we want to assess. Null hypothesis — H0 A test is designed to assess the strength of the evidence AGAINST this. This hypothesis is often the statement of “no difference.” Alternative hypothesis — Ha The claim about the population we are trying to find evidence FOR. This hypothesis should always be created BEFORE seeing the data. One sided less than or greater than Two sided not equal Both Null \u0026amp; Alternative Hypotheses refer to a POPULATION and use PARAMETERS (μ \u0026amp; p) p-value Assuming H0 is true, the probability the statistic (such as p^hat or x^bar) would take a value as extreme or more extreme than the one actually observed. The smaller the p-value, the STRONGER the evidence is AGAINST the H0 Interpretation \u0026ldquo;Assuming H0 is true, there is a [P-val] probability of getting the [sample val] [or even smaller/larger] by random chance with a sample size of [n]\u0026rdquo;\nsignificance level The level at which that, when our p-value falls below it, we consider it to be SIGNIFICANT We consider that our sample is so unlikely to happen IF H0 is true, that it likely did NOT happen by chance error + power Type 1: When H0 is true, but we REJECT H0 P(Type I) = confidence level α Type 2: When Ha is true, but we FAIL TO REJECT H0 P(Type II) = 1 - Power Power The ability of a test to correctly detect the alternative when it\u0026rsquo;s true. When Ha is true, and we CORRECTLY REJECT H0 Interpretation \u0026ldquo;Given Ha is true (in context), there is a (power) probability we correctly rejecting H0 (finding convincing evidence for Ha)\nPower = 1 – P(Type II Error) How to increase INCREASE the sample size (n), INCREASE the Confidence Level (α), or Make Ha further away from H0 STANDARDIZED TEST STATISTIC A standardized test statistic measures how far a sample statistic is from what we would expect if the null hypothesis H0 were true, in standard deviation units. Proportions # Four-step process State State your Hypotheses: H[relation] = [p/µ] interpret values — 1 proportion where p is the true proportion of [context] and Significance Level (alpha)\ninterpret values — 2 proportion where p is the true difference between the proportions of [parameter 1] and [parameter 2] of [context] and Significance Level (alpha)\nPlan Identify the appropriate testing method We\u0026rsquo;ll carry out a [ONE/TWO]-SAMPLE Z TEST FOR A POPULATION PROPORTION (because we\u0026rsquo;re estimating a proportion and we know the standard deviation)\nCheck conditions Do Perform the calculations. Because we know conditions are true, can do our calculations 1 sample 2 sample Proportion Difference find p-value Conclude Interpret your p-value in the context of the problem. P-value less than Assuming the [H0] is true, there is a [p-value] probability of getting [statistic found] or [more [and/neither] less — more for H0 \u0026lt; Ha; less for Ha \u0026lt; H0; both for H0 ≠ Ha] in a sample of [sample size] purely by chance. Because [p-value] is less than [alpha] of [confidence level], we have evidence to reject the null hypothesis, and have some evidence that the null hypothesis may be true, meaning [context]\nP-value greater than Assuming the [H0] is true, there is a [p-value] probability of getting [statistic found] or [more [and/neither] less — more for H0 \u0026lt; Ha; less for Ha \u0026lt; H0; both for H0 ≠ Ha] in a sample of [sample size] purely by chance. Because [p-value] is greater than [alpha] of [confidence level], we do not have enough evidence to reject the null hypothesis, meaning [context] Means # Four-step process State State your Hypotheses: H[relation] = [p/µ] interpret values — 1 proportion where p is the true mean of [context] and Significance Level (α)\ninterpret values — 2 proportion where p is the true difference between the means of [parameter 1] and [parameter 2] of [context] and Significance Level (α)\nPlan Identify the appropriate testing method We\u0026rsquo;ll carry out a [[ONE/TWO]-SAMPLE]/MATCHED PAIRS] T TEST FOR A POPULATION MEAN\nCheck conditions Do Perform the calculations. Because we know conditions are true, can do our calculations with (n-1) degrees of freedom 1 sample 2 sample Mean Difference Paired Paired find P-value Conclude Interpret your p-value in the context of the problem. P-value less than Assuming the parameter is true, there is a [p-value] probability of getting [statistic found] or more/less in a sample of [sample size] purely by chance. Because [p-value] is less than [alpha] of [confidence level], we have evidence to reject the null hypothesis, and have some evidence that the null hypothesis may be true, meaning [context]\nP-value greater than Assuming the parameter is true, there is a [p-value] probability of getting [statistic found] or more/less in a sample of [sample size] purely by chance. Because [p-value] is greater than [alpha] of [confidence level], we do not have enough evidence to reject the null hypothesis, meaning [context]\n"}]