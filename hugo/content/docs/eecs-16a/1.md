---
title: "Week 1"
weight: 2
---

# 1-25: Gaussian Elimination, Vectors

- [Slides](https://eecs16a.org/lecture/Lecture1A_Slides.pdf)
- Notes [2A](https://eecs16a.org/lecture/Note2A.pdf), [2B](https://eecs16a.org/lecture/Note2B.pdf)

---

## Upper Triangular Systems

{{< columns >}}<!-- mathjax fix -->
- Consider the following equation
$$x-y+2z=1$$
$$y-z=2$$
$$z=1$$
<---><!-- mathjax fix -->
...which can be represent as
$$\begin{bmatrix} 
    1 & -1 & 2 & \text{|} & 1\\\ 
    0 & 1 & -1 & \text{|} & 2\\\ 
    0 & 0 & 1 & \text{|} & 1
\end{bmatrix}$$
{{< /columns >}}
- These are called [upper triangle matrices](https://en.wikipedia.org/wiki/Triangular_matrix) -- they are nice in that they're easy to solve!
    - The solution is reached when the diagonal is all one, the remaining is zero (excluding the rightmost 'answer' colum)
- More precisely, a matrix is in [row echelon form](https://en.wikipedia.org/wiki/Row_echelon_form) when the following criteria are met:
{{< columns >}}<!-- mathjax fix -->
* All nonzero rows are above all zero rows.
* The leading coefficient of a non-zero row is always to the right of the leading coefficient of the row above it.
<---><!-- mathjax fix -->
$$\begin{bmatrix} 
    1 & * & * & * & \text{|} & *\\\ 
    0 & 1 & * & * & \text{|} & *\\\ 
    0 & 0 & 0 & 1 & \text{|} & *\\\
    0 & 0 & 0 & 0 & \text{|} & 0\\\
\end{bmatrix}$$
{{< /columns >}}
    * The leading coefficient of every non-zero row (which we call the pivot, and say is in the pivot position) is 1.
       - Some textbooks will require this third property, others don't
- In addition to row echelon form, there is also reduced row echelon form. 
    - This requires that: In addition, after the upwards propagation of variables in step (3), we will obtain a matrix with the following properties, in addition to the two mentioned above:
{{< columns >}}<!-- mathjax fix -->
1. The matrix is in row echelon form.
2. The leading coefficient of every non-zero row (which we call the pivot, and say is in the pivot position) is 1.
3. Each column with an element that is in the pivot position of some row has 0s everywhere else.
<---><!-- mathjax fix -->
$$\begin{bmatrix} 
    1 & 0 & * & 0 & \text{|} & *\\\ 
    0 & 1 & * & 0 & \text{|} & *\\\ 
    0 & 0 & 0 & 1 & \text{|} & *\\\
    0 & 0 & 0 & 0 & \text{|} & 0\\\
\end{bmatrix}$$
{{< /columns >}}
    - Sometimes abbreviated (especially in programming) as `rref`.
    - By construction, the Gaussian elimination algorithm always results in a matrix that is in reduced row echelon form. 
        - Once an augmented matrix is reduced to reduced row echelon form, variables corresponding to columns containing leading entries are called **basic variables**, and the remaining variables are called **free variables**
    - As will be discussed shortly, the distinction between basic and free variables allows us to characterize all solutions to the system of linear equations (if any exist!).


{{< expand "Example" >}}<!-- mathjax fix -->
{{< columns >}}<!-- mathjax fix -->
We start with the following system:
$$x-y+2z=1$$
$$2x+y+z=8$$
$$-4x+5y = 7$$
<---><!-- mathjax fix -->
...which we can write as a matrix
$$\begin{bmatrix} 
    1 & -1 & 2 & \text{|} & 1\\\ 
    2 & 1 & 1 & \text{|} & 8\\\ 
    -4 & 5 & 0 & \text{|} & 7
\end{bmatrix}$$
{{< /columns >}}
...and we can **row-reduce** to upper triangle (Row echelon)
<!-- - add .$-2 (1)$ to row two and .$4 (1)$ to row three 
$$\begin{bmatrix} 
    1 & -1 & 2 & \text{|} & 1\\\ 
    2-(2\cdot 1) & 1-(2\cdot -1) & 1-(2\cdot 2) & \text{|} & 8-(2\cdot 1)\\\ 
    0 & 1 & 8 & \text{|} & 11
\end{bmatrix}$$-->
{{< figure  src="/docs/eecs-16a/1/tri.png" >}}
...which we can use [**back substitution**](https://en.wikipedia.org/wiki/Triangular_matrix#Forward_and_back_substitution) to solve
{{< figure  src="/docs/eecs-16a/1/back.png" >}}
{{< /expand >}}

{{< expand "Tomograph Example" >}}
{{< figure  src="/docs/eecs-16a/1/tom1.png" >}}
{{< figure  src="/docs/eecs-16a/1/tom2.png" >}}
{{< figure  src="/docs/eecs-16a/1/tom3.png" >}}
{{< /expand >}}
- In real systems, we will always have noise (error) that makes our systems slightly skewed
    - So what if we repeat the example above, but have a measurement of .$+0.1$... are there any solutions?
    {{< expand "Error Example" >}}
{{< figure  src="/docs/eecs-16a/1/err.png" >}}
    {{< /expand >}}
- We can represent our solution as a set of linear equations, meaning we can represent them graphically
    ![](/docs/eecs-16a/1/single.png)
    ![](/docs/eecs-16a/1/no-sol.png)
    ![](/docs/eecs-16a/1/inf.png)



# 1-27: Vectors, Matrices, Multiplications, And Span

- [Slides](https://eecs16a.org/lecture/Lecture1B_Slides.pdf)
- [Notes 3](https://eecs16a.org/lecture/Note3.pdf)

---

## Vectors

- Given a collection of .$n$ real numbers such as .$x_1, x_2, \dots x_n$, we can represent this collection as a single point in an .$n$-dimensional _real space_ .$\mathbb{R}^n$, denoted as a .$\vec x$:
    $$\vec x = $$
    <!-- todo figure out latex -->
    - Each .$x_i$ (for .$i$ between .$1$ and .$n$) is called a **component**, or element, of the vector.
    - The **size** of a vector is the number of components it contains
    - Two vectors .$\vec x$ and .$\vec y$ are said to be **equal**, .$\vec x = \vec y$, if they have the same size and .$x_i = y_i$ for all .$i$
    - Vectors are interesting because they can represent any set of numbers
        - Representing as vectors lets us apply a lot of operations to them
        - E.x. color (RGB values), pictures (set of pixels), [solar cycles](https://en.wikipedia.org/wiki/Solar_cycle)


### Special vectors

{{< columns >}}<!-- mathjax fix -->
$$\vec 0 = $$
<---><!-- mathjax fix -->
$$\vec 1 = $$
<---><!-- mathjax fix -->
$$\vec e_1 = $$
<---><!-- mathjax fix -->
$$\vec e_2 = $$
<---><!-- mathjax fix -->
$$\vec e_n = $$
{{< /columns >}}
<!-- todo latex^ -->


### Vector Operations

#### Addition
- Must be same size and space (e.g. complex numbers, real numbers, etc.)
- Properties:
    - Many of the properties of addition you are already familiar with when adding individual numbers hold for vector addition as well. For three vectors .$\vec x, \vec y, \vec z \in \mathbb{R}^n$ (and .$\vec 0 \in \mathbb{R}^n$), the following properties hold:
    {{< columns >}}<!-- mathjax fix -->
Commutativity: .$\vec x + \vec y = \vec y + vec x$
<---><!-- mathjax fix -->
Associativity: .$(\vec x + \vec y) + \vec z = \vec x + (\vec y + \vec z)$$ 
<---><!-- mathjax fix -->
Additive identity: .$\vec x + 0 = \vec x$
<---><!-- mathjax fix -->
Additive inverse: .$\vec x + (- \vec x) = 0$
{{< /columns >}}
        
#### Scalar Multiplication


#### Vector Transpose
- .$\vec x$ is always a column vector
- To convert (represent) a row vector, we apply the transpose: .$\vec x^T$
- If the elements of the matrix .$A \in \mathbb{R}^{N \times M}$ are .$a_{ij}$
- The elements of .$A^T \in \mathbb{R}^{M \times N}$ are .$a_{ji}$
- Matrix transpose is not (generally) an inverse!


#### Vector-Vector Multiplication

- Multiplication is valid only for specific matching dimensions!
    - Width of the first, matches length of the second's transpose
    - e.x. given .$\vec x, \vec y \in \mathbb{R}^{N\times 1}$
        - We can take the transpose of .$y$ and multiply by .$\vec x$:
            $$\vec y^T \vec x = y_1 x_1 + y_2 x_2 + \dots + y_N x_N = \text{some scalar} \in \mathbb{R}^{1\times1}$$
            - This is also known as inner product or dot product
        - Alternatively, we can take .$\vec x$ and multiply by the transpose of .$y$
            $$\vec x \vec y^T = \text{some matrix} \in \mathbb{R}^{N \times N}$$
            - Do not commute!
            - Also known as outer product


## Matrices

- A collection of numbers in a rectangular form
    - Or a collection of .$M$, .$N$-length vectors
    <!-- todo latex -->
- Remark. Matrices are often represented by capital letters (e.g. .$A$),

### Vectors as Matrices
- A vector is a _degenerate_ matrix, that is, .$\vec x \in \mathbb{R}^{n \times 1}$
- A scalar is a _degenerate_ vector or matrix, that is, .$a \in \mathbb{R}^{1 \times 1} $


### Special Matrices


### Matrix Operations

#### Matrix Addition



#### Scalar-Matrix Multiplication



#### Matrix-Vector Multiplication

- Given .$A \in \mathbb{R}^{M \times N}, \vec x = \mathbb{R}^{N\times 1}$
    $$A\vec x = [ a_{11}  a_{12}  \dots a_{1N} ] \vec x = \vec y^T _1 \vec x$$
    - Notice we end with some _vector_ .$\in \mathbb{R}^{M\times 1}$
![](slides)

#### Matrix-Matrix Multiplication

- Given .$A \in \mathbb{R}^{M \times N}, B = \mathbb{R}^{N\times L}$
    $$AB = $$
    - Notice we end with some _matrix_ .$\in \mathbb{R}^{M \times L}$
- Matrix multiplication does not commute!
- Fun fact: Computers have to do so many multiply and add operations that it's [optimized at a processor level](https://en.wikipedia.org/wiki/Multiply%E2%80%93accumulate_operation) (leaned about this is 61C)


### Row vs Column Perspective

- Each row 
