---
title: "Week 5: Basis & Circuit Analysis"
weight: 6
---


# 02-22 Basis

- Note [10](https://eecs16a.org/lecture/Note10.pdf)
- [Slides](https://eecs16a.org/lecture/Lecture5A_Slides.pdf)

{{< youtube "P2LTAUO1TdA" >}}

---

## Change of Basis

{{< columns >}}<!-- mathjax fix -->
- Previously we’ve seen that a basis for a vector space is a minimal spanning set of vectors.
- We can also define the standard basis vectors, e,x. the standard basis for .$ \mathbb{R}^{3}$ is the set .$\mathbb{E}$:
<---><!-- mathjax fix -->
$$\mathbb{E} = \big(\hat i, \hat j, \hat k \big)$$
$$\dots \equiv ( \vec e_1, \vec e_2, \vec e_3)$$
$$\dots \equiv  \left( \begin{bmatrix}
    1\\\
    0\\\
    0\\\
\end{bmatrix}, \begin{bmatrix}
    0\\\
    1\\\
    0\\\
\end{bmatrix}, \begin{bmatrix}
    0\\\
    0\\\
    1\\\
\end{bmatrix}\right) $$
{{< /columns >}}
- We can represent any set of vectors that form as basis as a linear combination of the _standard_ .$ \mathbb{R}^{3}$ basis vectors
    - When you plot a vector, the basis is normally implicit as the standard basis vectors

- **Change Of Basis Operation:** Preserves the geometrical vector but modify its coordinates such that when plotted in the new basis, the original and final vector are physically the same
{{< columns >}}<!-- mathjax fix -->
- Such that we're essentially re-expressing the coordinates of some .$\vec v$ (formed with basis .$\mathbb{E}$) in some new basis .$\mathbb{E}'$
<---><!-- mathjax fix -->
- We know that the basis vectors define the linear transformation matrix:
$$\begin{bmatrix}
    | & & | \\\
    \vec e_1 ' & \dots & \vec e_n ' \\\
    | & & | \\\
\end{bmatrix}$$
{{< /columns >}}
    {{< columns >}}<!-- mathjax fix -->
- Thus, we need to solve for the scalars that multiply each of the new basis vectors, .$v_i ' $, to produce the same physical vector .$\vec v$ as before:
<---><!-- mathjax fix -->
$$\begin{bmatrix}
    | & | \\\
    \vec e_1 ' & \vec e_2 ' \\\
    | & | \\\
\end{bmatrix}\begin{bmatrix}
    v_1 ' \\\
    v_2 ' \\\
\end{bmatrix} = \begin{bmatrix}
    v_1 \\\
    v_2 \\\
\end{bmatrix} = \vec v $$
    {{< /columns >}}
    {{< columns >}}<!-- mathjax fix -->
- Generally, we can say that if given a vector .$\vec v$ expressed with coordinates in the standard .$n$-dimensional basis .$\mathbb{E}$, then we can solve for the coordinates .$\vec v ' $ in a different .$n$-dimensional basis .$A$ with .$\vec v ' = A^{-1} \vec v$
    - .$A$ is formed with the columns of the new basis. To do the opposite (basis .$A \to \mathbb{E}$), we apply .$A$: .$\vec v = A \vec v ' $
    - Finally, suppose that we’re given .$\vec v$ with coordinates in an arbitrary basis .$P$ (matrix with columns .$\vec p_1, \dots, \vec p_n$), and we want to change to another arbitrary basis .$Q$ with columns .$\vec q_1, \dots, \vec q_n$. We must apply the transformation .$Q^{-1} P$. This follows from first transforming .$\vec v$ to the standard basis from .$P$, and then transforming to the new basis .$Q$
    {{< /columns >}}

    - The only question is whether we preserve the coordinates (which are the mathematical representation) and change the physical vector in accordance with the new basis, or preserve the physical vector and find the new coordinates.
        - [Page 57](https://eecs16a.org/EECS16ACompendiumOfNotesAndPracticeProblems.pdf#page=57&zoom=100,572,589) has pretty figures

## Matrix Change of Basis

> We can apply our knowledge to linear transformations to understand what it means to change the basis of a matrix

- Given transformation .$T \in \mathbb{R}^{n \times n}$ with input .$\vec v_1$ and output .$\vec v_2$, we can apply our column-wise interpretation of the matrix-vector product, but now, we must take care of the fact that our vectors may lie in a different basis than .$E$.

{{< columns >}}<!-- mathjax fix -->
- Suppose we have basis vectors .$\vec a_1, \dots, \vec a_n$ forming .$A \in \mathbb{R}^{n \times n}$, and vectors .$\vec v_1, \vec v_2$ represented in this basis: .$\vec v_i = v_{i,1} ' \vec a_1 + \dots v_{i,n} ' \vec a_n$
    - Each .$v_{i,k}$ is the .$k$th element of the vector .$\vec v_i$
    - We can also represent .$T$ in this basis as .$T ' $
- We start with .$\vec v_1 = A \vec v_1 ' $ and .$\vec v_2 = A\vec v_2 ' $.
- Since .$T\vec v_1 = \vec v_2$ by definition of linear transformation .$T$, we can plug in: .$TA\vec v_1 ' = A\vec v_2 ' \Longrightarrow A^{−1} T A \vec v_1 ' = \vec v_2 ' $.
- Clearly, just as how we had .$T\vec v_1 = \vec v_2$, we have an analogous transformation in the new basis: .$T ' \vec v_1 ' = \vec v_2 ' $, where .$T ' = A^{−1} T A $.
<---><!-- mathjax fix -->
> ![](/docs/eecs-16a/5/cob.png)
> .$T ' $ is equivalent to .$A$, then .$T$, then .$A^{−1}$
{{< /columns >}}

## Diagonalization

- Why bother with change of basis? In practical applications, matrix operations form the core of some very computationally heavy algorithms -- we need to be able to easily invert matrices, raise them to high powers, and multiply them commutatively; all of these can be accomplished with [diagonal matrices](https://en.wikipedia.org/wiki/Diagonal_matrix), where all entries not on the diagonal are zero
    - An identity matrix of any size, or any multiple of it (a scalar matrix), is a diagonal matrix.
    - Its determinant is the product of its diagonal values
- First, we must choose a **diagonalizing basis** .$A$ that consists of the .$n$ eigenvectors of .$T$.
    - This is only possible if .$T$ is _diagonalizable_, which **requires it to have .$n$ linearly independent eigenvectors**
        - Note this is different from the .$n$ columns being independent!
    - Then, to figure out how .$T$ transforms .$\vec v_1$, we write .$\vec v_1 = \vec v_{11} ' \vec a_1 + \dots + \vec v_{1n} ' \vec a_n$
        $$T \vec v_1 = T \vec v_{11} ' \vec a_1 + \dots + T v_{1n} ' \vec a_n$$
        $$\dots = v_{11} (T \vec a_1) + \dots + v_{1n} (T\vec a_n)$$
        $$\dots = v_{11} (\lambda_1 \vec a_1) + \dots + v_{1n} (\lambda_n \vec a_n)$$
    - Using the middle matrix (diagonal!) as a sort of "sifting" matrix; each row only has one nonzero value, so we only multiply the .$v_{1i} \vec a_i$ value by the corresponding .$\lambda_i$:
        $$T \vec v_1 = \begin{bmatrix}
    | & & | \\\
    \vec a_1 & \dots & \vec a_n \\\
    | & & | \\\
\end{bmatrix}\begin{bmatrix}
    \lambda_{1} & & \\\
    & \ddots & \\\
    & & \lambda_{n}
\end{bmatrix}\begin{bmatrix}
    v_1 ' \\\
    v_2 ' \\\
    \vdots \\\
    v_n ' \\\
\end{bmatrix}$$
    - Notice now we have an equation of the form:
        $$T \vec v_1 = AD \vec v_1 ' \Longrightarrow T\vec v_1 = ADA^{-1} \vec v_1 \Longrightarrow T = ADA^{-1}$$
        - Here, .$D$ is the diagonal matrix containing the eigenvalues of the transformation
        - One useful result is that .$T^k = AD^k A^{-1}$ (expand and group .$A^{-1}A = I$)
            - Since .$D$ is easy to raise to high powers (.$D^k$ contains all diagonal entries raised to the .$k$th power), the computation of numerous transformations becomes much easier
            - To raise a diagonal matrix to a power, one can raise each element to that power









<!-- # 02-24 Intro to Circuit Analysis -->

<!-- - Note 11[A](https://eecs16a.org/lecture/Note11A.pdf) [B](https://eecs16a.org/lecture/Note11B.pdf) -->

<!-- --- -->
