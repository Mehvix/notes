---
title: "Week 4: Vector Spaces & Eigen"
weight: 5
---



# 02-15: Vector Spaces: Null Spaces and Columnspaces

- [Slides](https://eecs16a.org/lecture/Lecture4A_Slides.pdf)
- Notes [7](https://eecs16a.org/lecture/Note7.pdf) [8](https://eecs16a.org/lecture/Note8.pdf)



## Important Jargon 

- **Rank** a matrix .$A$ is the number of linearly independent columns
- **Nullspace** of a matrix is the set of solutions to .$A \vec x = 0$
- A **vector space** is a set of vectors connected by two operators: .$+, \times$ --- [page 48](https://eecs16a.org/lecture/Lecture3B_Slides.pdf#page=48)
- A vector **subspace** is a subset of vectors that have “nice properties” --- [page 50](https://eecs16a.org/lecture/Lecture3B_Slides.pdf#page=50)
- A **basis** for a vector space is a minimum set of vectors needed to represent all vectors in the space
- **Dimension** of a vector space is the number of basis vectors
- **Column space** is the span (range) of the columns of a matrix
- **Row space** is the span of the rows of a matrix

---


## Vector Spaces

- A vector space .$\mathbb{V}$ is a set of vectors and two operators .$+, \cdot$ that satisfy:

{{< columns >}}<!-- mathjax fix -->
**Vector Addition**
- Associative: .$\vec u + (\vec v + \vec w) = (\vec u + \vec v) + \vec w$ 
- Commutative: .$\vec u + \vec v = \vec v + \vec u$
- Additive Identity: There exists an additive identity .$\vec 0 \in \mathbb{V}$ such that .$\vec v + \vec 0 = \vec v$
- Additive Inverse: There exists .$- \vec v \in \mathbb{V}$ such that .$\vec v + (-\vec v) = \vec 0$. We call .$-\vec v$ the additive inverse of .$\vec v$.
- Closure under vector addition: The sum .$\vec v + \vec u$ must also be in .$\mathbb{V}$

<---><!-- mathjax fix -->
**Scalar Multiplication**

- Associative: .$\vec \alpha(\beta \vec v) = (\alpha \beta) \vec v$
- Multiplicative Identity: There exists .$1 \in \mathbb{R}$ where .$1 \cdot \vec v = \vec v$
- Distributive in vector addition: .$\alpha (\vec u + \vec v) = \alpha \vec u + \alpha \vec v$ 
- Distributive in scalar addition: .$(\alpha + \beta)\vec v = \alpha \vec v + \beta \vec v$
- Closure under scalar multiplication: The product .$\alpha \vec v$ must also be in .$\mathbb{V}$.
{{< /columns >}}
{{< center >}}
    ... for any .$\vec v, \vec u, \vec w \in \mathbb{V}; \alpha, \beta \in \mathbb{R}$
{{< /center >}}
  - These can be grouped by axioms of closure, addition, and scaling shown on [slide 10](https://eecs16a.org/lecture/Lecture4A_Slides.pdf#page=10)
- For example .$ \mathbb{R}^{n}$ is the vector space of all .$n$-dimensional vectors. 
    - In fact, the set of all matrices the same size is also a vector space .$ \mathbb{R}^{n \times o}$ since it fulfills all of the properties above as well 
    - In this class we will generally only deal with vector spaces containing vectors in .$\mathbb{R}^{n}$.

### Subspaces

- A subspace .$\mathbb{U}$ consists of a subset of .$\mathbb{V}$ in vector space (.$\mathbb{V}, \mathbb{F}, +, \cdot$). .$\mathbb{U} \subset \mathbb{V}$ and have 3 properties
    1. Contains .$\vec 0 $, i.e., .$\vec 0 \in \mathbb{U}$
    2. Closed under vector addition: .$\vec v_1, \vec v_2 \in \mathbb{U} \Longrightarrow \vec v_1 + \vec v_2 \in \mathbb{U}$
    3. Closed under scalar multiplication: .$\vec v \in \mathbb{U}, \alpha \in \mathbb{F} \Longrightarrow \alpha \vec v \in \mathbb{U}$
- Examples on [slide 13](https://eecs16a.org/lecture/Lecture4A_Slides.pdf#page=13)
- Intuitively, a subspace is a closed subset of all the vectors in .$ \mathbb{V}$. 
    - Any linear combination of vectors in the subspace must also lie in that subspace. 
- **Basis for a Subspace:** set of linearly independent vectors that span the subspace (minimal set of subspace-spanning vectors)
- **Subspace Dimension:** number of vectors in subspace-basis

## Basis

- **Basis:** Given a vector space .$\mathbb{V}$, a set of vectors .$\\{\vec v_1, \dots \vec v_n\\}$ is a basis of the vector space if it satisfies the following properties:
    1. .$\vec v_1, \dots, \vec v_n$ are linearly independent vectors
    2. .$\text{span}(\\{\vec v_1, \dots, \vec v_n\\}) = \mathbb{V} \Longrightarrow \forall \vec v \in \mathbb{V},  \exists \alpha_1, \dots, \alpha_{n-1} \in \mathbb{R}$ such that .$\vec v = \alpha_1 \vec v_2 + \dots \alpha_{n-1} \vec v_n$
    > Minimum set of vectors that spans a vector space
- A basis of a vector space is the **_minimum_ set of vectors needed to represent all vectors** in the vector space. 
    - If a set of vectors is linearly dependent and “spans” the vector space, it is still _not_ a basis -- we can remove at least one vector from the set and the resulting set will still span the vector space 

{{< youtube "P2LTAUO1TdA" >}}

### Basis is not unique. 

- Intuitively, think about multiplying one of the vectors in a given basis by a nonzero scalar will not affect the linear independence or span of the vectors. 
- We could alternatively construct another basis by replacing one of the vectors with the sum of itself and any other vector in the set.
- Mathematically, suppose that .$\\{\vec v_1, \dots, \vec v_n \\}$ is a basis for the vector space we are considering.
    - Thus .$\\{\alpha \vec v_1, \dots, \vec v_n \\}$ where .$\alpha \neq 0$ is also a basis because, just as we’ve seen in Gaussian elimination row operations, multiplying a row by a nonzero constant does not change the linear independence or dependence of the rows.
        - We can generalize this to say that multiplying a vector by a nonzero scalar also does not change the linear independence of the set of vectors. 
    - In addition, we know that .$\text{span}(\\{ \vec v_1, \dots, \vec v_n \\}) = \text{span}( \\{\alpha \vec v_1, \dots, \vec v_n \\} )$
        - Any vector in .$\text{span}(\\{ \vec v_1, \dots, \vec v_n \\})$ can be created as a linear combination of the set .$\text{span}(\\{ \alpha \vec v_1, \dots, \vec v_n \\})$ by dividing the scale factor on .$\vec v_1$ by .$\alpha$. 
        - We can use a similar argument to show that .$\\{\alpha \vec v_1, \dots, \vec v_n \\}$ is also a basis for the same vector space.
    > To generalize, for .$\mathbb{R}^{N}$, any .$N$ (and _only_ .$N$) linearly independent vectors form a basis

## Dimension

- **Dimension:** The dimension of a vector space is the number of basis vectors.
- Since each basis vector can be scaled by one coefficient, the dimension of a space as the **fewest number of parameters needed to describe an element** or member of that space.
- The dimension can also be thought of as the **degrees of freedom of your space** -- that is, the number of parameters that can be varied when describing a member of that space.
    > __A vector space can have many bases, but each basis must have the same number of vectors:__
    > - Suppose a basis for the vector space we’re considering has .$n$ vectors. This means that the minimum number of vectors we can use to represent all vectors in the vector space is .$n$, because the vectors in the basis would not be linearly independent if the vector space could be represented with fewer vectors. 
    > - Then we can show that any set with less than .$n$ vectors cannot be a basis because it does not have enough vectors to span the vector space -- there would be some vectors in the vector space that cannot be expressed as a linear combination of the vectors in the set. 
    > - In addition, we can show that any set with more than .$n$ vectors must be linearly dependent and therefore cannot be a basis. 
    > - Combining the two arguments, we have that any other set of vectors that forms a basis for the vector space must have exactly .$n$ vectors!

## Column Space

- The range/span/column space of matrix .$A \in \mathbb{R}^{m \times n}$ -- which we can represent as a set of vectors .$\\{ \vec a_1, \dots \vec a_n \\}$ -- is a set of all possible linear combinations:
$$\text{span}\big(\\{\vec a_1, \dots, \vec a_n\\}\big) = \Bigg\\{\sum_{i=1}^N \alpha_i \vec a_i\ |\ \alpha_1, \dots, \alpha_n \in \mathbb{R} \Bigg\\}$$
- That is, the column space of a matrix .$A \in \mathbb{R}^{m \times n}$ is the span of the .$n$ columns in .$A$
- Thinking about .$A$ as a linear transformation from .$ \mathbb{R}^{n} \to \mathbb{R}^{m}$, the column space is effectively the **set of all outputs** that this matrix can transform input vectors to
- Note that in the general case, input vectors and output vectors can be _different lengths_
    - The column space describes all possible output vectors .$\vec b = \mathbb{R}^{m \times 1}$
    - It can be shown that .$\text{span}(A)$ forms a subspace of .$ \mathbb{R}^{m}$
        - Note that .$\text{span}(A)$  is not necessarily .$ \mathbb{R}^{m}$

## Row Space

- Similarly, the row space is the span of the .$n$ rows

## Rank

- The rank of .$A$ is defined as the **dimension of the column space** of .$A \in \mathbb{R}^{m \times n}$
    - .$\text{rank}(A) = \text{dim(span(A))}$
        - .$\text{dim(span}(A\text{)) ≡ dim(col(}A))$
        - It’s all too easy to confuse an actual space consisting of vectors, like a matrix range describing the output (column) space, with the dimension of that space, which is just a single scalar number. Keep them straight!
    - .$ \text{rank}(A) = \text{dim(span}(A)) \leq \text{min}(m, n)$
        - This is at most .$m$, but certainly can be less, since an arbitrary .$A \in \mathbb{R}^{m \times n}$ is not guaranteed to have columns whose **span** is all of .$ \mathbb{R}^{m}$
        - Consider the simple counterexample of the zero matrix .$\vec 0 \in \mathbb{R}^{m \times n}$, which maps all .$n$-dimensional input vectors to the .$m$-dimensional all-zero vector.
- In general, using the column-wise representation of matrix-vector multiplication we can show that .$\text{rank(}A)$ is the **number of linearly independent columns** in .$A$. 
    - Any output vector can be represented as a linear combination of the columns of .$A$.
    - But some of these columns might themselves be linear combinations of other columns, which means we can replace any redundant column with a weighted sum of the other columns. 
    - By removing all redundancies, we find that a matrix with .$k \leq \text{min}(n, m)$ linearly independent column vectors can "unlock" exactly .$k$ dimensions in the output.
- Thus, we find that .$\text{rank}(A)$ also equals the **number of pivots** in the [`RREF`](https://en.wikipedia.org/wiki/Row_echelon_form) of .$A$.
    - Since each pivot must belong to a row and a column, the number of pivots in .$A \in \mathbb{R}^{m \times n}$ is limited by the smaller dimension. 
    - For a tall matrix .$m > n$, the columns are the limiting dimension; for a wide matrix .$n > m$ the rows are.

## Null Space

- The null-space of .$ \mathbb{R}^{m \times n}$ is the set of all vectors .$\vec x \in \mathbb{R}^{m}$ such that .$A\vec x = 0$
    - That is, the set of all inputs that get mapped to .$\vec 0$ by .$A$
    - $\text{dim(null}(A))$ can be interpreted as the number of input directions for which the output is "compressed" down to zero.
- .$\vec 0$ is always in the null space — _trivial Null space_
    - This wouldn't hold if we had [affine](0/#note-1ab-extra) (instead of linear) functions
- Null space DNE when the determinant is not zero -- see [last week](3/#inverse-of-a-2x2-matrix)

### Procedure to Compute a Null-Space for a Given Matrix:

- Computing the nullspace of .$A$ requires us to solve .$A \vec x = \vec 0$ -- the procedure is as follows:
    1. Put .$A$ in `RREF`. Initialize the set .$\mathbb{S} = \\{ \vec 0 \\}$.
    2. Check each column for leading entries and find the number of free (.$F$) and basic (.$B$) variables.
    1. if .$F = 0$, stop and skip to the last step.
    2. if .$F \neq 0$, repeat the following for each free variable:
        - Set that free variable to .$1$, and all others to zero.
        - Solve .$A \vec x$ under these conditions; add the solution vector to .$\mathbb{S}$.
    3. Conclude that .$\text{null}(A) = \text{span}(S)$.
- Example is given on [page 37-38](https://eecs16a.org/EECS16ACompendiumOfNotesAndPracticeProblems.pdf#page=37)

{{< youtube "uQhTuRlWMxw" >}}

### Rank-Nullity Theorem

> How is the number of **free variables** related to the **total number of columns** in a matrix .$A \in \mathbb{R}^{m \times n}$? Well, each column of a matrix either contributes a "new direction" to the output or it is redundant with other columns and their already-discovered directions. In other words, each of .$n$ columns adds a dimension to .$\text{span}(A)$ or to .$\text{null}(A)$. Therefore, the following holds: 
> $$\text{dim(span}(A)) + \text{dim(null}(A)) = n$$
> $$\text{rank}(A) + \text{dim(null}(A)) = n$$


# 02-17: Eigenvectors, values











{{< youtube "PFDu9oVAE-g" >}}
